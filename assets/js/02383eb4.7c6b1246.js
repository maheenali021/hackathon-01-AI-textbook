"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[234],{8453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>s});var i=a(6540);const t={},o=i.createContext(t);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(o.Provider,{value:n},e.children)}},8668:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module3/module3-milestone","title":"Module 3 Milestone Project: Isaac ROS Perception Pipeline","description":"Project Overview","source":"@site/docs/module3/module3-milestone.md","sourceDirName":"module3","slug":"/module3/module3-milestone","permalink":"/hackathon-01-AI-textbook/docs/module3/module3-milestone","draft":false,"unlisted":false,"editUrl":"https://github.com/maheenali021/hackathon-01-AI-textbook/tree/main/docs/module3/module3-milestone.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Module 3 Milestone Project: Isaac ROS Perception Pipeline"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Navigation2 Path Planning and Execution","permalink":"/hackathon-01-AI-textbook/docs/module3/nav2-path-planning"},"next":{"title":"Chapter 1: Introduction to VLA","permalink":"/hackathon-01-AI-textbook/docs/module4/intro-to-vla"}}');var t=a(4848),o=a(8453);const r={sidebar_position:4,title:"Module 3 Milestone Project: Isaac ROS Perception Pipeline"},s="Module 3 Milestone Project: Isaac ROS Perception Pipeline",c={},l=[{value:"Project Overview",id:"project-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Project Requirements",id:"project-requirements",level:2},{value:"Step-by-Step Implementation",id:"step-by-step-implementation",level:2},{value:"Step 1: Create the Perception Launch File",id:"step-1-create-the-perception-launch-file",level:3},{value:"Step 2: Create the Navigation Integration Node",id:"step-2-create-the-navigation-integration-node",level:3},{value:"Step 3: Create the Perception Processing Node",id:"step-3-create-the-perception-processing-node",level:3},{value:"Step 4: Create the Main Pipeline Launch File",id:"step-4-create-the-main-pipeline-launch-file",level:3},{value:"Step 5: Package Configuration",id:"step-5-package-configuration",level:3},{value:"Running the Perception Pipeline",id:"running-the-perception-pipeline",level:2},{value:"To run the complete perception pipeline:",id:"to-run-the-complete-perception-pipeline",level:3},{value:"Testing Your Implementation",id:"testing-your-implementation",level:2},{value:"Test 1: Verify Perception Nodes Are Running",id:"test-1-verify-perception-nodes-are-running",level:3},{value:"Test 2: Monitor Performance",id:"test-2-monitor-performance",level:3},{value:"Test 3: Visualization",id:"test-3-visualization",level:3},{value:"Enhancement Challenges",id:"enhancement-challenges",level:2},{value:"Solution Summary",id:"solution-summary",level:2},{value:"Learning Review",id:"learning-review",level:2},{value:"Next Steps",id:"next-steps",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-3-milestone-project-isaac-ros-perception-pipeline",children:"Module 3 Milestone Project: Isaac ROS Perception Pipeline"})}),"\n",(0,t.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,t.jsx)(n.p,{children:"In this milestone project, you'll create a complete perception pipeline using Isaac ROS packages. You'll integrate multiple perception capabilities including visual SLAM, object detection, and sensor processing to enable a robot to understand and navigate its environment."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this project, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Configure and run Isaac ROS perception nodes"}),"\n",(0,t.jsx)(n.li,{children:"Integrate multiple perception capabilities in a single pipeline"}),"\n",(0,t.jsx)(n.li,{children:"Process sensor data using GPU acceleration"}),"\n",(0,t.jsx)(n.li,{children:"Combine perception outputs for navigation and manipulation"}),"\n",(0,t.jsx)(n.li,{children:"Optimize perception pipelines for real-time performance"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"project-requirements",children:"Project Requirements"}),"\n",(0,t.jsx)(n.p,{children:"Create a perception pipeline with:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Visual SLAM for mapping and localization"}),"\n",(0,t.jsx)(n.li,{children:"Object detection for environment understanding"}),"\n",(0,t.jsx)(n.li,{children:"Depth estimation for obstacle detection"}),"\n",(0,t.jsx)(n.li,{children:"Sensor fusion for comprehensive perception"}),"\n",(0,t.jsx)(n.li,{children:"Integration with navigation systems"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-by-step-implementation",children:"Step-by-Step Implementation"}),"\n",(0,t.jsx)(n.h3,{id:"step-1-create-the-perception-launch-file",children:"Step 1: Create the Perception Launch File"}),"\n",(0,t.jsxs)(n.p,{children:["Create the file ",(0,t.jsx)(n.code,{children:"perception_pipeline/launch/perception_pipeline.launch.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.conditions import IfCondition\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node, ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\nfrom ament_index_python.packages import get_package_share_directory\n\n\ndef generate_launch_description():\n    # Launch arguments\n    use_composition = LaunchConfiguration('use_composition', default='False')\n    use_viz = LaunchConfiguration('use_viz', default='True')\n    use_laser = LaunchConfiguration('use_laser', default='True')\n    use_pointcloud = LaunchConfiguration('use_pointcloud', default='True')\n\n    # Package directories\n    pkg_isaac_ros_visual_slam = get_package_share_directory('isaac_ros_visual_slam')\n    pkg_isaac_ros_detect_and_track = get_package_share_directory('isaac_ros_detect_and_track')\n    pkg_isaac_ros_stereo_image_proc = get_package_share_directory('isaac_ros_stereo_image_proc')\n    pkg_isaac_ros_occupancy_grid_localizer = get_package_share_directory('isaac_ros_occupancy_grid_localizer')\n    pkg_isaac_ros_ess = get_package_share_directory('isaac_ros_ess')\n    pkg_isaac_ros_gxf = get_package_share_directory('isaac_ros_gxf')\n    pkg_isaac_ros_apriltag = get_package_share_directory('isaac_ros_apriltag')\n\n    # Visual SLAM node\n    visual_slam_node = ComposableNode(\n        name='visual_slam_node',\n        package='isaac_ros_visual_slam',\n        plugin='isaac_ros::slam::VisualSLAMNode',\n        parameters=[{\n            'enable_occupancy_grid': True,\n            'enable_diagnostics': False,\n            'occupancy_grid_resolution': 0.05,\n            'min_num_features': 100,\n            'enable_slam_visualization': True,\n            'enable_landmarks_display': True,\n        }],\n        remappings=[\n            ('stereo_camera/left/image', 'camera/left/image_raw'),\n            ('stereo_camera/left/camera_info', 'camera/left/camera_info'),\n            ('stereo_camera/right/image', 'camera/right/image_raw'),\n            ('stereo_camera/right/camera_info', 'camera/right/camera_info'),\n            ('visual_slam/imu', 'imu/data'),\n        ]\n    )\n\n    # Isaac ROS Apriltag node for precise localization\n    apriltag_node = ComposableNode(\n        name='apriltag',\n        package='isaac_ros_apriltag',\n        plugin='nvidia::isaac_ros::apriltag::AprilTagNode',\n        parameters=[{\n            'size': 0.32,\n            'max_tags': 64,\n            'tile_size': 2,\n            'quad_decimate': 2.0,\n            'quad_sigma': 0.0,\n            'refine_edges': True,\n            'decode_sharpening': 0.25,\n            'min_tag_width': 8,\n        }],\n        remappings=[\n            ('image', 'camera/image_raw'),\n            ('camera_info', 'camera/camera_info'),\n            ('detections', 'apriltag_detections'),\n        ]\n    )\n\n    # Stereo DNN node for object detection\n    stereo_dnn_node = ComposableNode(\n        name='stereo_dnn',\n        package='isaac_ros_stereo_dnn',\n        plugin='nvidia::isaac_ros::stereo_dnn::StereoDNNNode',\n        parameters=[{\n            'network_info.input_layer_names': ['input'],\n            'network_info.output_layer_names': ['output'],\n            'network_info.network_name': 'detectnet',\n            'network_info.model_namespace': 'Isaac-ROS-Models',\n            'input_layer_width': 960,\n            'input_layer_height': 544,\n            'max_batch_size': 1,\n            'engine_file_path': '/tmp/detectnet.engine',\n            'threshold': 0.5,\n            'enable_padding': True,\n            'input_tensor': 'input',\n            'output_bbox_tensor': 'output',\n            'output_covariance_tensor': 'output_cov',\n            'bbox_container': 'bbox',\n            'covariance_container': 'cov',\n            'image_width': 960,\n            'image_height': 576,\n            'do_resize': True,\n            'scale': 1.0,\n            'flip_image': 0,\n            'encoding_desired': 'rgb8',\n            'apply_edge_aware_threshold': False,\n            'enable_segmentation_masks': False,\n            'mask_threshold': 0.5,\n            'mask_bbox_overlap_threshold': 0.8,\n            'enable_bounding_box_crop': False,\n            'class_labels_file_path': '/tmp/class_labels.txt',\n        }],\n        remappings=[\n            ('left_image', 'camera/left/image_raw'),\n            ('right_image', 'camera/right/image_raw'),\n            ('left_camera_info', 'camera/left/camera_info'),\n            ('right_camera_info', 'camera/right/camera_info'),\n            ('detections', 'stereo_detections'),\n        ]\n    )\n\n    # Point Cloud Node for 3D reconstruction\n    point_cloud_node = ComposableNode(\n        name='point_cloud',\n        package='isaac_ros_stereo_image_proc',\n        plugin='nvidia::isaac_ros::stereo_image_proc::PointCloudNode',\n        parameters=[{\n            'use_color': True,\n            'unit_scaling': 1.0,\n        }],\n        remappings=[\n            ('left/image_rect_color', 'camera/left/image_rect_color'),\n            ('left/camera_info', 'camera/left/camera_info'),\n            ('right/camera_info', 'camera/right/camera_info'),\n            ('disparity', 'stereo/disparity'),\n            ('points', 'points'),\n        ]\n    )\n\n    # Container for all perception nodes\n    perception_container = ComposableNodeContainer(\n        name='perception_container',\n        namespace='',\n        package='rclcpp_components',\n        executable='component_container_mt',\n        composable_node_descriptions=[\n            visual_slam_node,\n            apriltag_node,\n            stereo_dnn_node,\n            point_cloud_node,\n        ],\n        output='screen'\n    )\n\n    # RViz2 node for visualization\n    rviz_config = os.path.join(\n        pkg_isaac_ros_visual_slam,\n        'rviz',\n        'visual_slam.rviz'\n    )\n\n    rviz_node = Node(\n        condition=IfCondition(use_viz),\n        package='rviz2',\n        executable='rviz2',\n        name='rviz2',\n        arguments=['-d', rviz_config],\n        output='screen'\n    )\n\n    # Occupancy grid localizer node\n    occupancy_grid_localizer_node = ComposableNode(\n        name='occupancy_grid_localizer',\n        package='isaac_ros_occupancy_grid_localizer',\n        plugin='nvidia::isaac_ros::occupancy_grid_localizer::OccupancyGridLocalizerNode',\n        parameters=[{\n            'map_width': 50.0,\n            'map_height': 50.0,\n            'map_resolution': 0.05,\n            'map_frame': 'map',\n            'robot_frame': 'base_link',\n            'update_mode': 'match_global_localization',\n            'global_localization_interval': 5.0,\n        }],\n        remappings=[\n            ('localization', 'localization'),\n            ('occupancy_grid', 'map'),\n            ('global_localization_pose', 'initialpose'),\n        ]\n    )\n\n    # Container for localization nodes\n    localization_container = ComposableNodeContainer(\n        name='localization_container',\n        namespace='',\n        package='rclcpp_components',\n        executable='component_container_mt',\n        composable_node_descriptions=[\n            occupancy_grid_localizer_node,\n        ],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_composition', default_value='False',\n            description='Use composed bringup if True'),\n        DeclareLaunchArgument(\n            'use_viz', default_value='True',\n            description='Launch RViz if True'),\n        DeclareLaunchArgument(\n            'use_laser', default_value='True',\n            description='Use laser-based perception if True'),\n        DeclareLaunchArgument(\n            'use_pointcloud', default_value='True',\n            description='Use point cloud processing if True'),\n        perception_container,\n        localization_container,\n        rviz_node,\n    ])\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-create-the-navigation-integration-node",children:"Step 2: Create the Navigation Integration Node"}),"\n",(0,t.jsxs)(n.p,{children:["Create the file ",(0,t.jsx)(n.code,{children:"perception_pipeline/src/navigation_integration.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav_msgs.msg import OccupancyGrid, Odometry\nfrom visualization_msgs.msg import MarkerArray\nfrom builtin_interfaces.msg import Duration\nfrom std_msgs.msg import Header\nimport numpy as np\nimport math\nfrom tf2_ros import TransformListener, Buffer\nimport tf2_geometry_msgs\nfrom geometry_msgs.msg import Point, Pose, PoseStamped\n\n\nclass NavigationIntegration(Node):\n    def __init__(self):\n        super().__init__(\'navigation_integration\')\n\n        # QoS profile for sensor data\n        qos_profile = QoSProfile(\n            reliability=ReliabilityPolicy.BEST_EFFORT,\n            history=HistoryPolicy.KEEP_LAST,\n            depth=1\n        )\n\n        # Subscriptions\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.scan_callback,\n            qos_profile\n        )\n\n        self.map_sub = self.create_subscription(\n            OccupancyGrid,\n            \'/map\',\n            self.map_callback,\n            10\n        )\n\n        self.odom_sub = self.create_subscription(\n            Odometry,\n            \'/odom\',\n            self.odom_callback,\n            10\n        )\n\n        self.detection_sub = self.create_subscription(\n            MarkerArray,\n            \'/object_detections\',\n            self.detection_callback,\n            10\n        )\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, \'/goal_pose\', 10)\n        self.safe_path_pub = self.create_publisher(PoseStamped, \'/safe_path\', 10)\n\n        # TF buffer and listener\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Robot state\n        self.current_pose = Pose()\n        self.current_scan = None\n        self.current_map = None\n        self.obstacles = []\n        self.navigation_goals = []\n\n        # Navigation parameters\n        self.safe_distance = 0.5  # meters\n        self.max_linear_speed = 0.5\n        self.max_angular_speed = 1.0\n\n        # Timer for navigation loop\n        self.nav_timer = self.create_timer(0.1, self.navigation_loop)\n\n    def scan_callback(self, msg):\n        """Process LIDAR scan data"""\n        self.current_scan = msg\n        self.get_logger().info(f\'Received scan with {len(msg.ranges)} points\')\n\n    def map_callback(self, msg):\n        """Process occupancy grid map"""\n        self.current_map = msg\n        self.get_logger().info(\'Received occupancy grid map\')\n\n    def odom_callback(self, msg):\n        """Process odometry data"""\n        self.current_pose = msg.pose.pose\n\n    def detection_callback(self, msg):\n        """Process object detection results"""\n        self.obstacles = []\n        for marker in msg.markers:\n            if marker.type == marker.CUBE:  # Assuming detected objects are cubes\n                obstacle = {\n                    \'position\': marker.pose.position,\n                    \'size\': marker.scale,\n                    \'label\': marker.ns\n                }\n                self.obstacles.append(obstacle)\n\n    def navigation_loop(self):\n        """Main navigation decision-making loop"""\n        if not self.current_scan or not self.current_pose:\n            return\n\n        # Calculate safe movement based on sensor data\n        cmd_vel = Twist()\n\n        # Check for obstacles in front of robot\n        front_ranges = self.get_front_scan_ranges()\n        if front_ranges:\n            min_distance = min(front_ranges)\n            if min_distance < self.safe_distance:\n                # Obstacle too close - stop or turn\n                cmd_vel.linear.x = 0.0\n                cmd_vel.angular.z = 0.5  # Turn right\n            else:\n                # Path clear - move forward\n                cmd_vel.linear.x = min(self.max_linear_speed, min_distance * 0.5)\n                cmd_vel.angular.z = 0.0\n\n        # Apply obstacle avoidance from detections\n        if self.obstacles:\n            avoidance_cmd = self.calculate_obstacle_avoidance()\n            cmd_vel.linear.x = min(cmd_vel.linear.x, avoidance_cmd.linear.x)\n            cmd_vel.angular.z += avoidance_cmd.angular.z\n\n        # Publish navigation command\n        self.cmd_vel_pub.publish(cmd_vel)\n\n        # Log navigation decision\n        self.get_logger().info(\n            f\'Navigation: Lin: {cmd_vel.linear.x:.2f}, \'\n            f\'Ang: {cmd_vel.angular.z:.2f}, \'\n            f\'Min dist: {min(front_ranges) if front_ranges else 0:.2f}m\'\n        )\n\n    def get_front_scan_ranges(self):\n        """Get scan ranges in front of the robot (forward 90 degrees)"""\n        if not self.current_scan:\n            return []\n\n        # Calculate indices for front 90 degrees\n        angle_min = self.current_scan.angle_min\n        angle_increment = self.current_scan.angle_increment\n        ranges = self.current_scan.ranges\n\n        # Front angles: -45 to +45 degrees\n        start_idx = int((math.radians(-45) - angle_min) / angle_increment)\n        end_idx = int((math.radians(45) - angle_min) / angle_increment)\n\n        start_idx = max(0, start_idx)\n        end_idx = min(len(ranges), end_idx)\n\n        return ranges[start_idx:end_idx]\n\n    def calculate_obstacle_avoidance(self):\n        """Calculate avoidance commands based on object detections"""\n        cmd_vel = Twist()\n\n        # Find closest obstacle in front of robot\n        closest_obstacle = None\n        min_distance = float(\'inf\')\n\n        for obstacle in self.obstacles:\n            # Calculate distance to obstacle\n            dx = obstacle[\'position\'].x - self.current_pose.position.x\n            dy = obstacle[\'position\'].y - self.current_pose.position.y\n            distance = math.sqrt(dx*dx + dy*dy)\n\n            if distance < min_distance and distance < 2.0:  # Only consider obstacles within 2m\n                min_distance = distance\n                closest_obstacle = obstacle\n\n        if closest_obstacle and min_distance < 1.0:\n            # Calculate direction to move away from obstacle\n            dx = closest_obstacle[\'position\'].x - self.current_pose.position.x\n            dy = closest_obstacle[\'position\'].y - self.current_pose.position.y\n\n            # Calculate angle to obstacle\n            obstacle_angle = math.atan2(dy, dx)\n            robot_yaw = self.get_robot_yaw()\n\n            # Calculate relative angle\n            relative_angle = obstacle_angle - robot_yaw\n            relative_angle = math.atan2(math.sin(relative_angle), math.cos(relative_angle))\n\n            # Turn away from obstacle\n            if abs(relative_angle) < math.pi / 2:\n                # Obstacle is in front - turn away\n                cmd_vel.angular.z = 0.5 if relative_angle > 0 else -0.5\n                cmd_vel.linear.x = max(0.1, self.max_linear_speed * (1.0 - min_distance/1.0))\n\n        return cmd_vel\n\n    def get_robot_yaw(self):\n        """Extract yaw from robot\'s orientation quaternion"""\n        orientation = self.current_pose.orientation\n        siny_cosp = 2 * (orientation.w * orientation.z + orientation.x * orientation.y)\n        cosy_cosp = 1 - 2 * (orientation.y * orientation.y + orientation.z * orientation.z)\n        return math.atan2(siny_cosp, cosy_cosp)\n\n    def find_navigation_goals(self):\n        """Find potential navigation goals based on map and detections"""\n        goals = []\n\n        if self.current_map and self.obstacles:\n            # Look for areas that are free in the map but may contain interesting objects\n            for obstacle in self.obstacles:\n                # Create a goal near interesting objects\n                goal = PoseStamped()\n                goal.header.stamp = self.get_clock().now().to_msg()\n                goal.header.frame_id = \'map\'\n                goal.pose.position.x = obstacle[\'position\'].x\n                goal.pose.position.y = obstacle[\'position\'].y\n                goal.pose.position.z = 0.0\n                goal.pose.orientation.w = 1.0\n\n                goals.append(goal)\n\n        return goals\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    navigation_integration = NavigationIntegration()\n\n    try:\n        rclpy.spin(navigation_integration)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        navigation_integration.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-create-the-perception-processing-node",children:"Step 3: Create the Perception Processing Node"}),"\n",(0,t.jsxs)(n.p,{children:["Create the file ",(0,t.jsx)(n.code,{children:"perception_pipeline/src/perception_processor.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\nfrom vision_msgs.msg import Detection2DArray\nfrom geometry_msgs.msg import Point, Pose, TransformStamped\nfrom tf2_ros import TransformBroadcaster\nfrom builtin_interfaces.msg import Time\nfrom std_msgs.msg import Header\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\nimport message_filters\nfrom tf2_ros import Buffer, TransformListener\nimport tf2_geometry_msgs\nfrom visualization_msgs.msg import MarkerArray, Marker\nfrom geometry_msgs.msg import Point as GeometryPoint\n\n\nclass PerceptionProcessor(Node):\n    def __init__(self):\n        super().__init__('perception_processor')\n\n        # CV Bridge for image processing\n        self.bridge = CvBridge()\n\n        # Transform broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Publishers\n        self.object_detection_pub = self.create_publisher(Detection2DArray, '/processed_detections', 10)\n        self.marker_pub = self.create_publisher(MarkerArray, '/object_markers', 10)\n        self.point_cloud_pub = self.create_publisher(PointCloud2, '/processed_pointcloud', 10)\n\n        # Subscriptions with message filters for synchronization\n        self.image_sub = message_filters.Subscriber(self, Image, '/camera/image_raw')\n        self.camera_info_sub = message_filters.Subscriber(self, CameraInfo, '/camera/camera_info')\n\n        # Approximate time synchronizer\n        self.sync = message_filters.ApproximateTimeSynchronizer(\n            [self.image_sub, self.camera_info_sub],\n            queue_size=10,\n            slop=0.1\n        )\n        self.sync.registerCallback(self.process_camera_data)\n\n        # Internal state\n        self.latest_image = None\n        self.latest_camera_info = None\n\n        # Object detection parameters\n        self.confidence_threshold = 0.5\n        self.class_labels = [\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n            'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n            'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n            'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n            'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n            'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n            'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n            'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n        ]\n\n    def process_camera_data(self, image_msg, camera_info_msg):\n        \"\"\"Process synchronized camera data\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(image_msg, desired_encoding='bgr8')\n\n            # Process the image for object detection\n            detections = self.detect_objects(cv_image)\n\n            # Create detection message\n            detection_msg = self.create_detection_message(detections, image_msg.header)\n\n            # Publish detections\n            self.object_detection_pub.publish(detection_msg)\n\n            # Create and publish visualization markers\n            marker_array = self.create_visualization_markers(detections, image_msg.header)\n            self.marker_pub.publish(marker_array)\n\n            # Log detection results\n            self.get_logger().info(f'Detected {len(detections)} objects')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing camera data: {str(e)}')\n\n    def detect_objects(self, cv_image):\n        \"\"\"Simulate object detection (in real implementation, this would use a DNN)\"\"\"\n        # In a real implementation, this would use Isaac ROS DNN packages\n        # For simulation, we'll create some synthetic detections\n        height, width = cv_image.shape[:2]\n\n        # Simulated detections (in practice, these would come from a neural network)\n        detections = []\n\n        # Add some synthetic detections for demonstration\n        if np.random.random() > 0.7:  # 30% chance of detection\n            for i in range(np.random.randint(1, 4)):  # 1-3 detections\n                x = np.random.randint(0, width - 100)\n                y = np.random.randint(0, height - 100)\n                w = np.random.randint(50, 150)\n                h = np.random.randint(50, 150)\n\n                confidence = np.random.uniform(0.6, 0.99)\n                class_id = np.random.randint(0, len(self.class_labels))\n                class_name = self.class_labels[class_id]\n\n                detection = {\n                    'bbox': (x, y, w, h),\n                    'confidence': confidence,\n                    'class_id': class_id,\n                    'class_name': class_name\n                }\n                detections.append(detection)\n\n        return detections\n\n    def create_detection_message(self, detections, header):\n        \"\"\"Create vision_msgs/Detection2DArray message\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for detection in detections:\n            bbox_x, bbox_y, bbox_w, bbox_h = detection['bbox']\n\n            detection_2d = Detection2D()\n            detection_2d.header = header\n\n            # Set the bounding box\n            detection_2d.bbox.center.x = bbox_x + bbox_w / 2\n            detection_2d.bbox.center.y = bbox_y + bbox_h / 2\n            detection_2d.bbox.size_x = bbox_w\n            detection_2d.bbox.size_y = bbox_h\n\n            # Set the confidence\n            detection_2d.results.append(Detection2D())\n            detection_2d.results[0].score = detection['confidence']\n            detection_2d.results[0].class_id = detection['class_name']\n\n            detection_array.detections.append(detection_2d)\n\n        return detection_array\n\n    def create_visualization_markers(self, detections, header):\n        \"\"\"Create visualization markers for detected objects\"\"\"\n        marker_array = MarkerArray()\n\n        for i, detection in enumerate(detections):\n            # Create a marker for the bounding box\n            marker = Marker()\n            marker.header = header\n            marker.ns = \"detections\"\n            marker.id = i\n            marker.type = Marker.CUBE\n            marker.action = Marker.ADD\n\n            bbox_x, bbox_y, bbox_w, bbox_h = detection['bbox']\n\n            # Position in 3D space (projected from 2D image)\n            marker.pose.position.x = (bbox_x + bbox_w / 2) / header.frame_id.width * 2 - 1  # Normalize to -1 to 1\n            marker.pose.position.y = (bbox_y + bbox_h / 2) / header.frame_id.height * 2 - 1  # Normalize to -1 to 1\n            marker.pose.position.z = 1.0  # Fixed distance for visualization\n\n            # Size based on bounding box\n            marker.scale.x = bbox_w / header.frame_id.width * 2\n            marker.scale.y = bbox_h / header.frame_id.height * 2\n            marker.scale.z = 0.1  # Thin box for visualization\n\n            # Color based on class\n            marker.color.r = 1.0\n            marker.color.g = 0.0 if detection['confidence'] > 0.7 else 0.5\n            marker.color.b = 0.0 if detection['confidence'] > 0.7 else 0.5\n            marker.color.a = detection['confidence']\n\n            marker_array.markers.append(marker)\n\n            # Create a text marker for the label\n            text_marker = Marker()\n            text_marker.header = header\n            text_marker.ns = \"labels\"\n            text_marker.id = i + 1000  # Different ID space\n            text_marker.type = Marker.TEXT_VIEW_FACING\n            text_marker.action = Marker.ADD\n\n            text_marker.pose.position.x = marker.pose.position.x\n            text_marker.pose.position.y = marker.pose.position.y - 0.1  # Below the box\n            text_marker.pose.position.z = marker.pose.position.z\n\n            text_marker.scale.z = 0.1  # Text size\n            text_marker.color.r = 1.0\n            text_marker.color.g = 1.0\n            text_marker.color.b = 1.0\n            text_marker.color.a = 1.0\n\n            text_marker.text = f\"{detection['class_name']}: {detection['confidence']:.2f}\"\n\n            marker_array.markers.append(text_marker)\n\n        return marker_array\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_processor = PerceptionProcessor()\n\n    try:\n        rclpy.spin(perception_processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        perception_processor.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-4-create-the-main-pipeline-launch-file",children:"Step 4: Create the Main Pipeline Launch File"}),"\n",(0,t.jsxs)(n.p,{children:["Create the file ",(0,t.jsx)(n.code,{children:"perception_pipeline/launch/main_pipeline.launch.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import os\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription, DeclareLaunchArgument\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\n\ndef generate_launch_description():\n    # Launch configurations\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n    camera_namespace = LaunchConfiguration('camera_namespace', default='/camera')\n    robot_namespace = LaunchConfiguration('robot_namespace', default='')\n\n    # Package directories\n    pkg_perception_pipeline = get_package_share_directory('perception_pipeline')\n    pkg_isaac_ros_visual_slam = get_package_share_directory('isaac_ros_visual_slam')\n    pkg_isaac_ros_detect_and_track = get_package_share_directory('isaac_ros_detect_and_track')\n\n    # Include the perception pipeline launch\n    perception_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource(\n            os.path.join(pkg_perception_pipeline, 'launch', 'perception_pipeline.launch.py')\n        ),\n        launch_arguments={\n            'use_sim_time': use_sim_time\n        }.items()\n    )\n\n    # Perception processing node\n    perception_processor_node = Node(\n        package='perception_pipeline',\n        executable='perception_processor',\n        name='perception_processor',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }],\n        remappings=[\n            ('/camera/image_raw', [camera_namespace, '/image_raw']),\n            ('/camera/camera_info', [camera_namespace, '/camera_info']),\n        ],\n        output='screen'\n    )\n\n    # Navigation integration node\n    navigation_integration_node = Node(\n        package='perception_pipeline',\n        executable='navigation_integration',\n        name='navigation_integration',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }],\n        output='screen'\n    )\n\n    # Performance monitor node\n    performance_monitor_node = Node(\n        package='perception_pipeline',\n        executable='performance_monitor',\n        name='performance_monitor',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n        }],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation time if true'),\n        DeclareLaunchArgument(\n            'camera_namespace',\n            default_value='/camera',\n            description='Namespace for camera topics'),\n        DeclareLaunchArgument(\n            'robot_namespace',\n            default_value='',\n            description='Namespace for robot topics'),\n\n        perception_launch,\n        perception_processor_node,\n        navigation_integration_node,\n        performance_monitor_node,\n    ])\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-5-package-configuration",children:"Step 5: Package Configuration"}),"\n",(0,t.jsxs)(n.p,{children:["Create the file ",(0,t.jsx)(n.code,{children:"perception_pipeline/package.xml"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>perception_pipeline</name>\n  <version>0.0.0</version>\n  <description>Isaac ROS perception pipeline for Module 3 milestone project</description>\n  <maintainer email="your_email@example.com">Your Name</maintainer>\n  <license>TODO: License declaration</license>\n\n  <buildtool_depend>ament_cmake</buildtool_depend>\n  <buildtool_depend>ament_cmake_python</buildtool_depend>\n\n  <depend>rclcpp</depend>\n  <depend>rclpy</depend>\n  <depend>std_msgs</depend>\n  <depend>sensor_msgs</depend>\n  <depend>geometry_msgs</depend>\n  <depend>nav_msgs</depend>\n  <depend>visualization_msgs</depend>\n  <depend>vision_msgs</depend>\n  <depend>tf2</depend>\n  <depend>tf2_ros</depend>\n  <depend>tf2_geometry_msgs</depend>\n  <depend>cv_bridge</depend>\n  <depend>message_filters</depend>\n\n  <depend>isaac_ros_visual_slam</depend>\n  <depend>isaac_ros_detect_and_track</depend>\n  <depend>isaac_ros_stereo_image_proc</depend>\n  <depend>isaac_ros_occupancy_grid_localizer</depend>\n  <depend>isaac_ros_apriltag</depend>\n\n  <exec_depend>launch</exec_depend>\n  <exec_depend>launch_ros</exec_depend>\n\n  <test_depend>ament_lint_auto</test_depend>\n  <test_depend>ament_lint_common</test_depend>\n\n  <export>\n    <build_type>ament_cmake</build_type>\n  </export>\n</package>\n'})}),"\n",(0,t.jsxs)(n.p,{children:["And create the ",(0,t.jsx)(n.code,{children:"perception_pipeline/CMakeLists.txt"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-cmake",children:'cmake_minimum_required(VERSION 3.8)\nproject(perception_pipeline)\n\nif(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES "Clang")\n  add_compile_options(-Wall -Wextra -Wpedantic)\nendif()\n\n# find dependencies\nfind_package(ament_cmake REQUIRED)\nfind_package(rclcpp REQUIRED)\nfind_package(rclpy REQUIRED)\nfind_package(std_msgs REQUIRED)\nfind_package(sensor_msgs REQUIRED)\nfind_package(geometry_msgs REQUIRED)\nfind_package(nav_msgs REQUIRED)\nfind_package(visualization_msgs REQUIRED)\nfind_package(vision_msgs REQUIRED)\nfind_package(tf2 REQUIRED)\nfind_package(tf2_ros REQUIRED)\nfind_package(tf2_geometry_msgs REQUIRED)\nfind_package(cv_bridge REQUIRED)\nfind_package(message_filters REQUIRED)\n\n# Install Python modules\nament_python_install_package(${PROJECT_NAME})\n\n# Install Python executables\ninstall(PROGRAMS\n  src/navigation_integration.py\n  src/perception_processor.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n\n# Install launch files\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}/\n)\n\n# Install config files\ninstall(DIRECTORY config\n  DESTINATION share/${PROJECT_NAME}/\n)\n\n# Install RViz config\ninstall(DIRECTORY rviz\n  DESTINATION share/${PROJECT_NAME}/\n)\n\nif(BUILD_TESTING)\n  find_package(ament_lint_auto REQUIRED)\n  set(ament_cmake_copyright_FOUND TRUE)\n  set(ament_cmake_cpplint_FOUND TRUE)\n  ament_lint_auto_find_test_dependencies()\nendif()\n\nament_package()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"running-the-perception-pipeline",children:"Running the Perception Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"to-run-the-complete-perception-pipeline",children:"To run the complete perception pipeline:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Start the perception pipeline\nros2 launch perception_pipeline main_pipeline.launch.py\n\n# Terminal 2: Simulate camera and sensor data (if using real robot simulation)\n# This would typically come from your robot's sensors\n\n# Terminal 3: Monitor perception outputs\nros2 topic echo /processed_detections\nros2 topic echo /object_markers\nros2 topic echo /map\n"})}),"\n",(0,t.jsx)(n.h2,{id:"testing-your-implementation",children:"Testing Your Implementation"}),"\n",(0,t.jsx)(n.h3,{id:"test-1-verify-perception-nodes-are-running",children:"Test 1: Verify Perception Nodes Are Running"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Check active nodes\nros2 node list | grep perception\n\n# Check topics\nros2 topic list | grep -E "(detection|scan|map|camera)"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"test-2-monitor-performance",children:"Test 2: Monitor Performance"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Monitor detection rates\nros2 topic echo /processed_detections --field detections | head -n 10\n\n# Check navigation commands\nros2 topic echo /cmd_vel | head -n 5\n"})}),"\n",(0,t.jsx)(n.h3,{id:"test-3-visualization",children:"Test 3: Visualization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Launch RViz to visualize the perception outputs\nros2 run rviz2 rviz2 -d `ros2 pkg prefix perception_pipeline`/share/perception_pipeline/rviz/config.rviz\n"})}),"\n",(0,t.jsx)(n.h2,{id:"enhancement-challenges",children:"Enhancement Challenges"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Add more perception capabilities (semantic segmentation, instance segmentation)"}),"\n",(0,t.jsx)(n.li,{children:"Implement a learning-based perception system"}),"\n",(0,t.jsx)(n.li,{children:"Add sensor fusion between different modalities"}),"\n",(0,t.jsx)(n.li,{children:"Optimize the pipeline for edge devices"}),"\n",(0,t.jsx)(n.li,{children:"Add robustness to lighting and environmental changes"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"solution-summary",children:"Solution Summary"}),"\n",(0,t.jsx)(n.p,{children:"This milestone project demonstrates how to create a complete perception pipeline using Isaac ROS with:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Visual SLAM for mapping and localization"}),"\n",(0,t.jsx)(n.li,{children:"Object detection for environment understanding"}),"\n",(0,t.jsx)(n.li,{children:"Sensor fusion for comprehensive perception"}),"\n",(0,t.jsx)(n.li,{children:"Navigation integration for autonomous behavior"}),"\n",(0,t.jsx)(n.li,{children:"Performance monitoring for optimization"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"learning-review",children:"Learning Review"}),"\n",(0,t.jsx)(n.p,{children:"After completing this project, reflect on:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"How did GPU acceleration improve the perception pipeline performance?"}),"\n",(0,t.jsx)(n.li,{children:"What challenges did you face when integrating multiple perception capabilities?"}),"\n",(0,t.jsx)(n.li,{children:"How did sensor fusion improve the robot's understanding of its environment?"}),"\n",(0,t.jsx)(n.li,{children:"What trade-offs exist between accuracy and real-time performance?"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"This perception pipeline provides the foundation for the AI and decision-making systems you'll develop in Module 4. You can use this pipeline to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Feed perception data to AI reasoning systems"}),"\n",(0,t.jsx)(n.li,{children:"Create semantic maps for higher-level planning"}),"\n",(0,t.jsx)(n.li,{children:"Generate training data for machine learning models"}),"\n",(0,t.jsx)(n.li,{children:"Test human-robot interaction scenarios"}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}}}]);