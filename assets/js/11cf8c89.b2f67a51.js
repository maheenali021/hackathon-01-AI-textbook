"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[960],{1470:(e,n,i)=>{i.d(n,{A:()=>A});var a=i(6540),t=i(4164),r=i(7559),o=i(3104),s=i(6347),l=i(205),c=i(7485),d=i(1682),u=i(679);function p(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:n,children:i}=e;return(0,a.useMemo)(()=>{const e=n??function(e){return p(e).map(({props:{value:e,label:n,attributes:i,default:a}})=>({value:e,label:n,attributes:i,default:a}))}(i);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,i])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function g({queryString:e=!1,groupId:n}){const i=(0,s.W6)(),t=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(t),(0,a.useCallback)(e=>{if(!t)return;const n=new URLSearchParams(i.location.search);n.set(t,e),i.replace({...i.location,search:n.toString()})},[t,i])]}function f(e){const{defaultValue:n,queryString:i=!1,groupId:t}=e,r=h(e),[o,s]=(0,a.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const i=n.find(e=>e.default)??n[0];if(!i)throw new Error("Unexpected error: 0 tabValues");return i.value}({defaultValue:n,tabValues:r})),[c,d]=g({queryString:i,groupId:t}),[p,f]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[i,t]=(0,u.Dv)(n);return[i,(0,a.useCallback)(e=>{n&&t.set(e)},[n,t])]}({groupId:t}),v=(()=>{const e=c??p;return m({value:e,tabValues:r})?e:null})();(0,l.A)(()=>{v&&s(v)},[v]);return{selectedValue:o,selectValue:(0,a.useCallback)(e=>{if(!m({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);s(e),d(e),f(e)},[d,f,r]),tabValues:r}}var v=i(2303);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=i(4848);function w({className:e,block:n,selectedValue:i,selectValue:a,tabValues:r}){const s=[],{blockElementScrollPositionUntilNextRender:l}=(0,o.a_)(),c=e=>{const n=e.currentTarget,t=s.indexOf(n),o=r[t].value;o!==i&&(l(n),a(o))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const i=s.indexOf(e.currentTarget)+1;n=s[i]??s[0];break}case"ArrowLeft":{const i=s.indexOf(e.currentTarget)-1;n=s[i]??s[s.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":n},e),children:r.map(({value:e,label:n,attributes:a})=>(0,x.jsx)("li",{role:"tab",tabIndex:i===e?0:-1,"aria-selected":i===e,ref:e=>{s.push(e)},onKeyDown:d,onClick:c,...a,className:(0,t.A)("tabs__item",b.tabItem,a?.className,{"tabs__item--active":i===e}),children:n??e},e))})}function _({lazy:e,children:n,selectedValue:i}){const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=r.find(e=>e.props.value===i);return e?(0,a.cloneElement)(e,{className:(0,t.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:r.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==i}))})}function j(e){const n=f(e);return(0,x.jsxs)("div",{className:(0,t.A)(r.G.tabs.container,"tabs-container",b.tabList),children:[(0,x.jsx)(w,{...n,...e}),(0,x.jsx)(_,{...n,...e})]})}function A(e){const n=(0,v.A)();return(0,x.jsx)(j,{...e,children:p(e.children)},String(n))}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>s});var a=i(6540);const t={},r=a.createContext(t);function o(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),a.createElement(r.Provider,{value:n},e.children)}},8987:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>l,metadata:()=>a,toc:()=>u});const a=JSON.parse('{"id":"module4/voice-to-action","title":"Chapter 2: Voice-to-Action with OpenAI Whisper","description":"Convert Spoken Commands to ROS 2 Actions","source":"@site/docs/module4/voice-to-action.md","sourceDirName":"module4","slug":"/module4/voice-to-action","permalink":"/hackathon-01-AI-textbook/docs/module4/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/maheenali021/hackathon-01-AI-textbook/tree/main/docs/module4/voice-to-action.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Chapter 2: Voice-to-Action with OpenAI Whisper"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Introduction to VLA","permalink":"/hackathon-01-AI-textbook/docs/module4/intro-to-vla"},"next":{"title":"Chapter 3: Cognitive Planning with Large Language Models","permalink":"/hackathon-01-AI-textbook/docs/module4/cognitive-planning"}}');var t=i(4848),r=i(8453),o=i(1470),s=i(9365);const l={sidebar_position:2,title:"Chapter 2: Voice-to-Action with OpenAI Whisper"},c="Voice-to-Action with OpenAI Whisper",d={},u=[{value:"Convert Spoken Commands to ROS 2 Actions",id:"convert-spoken-commands-to-ros-2-actions",level:2},{value:"Overview of Voice-to-Action Pipeline",id:"overview-of-voice-to-action-pipeline",level:2},{value:"OpenAI Whisper Integration",id:"openai-whisper-integration",level:2},{value:"What is Whisper?",id:"what-is-whisper",level:3},{value:"Installation",id:"installation",level:3},{value:"Basic Usage Example",id:"basic-usage-example",level:3},{value:"Audio Capture for Robotics",id:"audio-capture-for-robotics",level:2},{value:"Real-time Audio Processing",id:"real-time-audio-processing",level:3},{value:"Natural Language Understanding",id:"natural-language-understanding",level:2},{value:"Intent Recognition",id:"intent-recognition",level:3},{value:"Command Extraction",id:"command-extraction",level:3},{value:"ROS 2 Action Integration",id:"ros-2-action-integration",level:2},{value:"Creating a Voice Command Node",id:"creating-a-voice-command-node",level:3},{value:"Advanced Voice Processing",id:"advanced-voice-processing",level:2},{value:"Wake Word Detection",id:"wake-word-detection",level:3},{value:"Confidence Scoring",id:"confidence-scoring",level:3},{value:"Error Handling and Robustness",id:"error-handling-and-robustness",level:2},{value:"Audio Quality Considerations",id:"audio-quality-considerations",level:3},{value:"Fallback Mechanisms",id:"fallback-mechanisms",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"voice-to-action-with-openai-whisper",children:"Voice-to-Action with OpenAI Whisper"})}),"\n",(0,t.jsx)(n.h2,{id:"convert-spoken-commands-to-ros-2-actions",children:"Convert Spoken Commands to ROS 2 Actions"}),"\n",(0,t.jsx)(n.p,{children:"Voice-to-action systems enable robots to understand and respond to natural language commands. This chapter focuses on using OpenAI Whisper for speech recognition and converting the recognized text into ROS 2 actions that the robot can execute."}),"\n",(0,t.jsx)(n.h2,{id:"overview-of-voice-to-action-pipeline",children:"Overview of Voice-to-Action Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"The voice-to-action system consists of several components:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Capture"}),": Recording spoken commands from users"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech Recognition"}),": Converting audio to text using Whisper"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Processing"}),": Understanding the intent behind the text"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Mapping"}),": Converting intents to specific ROS 2 actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execution"}),": Performing the requested actions using ROS 2 services/actions"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"openai-whisper-integration",children:"OpenAI Whisper Integration"}),"\n",(0,t.jsx)(n.h3,{id:"what-is-whisper",children:"What is Whisper?"}),"\n",(0,t.jsx)(n.p,{children:"Whisper is a robust speech recognition model developed by OpenAI that can handle multiple languages, accents, and background noise. It's particularly suitable for robotics applications due to its accuracy and ability to work with diverse audio conditions."}),"\n",(0,t.jsxs)(o.A,{groupId:"whisper-models",children:[(0,t.jsx)(s.A,{value:"tiny",label:"Tiny Model",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Smallest model, fastest processing\npip install openai-whisper\n# Load tiny model\nmodel = whisper.load_model("tiny")\n'})})}),(0,t.jsx)(s.A,{value:"base",label:"Base Model",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Good balance of speed and accuracy\npip install openai-whisper\n# Load base model\nmodel = whisper.load_model("base")\n'})})}),(0,t.jsx)(s.A,{value:"large",label:"Large Model",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Most accurate, slower processing\npip install openai-whisper\n# Load large model\nmodel = whisper.load_model("large")\n'})})})]}),"\n",(0,t.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n# Additional dependencies for audio processing\npip install pyaudio soundfile\n"})}),"\n",(0,t.jsx)(n.h3,{id:"basic-usage-example",children:"Basic Usage Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import whisper\nimport pyaudio\nimport wave\n\n# Load the Whisper model\nmodel = whisper.load_model("base")\n\n# Transcribe an audio file\nresult = model.transcribe("command.wav")\nprint(result["text"])\n'})}),"\n",(0,t.jsx)(n.h2,{id:"audio-capture-for-robotics",children:"Audio Capture for Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"real-time-audio-processing",children:"Real-time Audio Processing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import pyaudio\nimport wave\nimport numpy as np\n\n# Audio parameters\nFORMAT = pyaudio.paInt16\nCHANNELS = 1\nRATE = 44100\nCHUNK = 1024\nWAVE_OUTPUT_FILENAME = "command.wav"\n\naudio = pyaudio.PyAudio()\n\n# Start recording\nstream = audio.open(format=FORMAT, channels=CHANNELS,\n                   rate=RATE, input=True,\n                   frames_per_buffer=CHUNK)\n\nframes = []\nprint("Listening...")\n\n# Record until silence is detected\nfor i in range(0, int(RATE / CHUNK * 2)):  # 2 seconds of recording\n    data = stream.read(CHUNK)\n    frames.append(data)\n\nprint("Processing...")\n\n# Stop recording\nstream.stop_stream()\nstream.close()\naudio.terminate()\n\n# Save the recorded audio\nwf = wave.open(WAVE_OUTPUT_FILENAME, \'wb\')\nwf.setnchannels(CHANNELS)\nwf.setsampwidth(audio.get_sample_size(FORMAT))\nwf.setframerate(RATE)\nwf.writeframes(b\'\'.join(frames))\nwf.close()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,t.jsx)(n.h3,{id:"intent-recognition",children:"Intent Recognition"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def recognize_intent(transcript):\n    """Simple intent recognition based on keywords"""\n    transcript_lower = transcript.lower()\n\n    if "move" in transcript_lower or "go" in transcript_lower:\n        return "navigation"\n    elif "pick" in transcript_lower or "grasp" in transcript_lower:\n        return "manipulation"\n    elif "stop" in transcript_lower or "halt" in transcript_lower:\n        return "stop"\n    else:\n        return "unknown"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"command-extraction",children:"Command Extraction"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def extract_command_parameters(transcript):\n    \"\"\"Extract parameters from the voice command\"\"\"\n    # Simple parameter extraction\n    params = {}\n\n    # Extract direction if present\n    if \"forward\" in transcript.lower():\n        params['direction'] = 'forward'\n    elif \"backward\" in transcript.lower():\n        params['direction'] = 'backward'\n    elif \"left\" in transcript.lower():\n        params['direction'] = 'left'\n    elif \"right\" in transcript.lower():\n        params['direction'] = 'right'\n\n    # Extract distance if specified\n    import re\n    distance_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(meters?|m)', transcript.lower())\n    if distance_match:\n        params['distance'] = float(distance_match.group(1))\n\n    return params\n"})}),"\n",(0,t.jsx)(n.h2,{id:"ros-2-action-integration",children:"ROS 2 Action Integration"}),"\n",(0,t.jsx)(n.h3,{id:"creating-a-voice-command-node",children:"Creating a Voice Command Node"}),"\n",(0,t.jsxs)(o.A,{groupId:"voice-node-implementation",children:[(0,t.jsx)(s.A,{value:"full",label:"Complete Implementation",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom action_msgs.msg import GoalStatus\nimport whisper\nimport threading\n\nclass VoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_node\')\n\n        # Initialize Whisper model\n        self.model = whisper.load_model("base")\n\n        # Publishers and subscribers\n        self.cmd_vel_publisher = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.voice_subscriber = self.create_subscription(\n            String, \'voice_commands\', self.voice_callback, 10)\n\n        # Start audio capture in a separate thread\n        self.audio_thread = threading.Thread(target=self.start_audio_capture)\n        self.audio_thread.daemon = True\n        self.audio_thread.start()\n\n    def start_audio_capture(self):\n        """Continuously capture and process audio"""\n        while rclpy.ok():\n            # Capture audio and process with Whisper\n            audio_file = self.capture_audio()\n            result = self.model.transcribe(audio_file)\n            command_text = result["text"]\n\n            # Publish the recognized command\n            msg = String()\n            msg.data = command_text\n            self.voice_subscriber.publish(msg)\n\n    def voice_callback(self, msg):\n        """Process the recognized voice command"""\n        command = msg.data\n        self.get_logger().info(f\'Received command: {command}\')\n\n        # Recognize intent and execute action\n        intent = recognize_intent(command)\n        params = extract_command_parameters(command)\n\n        if intent == "navigation":\n            self.execute_navigation(params)\n        elif intent == "stop":\n            self.execute_stop()\n        # Add more intents as needed\n'})})}),(0,t.jsx)(s.A,{value:"publisher",label:"Publisher Setup",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Publishers and subscribers setup\nself.cmd_vel_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\nself.voice_subscriber = self.create_subscription(\n    String, 'voice_commands', self.voice_callback, 10)\n"})})}),(0,t.jsx)(s.A,{value:"callback",label:"Callback Logic",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def voice_callback(self, msg):\n    """Process the recognized voice command"""\n    command = msg.data\n    self.get_logger().info(f\'Received command: {command}\')\n\n    # Recognize intent and execute action\n    intent = recognize_intent(command)\n    params = extract_command_parameters(command)\n\n    if intent == "navigation":\n        self.execute_navigation(params)\n    elif intent == "stop":\n        self.execute_stop()\n    # Add more intents as needed\n'})})})]}),"\n",(0,t.jsx)(n.h2,{id:"advanced-voice-processing",children:"Advanced Voice Processing"}),"\n",(0,t.jsx)(n.h3,{id:"wake-word-detection",children:"Wake Word Detection"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def detect_wake_word(audio_data, wake_word="robot"):\n    """Detect wake word before processing full command"""\n    # Use a lightweight model for wake word detection\n    # to reduce computational load\n    pass\n'})}),"\n",(0,t.jsx)(n.h3,{id:"confidence-scoring",children:"Confidence Scoring"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def get_confidence_score(transcript, original_audio):\n    """Get confidence score for the transcription"""\n    # Whisper doesn\'t directly provide confidence scores\n    # but we can use alternative methods\n    pass\n'})}),"\n",(0,t.jsx)(n.h2,{id:"error-handling-and-robustness",children:"Error Handling and Robustness"}),"\n",(0,t.jsx)(n.h3,{id:"audio-quality-considerations",children:"Audio Quality Considerations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Background noise filtering"}),"\n",(0,t.jsx)(n.li,{children:"Microphone positioning"}),"\n",(0,t.jsx)(n.li,{children:"Audio format compatibility"}),"\n",(0,t.jsx)(n.li,{children:"Network latency for cloud processing"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"fallback-mechanisms",children:"Fallback Mechanisms"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def handle_recognition_error():\n    """Handle cases where speech recognition fails"""\n    # Request repetition\n    # Use alternative input method\n    # Provide error feedback\n    pass\n'})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrate OpenAI Whisper with ROS 2 for voice recognition"}),"\n",(0,t.jsx)(n.li,{children:"Design a voice-to-action pipeline for robotics"}),"\n",(0,t.jsx)(n.li,{children:"Process audio input and convert it to ROS 2 commands"}),"\n",(0,t.jsx)(n.li,{children:"Handle errors and edge cases in voice processing"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,t.jsx)(n.p,{children:'Implement a simple voice command system that listens for basic navigation commands ("move forward", "turn left", etc.) and converts them to Twist messages for robot navigation.'})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},9365:(e,n,i)=>{i.d(n,{A:()=>o});i(6540);var a=i(4164);const t={tabItem:"tabItem_Ymn6"};var r=i(4848);function o({children:e,hidden:n,className:i}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,a.A)(t.tabItem,i),hidden:n,children:e})}}}]);