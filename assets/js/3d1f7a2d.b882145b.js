"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[137],{7811:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>g,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module1/bridging-python-agents","title":"Chapter 6: Bridging Python Agents with ROS 2","description":"Introduction to Python Agent Integration","source":"@site/docs/module1/bridging-python-agents.md","sourceDirName":"module1","slug":"/module1/bridging-python-agents","permalink":"/hackathon-01-AI-textbook/docs/module1/bridging-python-agents","draft":false,"unlisted":false,"editUrl":"https://github.com/maheenali021/hackathon-01-AI-textbook/tree/main/docs/module1/bridging-python-agents.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"Chapter 6: Bridging Python Agents with ROS 2"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: Services and Actions for Robot Control","permalink":"/hackathon-01-AI-textbook/docs/module1/services-actions-control"},"next":{"title":"Chapter 7: Understanding URDF for Humanoid Robots","permalink":"/hackathon-01-AI-textbook/docs/module1/understanding-urdf"}}');var a=r(4848),i=r(8453);const s={sidebar_position:6,title:"Chapter 6: Bridging Python Agents with ROS 2"},o="Bridging Python Agents with ROS 2",l={},c=[{value:"Introduction to Python Agent Integration",id:"introduction-to-python-agent-integration",level:2},{value:"AI Agent Architecture with ROS 2",id:"ai-agent-architecture-with-ros-2",level:2},{value:"Agent-Environment Interface",id:"agent-environment-interface",level:3},{value:"Integration with Popular AI Libraries",id:"integration-with-popular-ai-libraries",level:2},{value:"Using OpenAI Gym for Training",id:"using-openai-gym-for-training",level:3},{value:"Integration with Transformers",id:"integration-with-transformers",level:3},{value:"Advanced Agent Patterns",id:"advanced-agent-patterns",level:2},{value:"Hierarchical Agent Architecture",id:"hierarchical-agent-architecture",level:3},{value:"Multi-Agent Coordination",id:"multi-agent-coordination",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Threading and Concurrency",id:"threading-and-concurrency",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2}];function d(n){const e={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"bridging-python-agents-with-ros-2",children:"Bridging Python Agents with ROS 2"})}),"\n",(0,a.jsx)(e.h2,{id:"introduction-to-python-agent-integration",children:"Introduction to Python Agent Integration"}),"\n",(0,a.jsx)(e.p,{children:"Modern robotics increasingly involves AI agents that can reason, plan, and make decisions. This chapter covers how to bridge Python-based AI agents with ROS 2 for intelligent robot control."}),"\n",(0,a.jsx)(e.h2,{id:"ai-agent-architecture-with-ros-2",children:"AI Agent Architecture with ROS 2"}),"\n",(0,a.jsx)(e.h3,{id:"agent-environment-interface",children:"Agent-Environment Interface"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import LaserScan, Image\r\nfrom geometry_msgs.msg import Twist\r\nimport numpy as np\r\nimport json\r\n\r\nclass AgentBridgeNode(Node):\r\n    def __init__(self):\r\n        super().__init__('agent_bridge_node')\r\n\r\n        # Publishers for robot commands\r\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n\r\n        # Subscribers for sensor data\r\n        self.laser_sub = self.create_subscription(\r\n            LaserScan, '/scan', self.laser_callback, 10)\r\n        self.image_sub = self.create_subscription(\r\n            Image, '/camera/image_raw', self.image_callback, 10)\r\n\r\n        # Agent state\r\n        self.sensors = {\r\n            'laser': None,\r\n            'image': None,\r\n            'position': None\r\n        }\r\n\r\n        # Timer for agent execution\r\n        self.agent_timer = self.create_timer(0.1, self.agent_step)\r\n\r\n        self.get_logger().info('Agent bridge initialized')\r\n\r\n    def laser_callback(self, msg):\r\n        \"\"\"Process laser scan data for the agent\"\"\"\r\n        self.sensors['laser'] = {\r\n            'ranges': list(msg.ranges),\r\n            'min_range': msg.range_min,\r\n            'max_range': msg.range_max\r\n        }\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process image data for the agent\"\"\"\r\n        # Convert ROS Image to numpy array for processing\r\n        # (simplified - actual conversion depends on encoding)\r\n        self.sensors['image'] = {\r\n            'width': msg.width,\r\n            'height': msg.height,\r\n            'encoding': msg.encoding\r\n        }\r\n\r\n    def agent_step(self):\r\n        \"\"\"Execute one step of the AI agent\"\"\"\r\n        if all(self.sensors.values()):  # All sensors have data\r\n            # Get action from agent\r\n            action = self.run_agent_policy(self.sensors)\r\n\r\n            # Execute action\r\n            self.execute_action(action)\r\n\r\n    def run_agent_policy(self, observation):\r\n        \"\"\"Placeholder for AI agent policy - in real implementation, this would use ML models\"\"\"\r\n        # Simple obstacle avoidance policy\r\n        if self.sensors['laser']:\r\n            min_distance = min(self.sensors['laser']['ranges'])\r\n            if min_distance < 0.5:  # Obstacle too close\r\n                return {'linear': 0.0, 'angular': 0.5}  # Turn\r\n            else:\r\n                return {'linear': 0.3, 'angular': 0.0}  # Move forward\r\n        return {'linear': 0.0, 'angular': 0.0}  # Stop\r\n\r\n    def execute_action(self, action):\r\n        \"\"\"Execute the action returned by the agent\"\"\"\r\n        cmd = Twist()\r\n        cmd.linear.x = action['linear']\r\n        cmd.angular.z = action['angular']\r\n        self.cmd_vel_pub.publish(cmd)\n"})}),"\n",(0,a.jsx)(e.h2,{id:"integration-with-popular-ai-libraries",children:"Integration with Popular AI Libraries"}),"\n",(0,a.jsx)(e.h3,{id:"using-openai-gym-for-training",children:"Using OpenAI Gym for Training"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import gym\r\nfrom gym import spaces\r\nimport numpy as np\r\n\r\nclass ROS2GymEnvironment(gym.Env):\r\n    """Gym environment wrapper for ROS 2 robot control"""\r\n\r\n    def __init__(self, node):\r\n        super(ROS2GymEnvironment, self).__init__()\r\n\r\n        # Define action and observation spaces\r\n        self.action_space = spaces.Box(\r\n            low=np.array([-1.0, -1.0]),\r\n            high=np.array([1.0, 1.0]),\r\n            dtype=np.float32\r\n        )\r\n\r\n        self.observation_space = spaces.Box(\r\n            low=-np.inf,\r\n            high=np.inf,\r\n            shape=(360,),  # 360 laser ranges\r\n            dtype=np.float32\r\n        )\r\n\r\n        self.node = node\r\n        self.current_obs = None\r\n\r\n    def step(self, action):\r\n        # Execute action in ROS 2\r\n        cmd = Twist()\r\n        cmd.linear.x = action[0]\r\n        cmd.angular.z = action[1]\r\n        self.node.cmd_vel_pub.publish(cmd)\r\n\r\n        # Wait for next observation\r\n        self.current_obs = self.node.sensors[\'laser\'][\'ranges\'] if self.node.sensors[\'laser\'] else [0]*360\r\n\r\n        # Calculate reward (simple example)\r\n        reward = self.calculate_reward(action)\r\n        done = self.is_done()\r\n        info = {}\r\n\r\n        return np.array(self.current_obs), reward, done, info\r\n\r\n    def reset(self):\r\n        # Reset environment\r\n        return np.array(self.current_obs)\r\n\r\n    def calculate_reward(self, action):\r\n        """Calculate reward based on action and sensor data"""\r\n        # Simple reward: positive for moving forward when safe, negative for collisions\r\n        if self.current_obs and min(self.current_obs) < 0.3:  # Collision risk\r\n            return -10.0 if action[0] > 0 else 0.0  # Penalty for moving forward near obstacles\r\n        return action[0] * 0.1  # Small reward for forward movement\r\n\r\n    def is_done(self):\r\n        """Check if episode is done"""\r\n        if self.current_obs and min(self.current_obs) < 0.2:  # Collision\r\n            return True\r\n        return False\n'})}),"\n",(0,a.jsx)(e.h3,{id:"integration-with-transformers",children:"Integration with Transformers"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from transformers import pipeline\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\n\r\nclass NLPRobotController(Node):\r\n    def __init__(self):\r\n        super().__init__('nlp_robot_controller')\r\n\r\n        # Initialize NLP pipeline\r\n        self.nlp_pipeline = pipeline(\"text-classification\",\r\n                                   model=\"distilbert-base-uncased-finetuned-sst-2-english\")\r\n\r\n        # Command recognition\r\n        self.command_sub = self.create_subscription(\r\n            String, 'voice_commands', self.command_callback, 10)\r\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n\r\n        # Define command mappings\r\n        self.command_map = {\r\n            'forward': {'linear': 0.5, 'angular': 0.0},\r\n            'backward': {'linear': -0.5, 'angular': 0.0},\r\n            'left': {'linear': 0.0, 'angular': 0.5},\r\n            'right': {'linear': 0.0, 'angular': -0.5},\r\n            'stop': {'linear': 0.0, 'angular': 0.0}\r\n        }\r\n\r\n    def command_callback(self, msg):\r\n        \"\"\"Process natural language commands\"\"\"\r\n        command_text = msg.data.lower()\r\n\r\n        # Simple keyword matching (in practice, use more sophisticated NLP)\r\n        recognized_command = self.recognize_command(command_text)\r\n\r\n        if recognized_command in self.command_map:\r\n            action = self.command_map[recognized_command]\r\n            self.execute_command(action)\r\n            self.get_logger().info(f'Executed command: {recognized_command}')\r\n        else:\r\n            self.get_logger().info(f'Unknown command: {command_text}')\r\n\r\n    def recognize_command(self, text):\r\n        \"\"\"Simple command recognition\"\"\"\r\n        if 'forward' in text or 'go' in text or 'move' in text:\r\n            return 'forward'\r\n        elif 'backward' in text or 'back' in text:\r\n            return 'backward'\r\n        elif 'left' in text:\r\n            return 'left'\r\n        elif 'right' in text:\r\n            return 'right'\r\n        elif 'stop' in text or 'halt' in text:\r\n            return 'stop'\r\n        return 'unknown'\n"})}),"\n",(0,a.jsx)(e.h2,{id:"advanced-agent-patterns",children:"Advanced Agent Patterns"}),"\n",(0,a.jsx)(e.h3,{id:"hierarchical-agent-architecture",children:"Hierarchical Agent Architecture"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class HierarchicalAgent(Node):\r\n    def __init__(self):\r\n        super().__init__('hierarchical_agent')\r\n\r\n        # High-level planner\r\n        self.planner = HighLevelPlanner()\r\n\r\n        # Low-level controller\r\n        self.controller = LowLevelController()\r\n\r\n        # State machine\r\n        self.current_state = 'IDLE'\r\n        self.goal = None\r\n\r\n        self.state_timer = self.create_timer(0.1, self.state_machine)\r\n\r\n    def state_machine(self):\r\n        \"\"\"Main state machine for hierarchical agent\"\"\"\r\n        if self.current_state == 'IDLE':\r\n            new_goal = self.planner.get_next_goal()\r\n            if new_goal:\r\n                self.goal = new_goal\r\n                self.current_state = 'NAVIGATING'\r\n\r\n        elif self.current_state == 'NAVIGATING':\r\n            status = self.controller.navigate_to(self.goal)\r\n            if status == 'SUCCESS':\r\n                self.current_state = 'IDLE'\r\n            elif status == 'FAILED':\r\n                self.current_state = 'IDLE'  # Retry or plan alternative\n"})}),"\n",(0,a.jsx)(e.h3,{id:"multi-agent-coordination",children:"Multi-Agent Coordination"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from std_msgs.msg import String\r\nimport json\r\n\r\nclass MultiAgentCoordinator(Node):\r\n    def __init__(self):\r\n        super().__init__('multi_agent_coordinator')\r\n\r\n        # Communication with other agents\r\n        self.agent_status_sub = self.create_subscription(\r\n            String, 'agent_status', self.agent_status_callback, 10)\r\n        self.task_assignment_pub = self.create_publisher(\r\n            String, 'task_assignment', 10)\r\n\r\n        # Agent registry\r\n        self.agents = {}\r\n        self.tasks = []\r\n\r\n    def agent_status_callback(self, msg):\r\n        \"\"\"Process status from other agents\"\"\"\r\n        status = json.loads(msg.data)\r\n        agent_id = status['agent_id']\r\n        self.agents[agent_id] = status\r\n\r\n    def assign_task(self, task):\r\n        \"\"\"Assign task to most suitable agent\"\"\"\r\n        best_agent = self.find_best_agent_for_task(task)\r\n        if best_agent:\r\n            assignment = {\r\n                'agent_id': best_agent,\r\n                'task': task\r\n            }\r\n            assignment_msg = String()\r\n            assignment_msg.data = json.dumps(assignment)\r\n            self.task_assignment_pub.publish(assignment_msg)\n"})}),"\n",(0,a.jsx)(e.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,a.jsx)(e.h3,{id:"threading-and-concurrency",children:"Threading and Concurrency"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import threading\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\nclass ThreadedAgent(Node):\r\n    def __init__(self):\r\n        super().__init__(\'threaded_agent\')\r\n\r\n        # Use thread pool for AI computations\r\n        self.executor = ThreadPoolExecutor(max_workers=2)\r\n\r\n        # Thread-safe data structures\r\n        self.observation_lock = threading.Lock()\r\n        self.current_observation = None\r\n\r\n    def async_ai_processing(self, observation):\r\n        """Run AI processing in separate thread"""\r\n        future = self.executor.submit(self.ai_model.predict, observation)\r\n        return future\n'})}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Bridge Python AI agents with ROS 2 systems"}),"\n",(0,a.jsx)(e.li,{children:"Integrate popular AI libraries with ROS 2"}),"\n",(0,a.jsx)(e.li,{children:"Implement hierarchical agent architectures"}),"\n",(0,a.jsx)(e.li,{children:"Design multi-agent coordination systems"}),"\n",(0,a.jsx)(e.li,{children:"Consider performance implications of AI integration"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,a.jsx)(e.p,{children:"Create a simple AI agent that uses sensor data to navigate a room and avoid obstacles. Implement both a basic rule-based agent and explore using a machine learning model for decision making."}),"\n",(0,a.jsx)(e.admonition,{type:"tip",children:(0,a.jsxs)(e.p,{children:["Use ",(0,a.jsx)(e.code,{children:"rclpy.qos.QoSProfile"})," with appropriate settings to balance performance and reliability when bridging AI agents with ROS 2."]})}),"\n",(0,a.jsx)(e.admonition,{type:"warning",children:(0,a.jsx)(e.p,{children:"Be mindful of computational resources when running AI models on robots. Consider the trade-off between intelligence and real-time performance."})})]})}function g(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>o});var t=r(6540);const a={},i=t.createContext(a);function s(n){const e=t.useContext(i);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),t.createElement(i.Provider,{value:e},n.children)}}}]);