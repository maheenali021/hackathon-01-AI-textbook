"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[37],{4357:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"capstone/simulation-testing","title":"Chapter 2: Simulation and Testing","description":"Run Full Humanoid Simulation with Commands, Navigation, and Object Interaction","source":"@site/docs/capstone/simulation-testing.md","sourceDirName":"capstone","slug":"/capstone/simulation-testing","permalink":"/hackathon-01-AI-textbook/docs/capstone/simulation-testing","draft":false,"unlisted":false,"editUrl":"https://github.com/maheenali021/hackathon-01-AI-textbook/tree/main/docs/capstone/simulation-testing.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Chapter 2: Simulation and Testing"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Capstone Overview","permalink":"/hackathon-01-AI-textbook/docs/capstone/capstone-overview"},"next":{"title":"Chapter 3: Best Practices for Physical AI & Humanoid Robotics","permalink":"/hackathon-01-AI-textbook/docs/capstone/best-practices"}}');var o=i(4848),s=i(8453);const a={sidebar_position:2,title:"Chapter 2: Simulation and Testing"},l="Simulation and Testing",r={},c=[{value:"Run Full Humanoid Simulation with Commands, Navigation, and Object Interaction",id:"run-full-humanoid-simulation-with-commands-navigation-and-object-interaction",level:2},{value:"Simulation Environment Setup",id:"simulation-environment-setup",level:2},{value:"Selecting the Right Simulation Platform",id:"selecting-the-right-simulation-platform",level:3},{value:"Gazebo",id:"gazebo",level:4},{value:"NVIDIA Isaac Sim",id:"nvidia-isaac-sim",level:4},{value:"Unity Robotics Simulation",id:"unity-robotics-simulation",level:4},{value:"Complete Humanoid Robot Model",id:"complete-humanoid-robot-model",level:2},{value:"Robot Description (URDF/SDF)",id:"robot-description-urdfsdf",level:3},{value:"Example URDF Snippet",id:"example-urdf-snippet",level:3},{value:"Integration of All Modules",id:"integration-of-all-modules",level:2},{value:"ROS 2 Communication Framework",id:"ros-2-communication-framework",level:3},{value:"Perception Integration",id:"perception-integration",level:3},{value:"Action Execution",id:"action-execution",level:3},{value:"Simulation Scenarios",id:"simulation-scenarios",level:2},{value:"Scenario 1: Fetch and Carry Task",id:"scenario-1-fetch-and-carry-task",level:3},{value:"Scenario 2: Room Navigation and Mapping",id:"scenario-2-room-navigation-and-mapping",level:3},{value:"Testing Framework",id:"testing-framework",level:2},{value:"Unit Testing",id:"unit-testing",level:3},{value:"Integration Testing",id:"integration-testing",level:3},{value:"System Testing",id:"system-testing",level:3},{value:"Implementation Example: Voice-Controlled Navigation",id:"implementation-example-voice-controlled-navigation",level:2},{value:"ROS 2 Node Structure",id:"ros-2-node-structure",level:3},{value:"Performance Metrics",id:"performance-metrics",level:2},{value:"Quantitative Metrics",id:"quantitative-metrics",level:3},{value:"Qualitative Metrics",id:"qualitative-metrics",level:3},{value:"Debugging and Visualization",id:"debugging-and-visualization",level:2},{value:"RViz2 Integration",id:"rviz2-integration",level:3},{value:"Logging Strategy",id:"logging-strategy",level:3},{value:"Safety Considerations",id:"safety-considerations",level:2},{value:"Emergency Stop Mechanisms",id:"emergency-stop-mechanisms",level:3},{value:"Fail-Safe Behaviors",id:"fail-safe-behaviors",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"simulation-and-testing",children:"Simulation and Testing"})}),"\n",(0,o.jsx)(e.h2,{id:"run-full-humanoid-simulation-with-commands-navigation-and-object-interaction",children:"Run Full Humanoid Simulation with Commands, Navigation, and Object Interaction"}),"\n",(0,o.jsx)(e.p,{children:"This chapter focuses on implementing and testing a complete humanoid robot system in simulation, integrating all the components developed in previous modules. We'll use Gazebo and/or NVIDIA Isaac Sim for the simulation environment and test the complete pipeline from perception to action."}),"\n",(0,o.jsx)(e.h2,{id:"simulation-environment-setup",children:"Simulation Environment Setup"}),"\n",(0,o.jsx)(e.h3,{id:"selecting-the-right-simulation-platform",children:"Selecting the Right Simulation Platform"}),"\n",(0,o.jsx)(e.p,{children:"For our complete humanoid robot system, we have several simulation options:"}),"\n",(0,o.jsx)(e.h4,{id:"gazebo",children:"Gazebo"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Pros"}),": Mature, well-integrated with ROS/ROS 2, extensive model database"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Cons"}),": Less photorealistic than Isaac Sim, less suitable for perception testing"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Best for"}),": Basic navigation, path planning, basic manipulation testing"]}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"nvidia-isaac-sim",children:"NVIDIA Isaac Sim"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Pros"}),": Photorealistic rendering, synthetic data generation, GPU-accelerated"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Cons"}),": More resource-intensive, requires NVIDIA hardware for full acceleration"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Best for"}),": Perception system testing, domain randomization, sensor simulation"]}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"unity-robotics-simulation",children:"Unity Robotics Simulation"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Pros"}),": High-fidelity visualization, good for human-robot interaction scenarios"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Cons"}),": Different workflow, requires Unity licensing for commercial use"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Best for"}),": Social robotics, human-robot interaction testing"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"complete-humanoid-robot-model",children:"Complete Humanoid Robot Model"}),"\n",(0,o.jsx)(e.h3,{id:"robot-description-urdfsdf",children:"Robot Description (URDF/SDF)"}),"\n",(0,o.jsx)(e.p,{children:"For our simulation, we'll need a humanoid robot model with:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Base"}),": Mobile base with appropriate locomotion (wheels, legs)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Torso"}),": Upper body with degrees of freedom for movement"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Arms"}),": Manipulator arms with end effectors (hands/grippers)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Head"}),": Camera systems and sensors"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sensors"}),": IMU, cameras, LiDAR, force/torque sensors"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"example-urdf-snippet",children:"Example URDF Snippet"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="humanoid_robot">\n  \x3c!-- Base link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <cylinder length="0.2" radius="0.3"/>\n      </geometry>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.2" radius="0.3"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="10"/>\n      <inertia ixx="1.0" ixy="0.0" ixz="0.0" iyy="1.0" iyz="0.0" izz="1.0"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Additional links and joints for the humanoid structure --\x3e\n  \x3c!-- Arms, legs, head, sensors, etc. --\x3e\n</robot>\n'})}),"\n",(0,o.jsx)(e.h2,{id:"integration-of-all-modules",children:"Integration of All Modules"}),"\n",(0,o.jsx)(e.h3,{id:"ros-2-communication-framework",children:"ROS 2 Communication Framework"}),"\n",(0,o.jsx)(e.p,{children:"The complete system will use ROS 2 to coordinate between different subsystems:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"Voice Commands \u2192 NLP \u2192 Task Planner \u2192 Navigation \u2192 Manipulation \u2192 Robot Control\n     \u2193              \u2193         \u2193           \u2193           \u2193            \u2193\n  Whisper      Transformers  MoveIt     Navigation  Grasp Lib   Hardware Interface\n"})}),"\n",(0,o.jsx)(e.h3,{id:"perception-integration",children:"Perception Integration"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Vision"}),": Object detection, scene understanding using Isaac ROS packages"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Localization"}),": SLAM algorithms using Isaac ROS Visual SLAM"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Environment Mapping"}),": Creating and updating maps for navigation"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"action-execution",children:"Action Execution"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Navigation"}),": Path planning and execution using Navigation2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Manipulation"}),": Grasping and manipulation using MoveIt2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Interaction"}),": Natural language responses and feedback"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"simulation-scenarios",children:"Simulation Scenarios"}),"\n",(0,o.jsx)(e.h3,{id:"scenario-1-fetch-and-carry-task",children:"Scenario 1: Fetch and Carry Task"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Objective"}),": Robot receives a voice command to fetch an object and deliver it to a specified location."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Steps"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:'Voice command: "Robot, please bring me the red cup from the kitchen table"'}),"\n",(0,o.jsx)(e.li,{children:"NLP: Parse command, identify object (red cup), location (kitchen table)"}),"\n",(0,o.jsx)(e.li,{children:"Navigation: Plan path to kitchen"}),"\n",(0,o.jsx)(e.li,{children:"Perception: Detect and identify the red cup"}),"\n",(0,o.jsx)(e.li,{children:"Manipulation: Approach and grasp the cup"}),"\n",(0,o.jsx)(e.li,{children:"Navigation: Plan path to user's location"}),"\n",(0,o.jsx)(e.li,{children:"Interaction: Deliver cup and provide feedback"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"scenario-2-room-navigation-and-mapping",children:"Scenario 2: Room Navigation and Mapping"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Objective"}),": Robot maps a new room while avoiding obstacles and reporting its progress."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Steps"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:'Voice command: "Map the living room and report obstacles"'}),"\n",(0,o.jsx)(e.li,{children:"Navigation: Execute exploration algorithm"}),"\n",(0,o.jsx)(e.li,{children:"Perception: Build map and detect obstacles"}),"\n",(0,o.jsx)(e.li,{children:"Interaction: Report obstacles and mapping progress"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"testing-framework",children:"Testing Framework"}),"\n",(0,o.jsx)(e.h3,{id:"unit-testing",children:"Unit Testing"}),"\n",(0,o.jsx)(e.p,{children:"Test individual components:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Voice recognition accuracy"}),"\n",(0,o.jsx)(e.li,{children:"NLP intent classification"}),"\n",(0,o.jsx)(e.li,{children:"Navigation path planning"}),"\n",(0,o.jsx)(e.li,{children:"Manipulation grasp planning"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,o.jsx)(e.p,{children:"Test component interactions:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Voice command to navigation execution"}),"\n",(0,o.jsx)(e.li,{children:"Perception to manipulation pipeline"}),"\n",(0,o.jsx)(e.li,{children:"Sensor fusion effectiveness"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"system-testing",children:"System Testing"}),"\n",(0,o.jsx)(e.p,{children:"Test complete system behavior:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"End-to-end task execution"}),"\n",(0,o.jsx)(e.li,{children:"Error handling and recovery"}),"\n",(0,o.jsx)(e.li,{children:"Performance under varying conditions"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"implementation-example-voice-controlled-navigation",children:"Implementation Example: Voice-Controlled Navigation"}),"\n",(0,o.jsx)(e.h3,{id:"ros-2-node-structure",children:"ROS 2 Node Structure"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom move_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\n\nclass HumanoidCommander(Node):\n    def __init__(self):\n        super().__init__('humanoid_commander')\n\n        # Subscriptions\n        self.voice_sub = self.create_subscription(\n            String, 'voice_commands', self.voice_callback, 10)\n\n        # Action clients\n        self.nav_client = ActionClient(\n            self, NavigateToPose, 'navigate_to_pose')\n\n    def voice_callback(self, msg):\n        command = msg.data\n        # Parse command and execute appropriate action\n        if \"go to\" in command:\n            self.parse_navigation_command(command)\n        elif \"pick up\" in command:\n            self.parse_manipulation_command(command)\n\n    def parse_navigation_command(self, command):\n        # Extract destination from command\n        destination = self.extract_location(command)\n\n        # Create navigation goal\n        goal = NavigateToPose.Goal()\n        goal.pose = self.get_pose_for_location(destination)\n\n        # Send navigation goal\n        self.nav_client.send_goal_async(goal)\n"})}),"\n",(0,o.jsx)(e.h2,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,o.jsx)(e.h3,{id:"quantitative-metrics",children:"Quantitative Metrics"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Task Completion Rate"}),": Percentage of tasks successfully completed"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Navigation Success Rate"}),": Percentage of navigation goals reached"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Object Recognition Accuracy"}),": Percentage of objects correctly identified"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Response Time"}),": Average time from command to action initiation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Path Efficiency"}),": Ratio of optimal path to actual path length"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"qualitative-metrics",children:"Qualitative Metrics"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Naturalness of Interaction"}),": How natural does the voice interaction feel?"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Robustness"}),": How well does the system handle unexpected situations?"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Safety"}),": Does the system operate safely in human environments?"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"debugging-and-visualization",children:"Debugging and Visualization"}),"\n",(0,o.jsx)(e.h3,{id:"rviz2-integration",children:"RViz2 Integration"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Visualize robot state and sensor data"}),"\n",(0,o.jsx)(e.li,{children:"Display navigation paths and goals"}),"\n",(0,o.jsx)(e.li,{children:"Show object detection results"}),"\n",(0,o.jsx)(e.li,{children:"Monitor system status"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"logging-strategy",children:"Logging Strategy"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Log voice commands and interpretations"}),"\n",(0,o.jsx)(e.li,{children:"Record navigation and manipulation attempts"}),"\n",(0,o.jsx)(e.li,{children:"Track system state changes"}),"\n",(0,o.jsx)(e.li,{children:"Monitor performance metrics"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,o.jsx)(e.h3,{id:"emergency-stop-mechanisms",children:"Emergency Stop Mechanisms"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Implement software-based emergency stops"}),"\n",(0,o.jsx)(e.li,{children:"Use safety-rated hardware for critical functions"}),"\n",(0,o.jsx)(e.li,{children:"Monitor for collision risks"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"fail-safe-behaviors",children:"Fail-Safe Behaviors"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Return to safe position on error"}),"\n",(0,o.jsx)(e.li,{children:"Provide clear feedback when commands cannot be executed"}),"\n",(0,o.jsx)(e.li,{children:"Gracefully degrade functionality rather than failing completely"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Set up a complete humanoid robot simulation environment"}),"\n",(0,o.jsx)(e.li,{children:"Integrate all modules (ROS 2, Digital Twin, AI-Robot Brain, VLA) in simulation"}),"\n",(0,o.jsx)(e.li,{children:"Test the complete pipeline from voice command to robot action"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate system performance using appropriate metrics"}),"\n",(0,o.jsx)(e.li,{children:"Implement safety mechanisms for robot operation"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,o.jsx)(e.p,{children:"Set up a simple simulation scenario where your robot can respond to basic voice commands to navigate to specific locations in a Gazebo environment. Test the complete pipeline from speech recognition to navigation execution."})]})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>l});var t=i(6540);const o={},s=t.createContext(o);function a(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);