"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[466],{2499:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module4/multi-modal-integration","title":"Chapter 4: Multi-Modal Integration for VLA Systems","description":"Introduction to Multi-Modal AI","source":"@site/docs/module4/multi-modal-integration.md","sourceDirName":"module4","slug":"/module4/multi-modal-integration","permalink":"/hackathon-01-AI-textbook/docs/module4/multi-modal-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/maheenali021/hackathon-01-AI-textbook/tree/main/docs/module4/multi-modal-integration.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Chapter 4: Multi-Modal Integration for VLA Systems"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Cognitive Planning with Large Language Models","permalink":"/hackathon-01-AI-textbook/docs/module4/cognitive-planning"},"next":{"title":"Module 4 Milestone Project: Vision-Language-Action Robot Assistant","permalink":"/hackathon-01-AI-textbook/docs/module4/module4-milestone"}}');var o=t(4848),s=t(8453);const a={sidebar_position:4,title:"Chapter 4: Multi-Modal Integration for VLA Systems"},r="Multi-Modal Integration for VLA Systems",c={},l=[{value:"Introduction to Multi-Modal AI",id:"introduction-to-multi-modal-ai",level:2},{value:"Multi-Modal Architecture",id:"multi-modal-architecture",level:2},{value:"VLA System Design",id:"vla-system-design",level:3},{value:"Vision Processing System",id:"vision-processing-system",level:2},{value:"Advanced Computer Vision Pipeline",id:"advanced-computer-vision-pipeline",level:3},{value:"Language Processing System",id:"language-processing-system",level:2},{value:"Natural Language Understanding",id:"natural-language-understanding",level:3},{value:"Action Planning System",id:"action-planning-system",level:2},{value:"Multi-Modal Action Planning",id:"multi-modal-action-planning",level:3},{value:"Memory and Context Systems",id:"memory-and-context-systems",level:2},{value:"Perception Memory",id:"perception-memory",level:3},{value:"Robot State Management",id:"robot-state-management",level:2},{value:"State Representation",id:"state-representation",level:3},{value:"Integration and Coordination",id:"integration-and-coordination",level:2},{value:"Multi-Modal Fusion",id:"multi-modal-fusion",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"multi-modal-integration-for-vla-systems",children:"Multi-Modal Integration for VLA Systems"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction-to-multi-modal-ai",children:"Introduction to Multi-Modal AI"}),"\n",(0,o.jsx)(n.p,{children:"Vision-Language-Action (VLA) systems integrate multiple sensory modalities to create intelligent robotic agents capable of understanding and interacting with the world through natural language commands. This chapter explores how to effectively combine vision, language, and action systems."}),"\n",(0,o.jsx)(n.h2,{id:"multi-modal-architecture",children:"Multi-Modal Architecture"}),"\n",(0,o.jsx)(n.h3,{id:"vla-system-design",children:"VLA System Design"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import Twist, Pose\nfrom vision_msgs.msg import Detection2DArray\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\nimport threading\nfrom queue import Queue, Empty\nfrom typing import Dict, List, Any, Optional\nimport json\n\nclass VLAMultiModalNode(Node):\n    def __init__(self):\n        super().__init__('vla_multi_modal_node')\n\n        # Initialize components\n        self.bridge = CvBridge()\n\n        # Vision processing\n        self.vision_processor = VisionProcessor(self)\n        self.perception_memory = PerceptionMemory()\n\n        # Language processing\n        self.language_processor = LanguageProcessor(self)\n        self.conversation_history = []\n\n        # Action planning\n        self.action_planner = ActionPlanner(self)\n        self.robot_state = RobotState()\n\n        # Publishers and subscribers\n        self.setup_subscribers()\n        self.setup_publishers()\n\n        # Processing queues\n        self.vision_queue = Queue(maxsize=10)\n        self.language_queue = Queue(maxsize=10)\n        self.action_queue = Queue(maxsize=10)\n\n        # Processing threads\n        self.vision_thread = threading.Thread(target=self.process_vision, daemon=True)\n        self.language_thread = threading.Thread(target=self.process_language, daemon=True)\n        self.action_thread = threading.Thread(target=self.process_actions, daemon=True)\n\n        # Start processing threads\n        self.vision_thread.start()\n        self.language_thread.start()\n        self.action_thread.start()\n\n        self.get_logger().info('VLA Multi-Modal Node initialized')\n\n    def setup_subscribers(self):\n        \"\"\"Setup all subscribers for multi-modal input\"\"\"\n        # Camera image subscription\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n\n        # Camera info subscription\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, '/camera/camera_info', self.camera_info_callback, 10)\n\n        # Language command subscription\n        self.command_sub = self.create_subscription(\n            String, '/voice_commands', self.command_callback, 10)\n\n        # Robot state subscription\n        self.state_sub = self.create_subscription(\n            String, '/robot_state', self.state_callback, 10)\n\n    def setup_publishers(self):\n        \"\"\"Setup publishers for multi-modal output\"\"\"\n        self.vision_pub = self.create_publisher(Detection2DArray, '/vision_detections', 10)\n        self.response_pub = self.create_publisher(String, '/vla_response', 10)\n        self.action_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.status_pub = self.create_publisher(String, '/vla_status', 10)\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming image data\"\"\"\n        try:\n            if not self.vision_queue.full():\n                self.vision_queue.put({\n                    'image': msg,\n                    'timestamp': msg.header.stamp,\n                    'seq': msg.header.seq\n                })\n        except Exception as e:\n            self.get_logger().error(f'Error queuing image: {e}')\n\n    def command_callback(self, msg):\n        \"\"\"Process incoming language command\"\"\"\n        try:\n            if not self.language_queue.full():\n                self.language_queue.put({\n                    'command': msg.data,\n                    'timestamp': self.get_clock().now().to_msg()\n                })\n        except Exception as e:\n            self.get_logger().error(f'Error queuing command: {e}')\n\n    def state_callback(self, msg):\n        \"\"\"Process robot state updates\"\"\"\n        try:\n            state_data = json.loads(msg.data)\n            self.robot_state.update_state(state_data)\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid robot state JSON')\n\n    def camera_info_callback(self, msg):\n        \"\"\"Process camera calibration information\"\"\"\n        self.vision_processor.update_camera_info(msg)\n\n    def process_vision(self):\n        \"\"\"Process vision data in separate thread\"\"\"\n        while rclpy.ok():\n            try:\n                data = self.vision_queue.get(timeout=0.1)\n\n                # Convert ROS image to OpenCV\n                cv_image = self.bridge.imgmsg_to_cv2(data['image'], desired_encoding='bgr8')\n\n                # Process with vision system\n                detections = self.vision_processor.process_image(cv_image)\n\n                # Update perception memory\n                self.perception_memory.update_scene(detections, data['timestamp'])\n\n                # Publish detections\n                detection_msg = self.vision_processor.create_detection_message(detections)\n                self.vision_pub.publish(detection_msg)\n\n            except Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f'Vision processing error: {e}')\n\n    def process_language(self):\n        \"\"\"Process language commands in separate thread\"\"\"\n        while rclpy.ok():\n            try:\n                data = self.language_queue.get(timeout=0.1)\n\n                # Process command with language system\n                interpretation = self.language_processor.interpret_command(\n                    data['command'], self.perception_memory.get_current_scene())\n\n                # Plan actions based on interpretation\n                action_plan = self.action_planner.create_plan(interpretation)\n\n                # Add to action queue\n                self.action_queue.put({\n                    'plan': action_plan,\n                    'original_command': data['command'],\n                    'timestamp': data['timestamp']\n                })\n\n                # Publish response\n                response_msg = String()\n                response_msg.data = f\"Understood command: {data['command']}. Executing...\"\n                self.response_pub.publish(response_msg)\n\n            except Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f'Language processing error: {e}')\n\n    def process_actions(self):\n        \"\"\"Execute actions in separate thread\"\"\"\n        while rclpy.ok():\n            try:\n                data = self.action_queue.get(timeout=0.1)\n\n                # Execute action plan\n                success = self.action_planner.execute_plan(data['plan'])\n\n                # Publish execution status\n                status_msg = String()\n                status_msg.data = f\"Command '{data['original_command']}' {'succeeded' if success else 'failed'}\"\n                self.status_pub.publish(status_msg)\n\n            except Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f'Action execution error: {e}')\n"})}),"\n",(0,o.jsx)(n.h2,{id:"vision-processing-system",children:"Vision Processing System"}),"\n",(0,o.jsx)(n.h3,{id:"advanced-computer-vision-pipeline",children:"Advanced Computer Vision Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import torch\nimport torchvision.transforms as transforms\nfrom PIL import Image as PILImage\nimport supervision as sv\nfrom ultralytics import YOLO\n\nclass VisionProcessor:\n    def __init__(self, node):\n        self.node = node\n\n        # Initialize vision models\n        self.detection_model = self.load_detection_model()\n        self.segmentation_model = self.load_segmentation_model()\n        self.depth_model = self.load_depth_model()\n\n        # Camera calibration\n        self.camera_matrix = None\n        self.dist_coeffs = None\n\n        # Transform for neural networks\n        self.transform = transforms.Compose([\n            transforms.Resize((640, 640)),\n            transforms.ToTensor(),\n        ])\n\n    def load_detection_model(self):\n        """Load object detection model"""\n        # Using YOLOv8 for real-time detection\n        try:\n            model = YOLO(\'yolov8n.pt\')  # You can change the model size\n            return model\n        except Exception as e:\n            self.node.get_logger().error(f\'Could not load detection model: {e}\')\n            return None\n\n    def load_segmentation_model(self):\n        """Load instance segmentation model"""\n        try:\n            model = YOLO(\'yolov8n-seg.pt\')\n            return model\n        except Exception as e:\n            self.node.get_logger().error(f\'Could not load segmentation model: {e}\')\n            return None\n\n    def load_depth_model(self):\n        """Load depth estimation model"""\n        # Placeholder for depth estimation\n        # In practice, you might use MiDaS or similar\n        return None\n\n    def process_image(self, cv_image):\n        """Process image and extract multi-modal features"""\n        results = []\n\n        if self.detection_model:\n            # Run detection\n            detections = self.detection_model(cv_image)\n\n            # Process detections\n            for det in detections:\n                boxes = det.boxes.xyxy.cpu().numpy() if det.boxes is not None else []\n                confidences = det.boxes.conf.cpu().numpy() if det.boxes is not None else []\n                class_ids = det.boxes.cls.cpu().numpy() if det.boxes is not None else []\n\n                for box, conf, class_id in zip(boxes, confidences, class_ids):\n                    x1, y1, x2, y2 = box\n                    detection = {\n                        \'bbox\': [int(x1), int(y1), int(x2-x1), int(y2-y1)],\n                        \'confidence\': float(conf),\n                        \'class_id\': int(class_id),\n                        \'class_name\': self.detection_model.names[int(class_id)],\n                        \'center\': [(x1+x2)/2, (y1+y2)/2],\n                        \'area\': (x2-x1) * (y2-y1)\n                    }\n                    results.append(detection)\n\n        return results\n\n    def update_camera_info(self, camera_info_msg):\n        """Update camera calibration parameters"""\n        self.camera_matrix = np.array(camera_info_msg.k).reshape(3, 3)\n        self.dist_coeffs = np.array(camera_info_msg.d)\n\n    def create_detection_message(self, detections):\n        """Create ROS message from detections"""\n        from vision_msgs.msg import Detection2DArray, Detection2D\n        from geometry_msgs.msg import Point\n        from std_msgs.msg import Header\n\n        detection_array = Detection2DArray()\n        detection_array.header.stamp = self.node.get_clock().now().to_msg()\n        detection_array.header.frame_id = \'camera_frame\'\n\n        for det in detections:\n            detection_msg = Detection2D()\n            detection_msg.header.stamp = detection_array.header.stamp\n            detection_msg.header.frame_id = detection_array.header.frame_id\n\n            # Bounding box\n            detection_msg.bbox.center.x = det[\'center\'][0]\n            detection_msg.bbox.center.y = det[\'center\'][1]\n            detection_msg.bbox.size_x = det[\'bbox\'][2]\n            detection_msg.bbox.size_y = det[\'bbox\'][3]\n\n            # Classification result\n            from vision_msgs.msg import ObjectHypothesisWithPose\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.id = det[\'class_id\']\n            hypothesis.score = det[\'confidence\']\n            detection_msg.results.append(hypothesis)\n\n            detection_array.detections.append(detection_msg)\n\n        return detection_array\n\n    def extract_visual_features(self, image):\n        """Extract visual features for multi-modal fusion"""\n        # Convert to tensor\n        pil_image = PILImage.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        tensor = self.transform(pil_image).unsqueeze(0)  # Add batch dimension\n\n        # Extract features using CNN backbone\n        with torch.no_grad():\n            features = self.detection_model.model.backbone(tensor)\n\n        return features\n'})}),"\n",(0,o.jsx)(n.h2,{id:"language-processing-system",children:"Language Processing System"}),"\n",(0,o.jsx)(n.h3,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from transformers import AutoTokenizer, AutoModel, pipeline\nimport openai\nfrom sentence_transformers import SentenceTransformer\n\nclass LanguageProcessor:\n    def __init__(self, node):\n        self.node = node\n\n        # Initialize language models\n        self.tokenizer = self.load_tokenizer()\n        self.encoder = self.load_encoder()\n        self.nlp_pipeline = self.setup_nlp_pipeline()\n\n        # Intent classification\n        self.intent_classifier = self.setup_intent_classifier()\n\n        # Named Entity Recognition\n        self.ner_pipeline = self.setup_ner_pipeline()\n\n    def load_tokenizer(self):\n        \"\"\"Load tokenizer for language processing\"\"\"\n        try:\n            tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n            return tokenizer\n        except Exception as e:\n            self.node.get_logger().error(f'Could not load tokenizer: {e}')\n            return None\n\n    def load_encoder(self):\n        \"\"\"Load sentence encoder for semantic understanding\"\"\"\n        try:\n            encoder = SentenceTransformer('all-MiniLM-L6-v2')\n            return encoder\n        except Exception as e:\n            self.node.get_logger().error(f'Could not load encoder: {e}')\n            return None\n\n    def setup_nlp_pipeline(self):\n        \"\"\"Setup NLP processing pipeline\"\"\"\n        try:\n            # Named Entity Recognition\n            ner = pipeline(\"ner\",\n                          model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n                          aggregation_strategy=\"simple\")\n\n            # Question Answering\n            qa = pipeline(\"question-answering\",\n                         model=\"deepset/roberta-base-squad2\")\n\n            return {\n                'ner': ner,\n                'qa': qa\n            }\n        except Exception as e:\n            self.node.get_logger().error(f'Could not setup NLP pipeline: {e}')\n            return None\n\n    def setup_intent_classifier(self):\n        \"\"\"Setup intent classification system\"\"\"\n        # In practice, you would train a custom classifier\n        # For now, using simple keyword matching\n        intent_keywords = {\n            'navigation': ['go', 'move', 'navigate', 'drive', 'walk', 'forward', 'backward', 'left', 'right'],\n            'manipulation': ['pick', 'grasp', 'lift', 'take', 'grab', 'hold', 'place', 'put', 'drop'],\n            'perception': ['see', 'find', 'locate', 'detect', 'look', 'show', 'identify', 'recognize'],\n            'communication': ['say', 'speak', 'tell', 'talk', 'answer', 'respond']\n        }\n        return intent_keywords\n\n    def setup_ner_pipeline(self):\n        \"\"\"Setup Named Entity Recognition pipeline\"\"\"\n        # Using spaCy-style entities for robotics\n        robot_entities = {\n            'LOCATION': ['kitchen', 'bedroom', 'office', 'living room', 'bathroom', 'garage'],\n            'OBJECT': ['cup', 'book', 'phone', 'keys', 'bottle', 'plate', 'fork', 'spoon'],\n            'ACTION': ['bring', 'fetch', 'carry', 'move', 'go', 'come', 'follow', 'stop']\n        }\n        return robot_entities\n\n    def interpret_command(self, command: str, scene_context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Interpret natural language command in scene context\"\"\"\n        # Extract intent\n        intent = self.classify_intent(command)\n\n        # Extract entities\n        entities = self.extract_entities(command)\n\n        # Ground entities in scene\n        grounded_entities = self.ground_entities(entities, scene_context)\n\n        # Create interpretation\n        interpretation = {\n            'command': command,\n            'intent': intent,\n            'entities': entities,\n            'grounded_entities': grounded_entities,\n            'scene_context': scene_context,\n            'timestamp': self.node.get_clock().now().to_msg()\n        }\n\n        return interpretation\n\n    def classify_intent(self, command: str) -> str:\n        \"\"\"Classify the intent of the command\"\"\"\n        command_lower = command.lower()\n\n        for intent, keywords in self.intent_classifier.items():\n            for keyword in keywords:\n                if keyword in command_lower:\n                    return intent\n\n        return 'unknown'\n\n    def extract_entities(self, command: str) -> Dict[str, List[str]]:\n        \"\"\"Extract named entities from command\"\"\"\n        entities = {\n            'locations': [],\n            'objects': [],\n            'actions': []\n        }\n\n        command_lower = command.lower()\n\n        # Extract locations\n        for location in self.ner_pipeline['LOCATION']:\n            if location.replace(' ', '') in command_lower.replace(' ', ''):\n                entities['locations'].append(location)\n\n        # Extract objects\n        for obj in self.ner_pipeline['OBJECT']:\n            if obj in command_lower:\n                entities['objects'].append(obj)\n\n        # Extract actions\n        for action in self.ner_pipeline['ACTION']:\n            if action in command_lower:\n                entities['actions'].append(action)\n\n        return entities\n\n    def ground_entities(self, entities: Dict[str, List[str]], scene_context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Ground entities in the current scene\"\"\"\n        grounded = {\n            'locations': {},\n            'objects': {},\n            'actions': entities['actions']  # Actions don't need grounding\n        }\n\n        # Ground locations (map to coordinates)\n        for location in entities['locations']:\n            if location in scene_context.get('known_locations', {}):\n                grounded['locations'][location] = scene_context['known_locations'][location]\n\n        # Ground objects (map to detected objects)\n        for obj in entities['objects']:\n            if 'detections' in scene_context:\n                for detection in scene_context['detections']:\n                    if detection.get('class_name', '').lower() == obj.lower():\n                        grounded['objects'][obj] = detection\n\n        return grounded\n"})}),"\n",(0,o.jsx)(n.h2,{id:"action-planning-system",children:"Action Planning System"}),"\n",(0,o.jsx)(n.h3,{id:"multi-modal-action-planning",children:"Multi-Modal Action Planning"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class ActionPlanner:\n    def __init__(self, node):\n        self.node = node\n        self.action_library = self.initialize_action_library()\n\n    def initialize_action_library(self):\n        \"\"\"Initialize library of available actions\"\"\"\n        return {\n            'navigate_to_location': {\n                'preconditions': ['robot_operational', 'location_known'],\n                'effects': ['robot_at_location'],\n                'parameters': ['target_location'],\n                'implementation': self.execute_navigation\n            },\n            'pick_up_object': {\n                'preconditions': ['object_detected', 'object_graspable', 'robot_at_object_location'],\n                'effects': ['object_grasped', 'object_not_at_original_location'],\n                'parameters': ['target_object'],\n                'implementation': self.execute_manipulation\n            },\n            'place_down_object': {\n                'preconditions': ['object_grasped'],\n                'effects': ['object_placed', 'object_not_grasped'],\n                'parameters': ['target_location'],\n                'implementation': self.execute_manipulation\n            },\n            'detect_object': {\n                'preconditions': ['camera_operational'],\n                'effects': ['object_detection_attempted'],\n                'parameters': ['target_object'],\n                'implementation': self.execute_perception\n            },\n            'communicate': {\n                'preconditions': ['speech_system_operational'],\n                'effects': ['message_delivered'],\n                'parameters': ['message'],\n                'implementation': self.execute_communication\n            }\n        }\n\n    def create_plan(self, interpretation: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Create action plan from interpretation\"\"\"\n        intent = interpretation['intent']\n        entities = interpretation['grounded_entities']\n\n        plan = []\n\n        if intent == 'navigation':\n            if entities['locations']:\n                for location, coords in entities['locations'].items():\n                    plan.append({\n                        'action': 'navigate_to_location',\n                        'parameters': {'target_location': location, 'coordinates': coords},\n                        'description': f'Navigate to {location}'\n                    })\n\n        elif intent == 'manipulation':\n            if entities['objects']:\n                for obj_name, obj_data in entities['objects'].items():\n                    # First navigate to object\n                    plan.append({\n                        'action': 'navigate_to_location',\n                        'parameters': {\n                            'target_location': 'object_location',\n                            'coordinates': obj_data['center']\n                        },\n                        'description': f'Navigate to {obj_name}'\n                    })\n\n                    # Then pick it up\n                    plan.append({\n                        'action': 'pick_up_object',\n                        'parameters': {'target_object': obj_name},\n                        'description': f'Pick up {obj_name}'\n                    })\n\n        elif intent == 'perception':\n            if entities['objects']:\n                for obj_name in entities['objects'].keys():\n                    plan.append({\n                        'action': 'detect_object',\n                        'parameters': {'target_object': obj_name},\n                        'description': f'Detect {obj_name}'\n                    })\n\n        elif intent == 'communication':\n            plan.append({\n                'action': 'communicate',\n                'parameters': {'message': interpretation['command']},\n                'description': f'Communicate: {interpretation[\"command\"]}'\n            })\n\n        return plan\n\n    def execute_plan(self, plan: List[Dict[str, Any]]) -> bool:\n        \"\"\"Execute the action plan\"\"\"\n        success = True\n\n        for step in plan:\n            action_name = step['action']\n            parameters = step['parameters']\n\n            if action_name in self.action_library:\n                action_def = self.action_library[action_name]\n\n                # Check preconditions\n                if not self.check_preconditions(action_def['preconditions'], parameters):\n                    self.node.get_logger().error(f'Preconditions not met for {action_name}')\n                    success = False\n                    break\n\n                # Execute action\n                action_success = action_def['implementation'](action_name, parameters)\n\n                if not action_success:\n                    self.node.get_logger().error(f'Action {action_name} failed')\n                    success = False\n                    break\n\n                # Update effects\n                self.update_effects(action_def['effects'], parameters)\n            else:\n                self.node.get_logger().error(f'Unknown action: {action_name}')\n                success = False\n                break\n\n        return success\n\n    def check_preconditions(self, preconditions: List[str], parameters: Dict[str, Any]) -> bool:\n        \"\"\"Check if preconditions are satisfied\"\"\"\n        # In practice, this would check robot state, sensor data, etc.\n        # For simulation, assume all preconditions are met\n        return True\n\n    def update_effects(self, effects: List[str], parameters: Dict[str, Any]):\n        \"\"\"Update system state based on action effects\"\"\"\n        # Update internal state based on action effects\n        pass\n\n    def execute_navigation(self, action_name: str, parameters: Dict[str, Any]) -> bool:\n        \"\"\"Execute navigation action\"\"\"\n        try:\n            target_location = parameters['target_location']\n            coordinates = parameters.get('coordinates')\n\n            self.node.get_logger().info(f'Navigating to {target_location}')\n\n            # In practice, this would send navigation command to navigation stack\n            # For simulation, return success\n            return True\n\n        except Exception as e:\n            self.node.get_logger().error(f'Navigation error: {e}')\n            return False\n\n    def execute_manipulation(self, action_name: str, parameters: Dict[str, Any]) -> bool:\n        \"\"\"Execute manipulation action\"\"\"\n        try:\n            if action_name == 'pick_up_object':\n                target_object = parameters['target_object']\n                self.node.get_logger().info(f'Attempting to pick up {target_object}')\n            elif action_name == 'place_down_object':\n                target_location = parameters['target_location']\n                self.node.get_logger().info(f'Attempting to place object at {target_location}')\n\n            # In practice, this would send commands to manipulator\n            # For simulation, return success\n            return True\n\n        except Exception as e:\n            self.node.get_logger().error(f'Manipulation error: {e}')\n            return False\n\n    def execute_perception(self, action_name: str, parameters: Dict[str, Any]) -> bool:\n        \"\"\"Execute perception action\"\"\"\n        try:\n            target_object = parameters['target_object']\n            self.node.get_logger().info(f'Attempting to detect {target_object}')\n\n            # In practice, this would trigger perception pipeline\n            # For simulation, return success\n            return True\n\n        except Exception as e:\n            self.node.get_logger().error(f'Perception error: {e}')\n            return False\n\n    def execute_communication(self, action_name: str, parameters: Dict[str, Any]) -> bool:\n        \"\"\"Execute communication action\"\"\"\n        try:\n            message = parameters['message']\n            self.node.get_logger().info(f'Communicating: {message}')\n\n            # In practice, this would trigger speech synthesis\n            # For simulation, return success\n            return True\n\n        except Exception as e:\n            self.node.get_logger().error(f'Communication error: {e}')\n            return False\n"})}),"\n",(0,o.jsx)(n.h2,{id:"memory-and-context-systems",children:"Memory and Context Systems"}),"\n",(0,o.jsx)(n.h3,{id:"perception-memory",children:"Perception Memory"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from collections import deque\nimport datetime\n\nclass PerceptionMemory:\n    def __init__(self, max_age_seconds=30):\n        self.scene_history = deque(maxlen=100)  # Last 100 scenes\n        self.object_tracking = {}  # Track objects over time\n        self.location_memory = {}  # Remember object locations\n        self.max_age = max_age_seconds\n\n    def update_scene(self, detections, timestamp):\n        """Update memory with new scene"""\n        scene_data = {\n            \'detections\': detections,\n            \'timestamp\': timestamp,\n            \'objects\': {det[\'class_name\']: det for det in detections},\n            \'object_counts\': self.count_objects(detections)\n        }\n\n        self.scene_history.append(scene_data)\n\n        # Update object tracking\n        self.update_object_tracking(detections, timestamp)\n\n        # Update location memory\n        self.update_location_memory(detections)\n\n    def get_current_scene(self) -> Dict[str, Any]:\n        """Get the most recent scene"""\n        if self.scene_history:\n            return self.scene_history[-1]\n        return {\'detections\': [], \'timestamp\': None, \'objects\': {}, \'object_counts\': {}}\n\n    def count_objects(self, detections):\n        """Count objects by class"""\n        counts = {}\n        for det in detections:\n            class_name = det[\'class_name\']\n            counts[class_name] = counts.get(class_name, 0) + 1\n        return counts\n\n    def update_object_tracking(self, detections, timestamp):\n        """Track objects across frames"""\n        current_objects = {det[\'class_name\']: det for det in detections}\n\n        for obj_name, detection in current_objects.items():\n            if obj_name not in self.object_tracking:\n                self.object_tracking[obj_name] = deque(maxlen=50)\n\n            self.object_tracking[obj_name].append({\n                \'detection\': detection,\n                \'timestamp\': timestamp,\n                \'location\': detection[\'center\']\n            })\n\n    def update_location_memory(self, detections):\n        """Update memory of where objects are located"""\n        for detection in detections:\n            obj_name = detection[\'class_name\']\n            location = detection[\'center\']\n\n            if obj_name not in self.location_memory:\n                self.location_memory[obj_name] = []\n\n            # Update with most recent location\n            self.location_memory[obj_name] = location\n\n    def get_object_history(self, obj_name: str) -> List[Dict[str, Any]]:\n        """Get history of an object\'s detections"""\n        if obj_name in self.object_tracking:\n            return list(self.object_tracking[obj_name])\n        return []\n\n    def get_known_locations(self) -> Dict[str, List[float]]:\n        """Get known locations of objects"""\n        return self.location_memory.copy()\n\n    def get_relevant_objects(self, category: str = None) -> Dict[str, Any]:\n        """Get objects relevant to current task"""\n        current_scene = self.get_current_scene()\n        objects = current_scene.get(\'objects\', {})\n\n        if category:\n            # Filter by category (in practice, you\'d have a taxonomy)\n            filtered_objects = {}\n            for name, obj in objects.items():\n                if category.lower() in name.lower():\n                    filtered_objects[name] = obj\n            return filtered_objects\n\n        return objects\n\n    def forget_old_data(self):\n        """Remove old data that exceeds max age"""\n        current_time = datetime.datetime.now()\n\n        # Clean up scene history\n        recent_scenes = []\n        for scene in self.scene_history:\n            if (current_time - scene[\'timestamp\']).seconds < self.max_age:\n                recent_scenes.append(scene)\n        self.scene_history = deque(recent_scenes, maxlen=100)\n\n        # Clean up object tracking\n        for obj_name, track_history in self.object_tracking.items():\n            recent_track = []\n            for entry in track_history:\n                if (current_time - entry[\'timestamp\']).seconds < self.max_age:\n                    recent_track.append(entry)\n            self.object_tracking[obj_name] = deque(recent_track, maxlen=50)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"robot-state-management",children:"Robot State Management"}),"\n",(0,o.jsx)(n.h3,{id:"state-representation",children:"State Representation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class RobotState:\n    def __init__(self):\n        self.position = {'x': 0.0, 'y': 0.0, 'theta': 0.0}\n        self.velocity = {'linear': 0.0, 'angular': 0.0}\n        self.battery_level = 100.0\n        self.operational_status = 'operational'\n        self.current_task = None\n        self.last_command = None\n        self.safety_status = 'safe'\n        self.components_status = {\n            'navigation': 'ready',\n            'manipulation': 'ready',\n            'perception': 'ready',\n            'communication': 'ready'\n        }\n\n    def update_state(self, state_data: Dict[str, Any]):\n        \"\"\"Update robot state from external source\"\"\"\n        if 'position' in state_data:\n            self.position.update(state_data['position'])\n\n        if 'velocity' in state_data:\n            self.velocity.update(state_data['velocity'])\n\n        if 'battery_level' in state_data:\n            self.battery_level = state_data['battery_level']\n\n        if 'operational_status' in state_data:\n            self.operational_status = state_data['operational_status']\n\n        if 'current_task' in state_data:\n            self.current_task = state_data['current_task']\n\n        if 'last_command' in state_data:\n            self.last_command = state_data['last_command']\n\n        if 'safety_status' in state_data:\n            self.safety_status = state_data['safety_status']\n\n        if 'components_status' in state_data:\n            self.components_status.update(state_data['components_status'])\n\n    def is_component_ready(self, component: str) -> bool:\n        \"\"\"Check if a component is ready\"\"\"\n        return self.components_status.get(component, 'unknown') == 'ready'\n\n    def can_execute_action(self, action_requirements: List[str]) -> bool:\n        \"\"\"Check if robot can execute an action based on state\"\"\"\n        for req in action_requirements:\n            if req == 'operational':\n                if self.operational_status != 'operational':\n                    return False\n            elif req == 'battery_sufficient':\n                if self.battery_level < 20:  # 20% threshold\n                    return False\n            elif req.startswith('component_'):\n                component = req.split('_', 1)[1]\n                if not self.is_component_ready(component):\n                    return False\n\n        return True\n\n    def get_state_summary(self) -> str:\n        \"\"\"Get a textual summary of the robot state\"\"\"\n        return f\"\"\"\n        Position: ({self.position['x']:.2f}, {self.position['y']:.2f}, {self.position['theta']:.2f})\n        Battery: {self.battery_level:.1f}%\n        Status: {self.operational_status}\n        Current Task: {self.current_task or 'None'}\n        Safety: {self.safety_status}\n        Components: {self.components_status}\n        \"\"\"\n"})}),"\n",(0,o.jsx)(n.h2,{id:"integration-and-coordination",children:"Integration and Coordination"}),"\n",(0,o.jsx)(n.h3,{id:"multi-modal-fusion",children:"Multi-Modal Fusion"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class MultiModalFusion:\n    def __init__(self, node):\n        self.node = node\n        self.confidence_threshold = 0.7\n        self.temporal_window = 2.0  # seconds\n\n    def fuse_vision_language(self, vision_data: Dict[str, Any],\n                           language_interpretation: Dict[str, Any]) -> Dict[str, Any]:\n        """Fuse visual and linguistic information"""\n        fused_result = {\n            \'entities_mentioned\': language_interpretation.get(\'entities\', {}),\n            \'entities_detected\': self.extract_entities_from_vision(vision_data),\n            \'alignment_score\': 0.0,\n            \'resolved_entities\': {},\n            \'confidence\': 0.0\n        }\n\n        # Align mentioned entities with detected entities\n        aligned_entities = self.align_entities(\n            fused_result[\'entities_mentioned\'],\n            fused_result[\'entities_detected\']\n        )\n\n        fused_result[\'resolved_entities\'] = aligned_entities\n        fused_result[\'alignment_score\'] = self.calculate_alignment_score(aligned_entities)\n        fused_result[\'confidence\'] = self.calculate_fusion_confidence(fused_result)\n\n        return fused_result\n\n    def extract_entities_from_vision(self, vision_data: Dict[str, Any]) -> Dict[str, Any]:\n        """Extract entities from vision data"""\n        entities = {}\n        for detection in vision_data.get(\'detections\', []):\n            class_name = detection.get(\'class_name\', \'\').lower()\n            if class_name not in entities:\n                entities[class_name] = []\n            entities[class_name].append(detection)\n        return entities\n\n    def align_entities(self, mentioned_entities: Dict[str, List[str]],\n                      detected_entities: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:\n        """Align entities mentioned in language with detected in vision"""\n        resolved = {}\n\n        for entity_type, mentioned_list in mentioned_entities.items():\n            for mentioned_entity in mentioned_list:\n                # Find best match among detected entities\n                best_match = self.find_best_entity_match(mentioned_entity, detected_entities)\n                if best_match:\n                    resolved[mentioned_entity] = best_match\n\n        return resolved\n\n    def find_best_entity_match(self, mentioned_entity: str,\n                              detected_entities: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:\n        """Find the best visual match for a mentioned entity"""\n        best_match = None\n        best_score = 0.0\n\n        for detected_class, detections in detected_entities.items():\n            # Simple string similarity (in practice, use more sophisticated matching)\n            similarity = self.calculate_similarity(mentioned_entity, detected_class)\n            if similarity > best_score and similarity > self.confidence_threshold:\n                # Take the first detection (in practice, use spatial reasoning)\n                best_match = detections[0]\n                best_score = similarity\n\n        return best_match\n\n    def calculate_similarity(self, str1: str, str2: str) -> float:\n        """Calculate similarity between two strings"""\n        # Simple token overlap (in practice, use embeddings or more sophisticated measures)\n        tokens1 = set(str1.lower().split())\n        tokens2 = set(str2.lower().split())\n\n        intersection = tokens1.intersection(tokens2)\n        union = tokens1.union(tokens2)\n\n        if not union:\n            return 0.0\n\n        return len(intersection) / len(union)\n\n    def calculate_alignment_score(self, aligned_entities: Dict[str, Any]) -> float:\n        """Calculate overall alignment score"""\n        if not aligned_entities:\n            return 0.0\n\n        total_score = sum(1.0 for _ in aligned_entities.values())\n        return total_score / len(aligned_entities) if aligned_entities else 0.0\n\n    def calculate_fusion_confidence(self, fused_result: Dict[str, Any]) -> float:\n        """Calculate confidence in the fusion result"""\n        # Combine multiple confidence factors\n        alignment_confidence = fused_result[\'alignment_score\']\n        detection_confidence = np.mean([\n            det.get(\'confidence\', 0.0)\n            for det_list in fused_result[\'entities_detected\'].values()\n            for det in det_list\n        ]) if fused_result[\'entities_detected\'] else 0.0\n\n        return (alignment_confidence + detection_confidence) / 2.0\n'})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Design multi-modal architectures that integrate vision, language, and action systems"}),"\n",(0,o.jsx)(n.li,{children:"Implement vision processing pipelines for robotic applications"}),"\n",(0,o.jsx)(n.li,{children:"Create language understanding systems that ground in visual context"}),"\n",(0,o.jsx)(n.li,{children:"Build action planning systems that utilize multi-modal information"}),"\n",(0,o.jsx)(n.li,{children:"Develop memory systems for maintaining context across modalities"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,o.jsx)(n.p,{children:"Create a complete VLA system that takes a natural language command, processes visual input to understand the environment, and executes appropriate robot actions. Test the system with various commands and evaluate its ability to ground language in visual perception."}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsx)(n.p,{children:"Use attention mechanisms to focus processing on relevant parts of the input modalities."})}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsx)(n.p,{children:"Consider the timing constraints of multi-modal processing - vision processing, language understanding, and action execution all have different latencies that need to be coordinated."})})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);