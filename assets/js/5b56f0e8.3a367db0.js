"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[802],{3841:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module4/module4-milestone","title":"Module 4 Milestone Project: Vision-Language-Action Robot Assistant","description":"Project Overview","source":"@site/docs/module4/module4-milestone.md","sourceDirName":"module4","slug":"/module4/module4-milestone","permalink":"/hackathon-01-AI-textbook/docs/module4/module4-milestone","draft":false,"unlisted":false,"editUrl":"https://github.com/maheenali021/hackathon-01-AI-textbook/tree/main/docs/module4/module4-milestone.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Module 4 Milestone Project: Vision-Language-Action Robot Assistant"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Multi-Modal Integration for VLA Systems","permalink":"/hackathon-01-AI-textbook/docs/module4/multi-modal-integration"},"next":{"title":"Chapter 1: Capstone Overview","permalink":"/hackathon-01-AI-textbook/docs/capstone/capstone-overview"}}');var o=t(4848),a=t(8453);const i={sidebar_position:5,title:"Module 4 Milestone Project: Vision-Language-Action Robot Assistant"},r="Module 4 Milestone Project: Vision-Language-Action Robot Assistant",l={},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Project Requirements",id:"project-requirements",level:2},{value:"Step-by-Step Implementation",id:"step-by-step-implementation",level:2},{value:"Step 1: Create the VLA System Architecture",id:"step-1-create-the-vla-system-architecture",level:3},{value:"Step 2: Create the Main VLA Controller",id:"step-2-create-the-main-vla-controller",level:3},{value:"Step 3: Create the Voice Processor Node",id:"step-3-create-the-voice-processor-node",level:3},{value:"Step 4: Create the State Manager Node",id:"step-4-create-the-state-manager-node",level:3},{value:"Step 5: Package Configuration",id:"step-5-package-configuration",level:3},{value:"Running the VLA System",id:"running-the-vla-system",level:2},{value:"To run the complete VLA system:",id:"to-run-the-complete-vla-system",level:3},{value:"Testing Your Implementation",id:"testing-your-implementation",level:2},{value:"Test 1: Verify VLA Nodes Are Running",id:"test-1-verify-vla-nodes-are-running",level:3},{value:"Test 2: Test Voice Command Processing",id:"test-2-test-voice-command-processing",level:3},{value:"Test 3: Check System State Updates",id:"test-3-check-system-state-updates",level:3},{value:"Enhancement Challenges",id:"enhancement-challenges",level:2},{value:"Solution Summary",id:"solution-summary",level:2},{value:"Learning Review",id:"learning-review",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-milestone-project-vision-language-action-robot-assistant",children:"Module 4 Milestone Project: Vision-Language-Action Robot Assistant"})}),"\n",(0,o.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,o.jsx)(n.p,{children:"In this milestone project, you'll create a complete Vision-Language-Action (VLA) system that allows users to interact with a robot using natural language commands. The system will integrate perception, language understanding, and action execution to create an intelligent robot assistant."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"After completing this project, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate vision, language, and action systems into a cohesive pipeline"}),"\n",(0,o.jsx)(n.li,{children:"Process natural language commands and map them to robot actions"}),"\n",(0,o.jsx)(n.li,{children:"Implement cognitive planning for complex task execution"}),"\n",(0,o.jsx)(n.li,{children:"Create multimodal interfaces for human-robot interaction"}),"\n",(0,o.jsx)(n.li,{children:"Build end-to-end VLA systems for real-world applications"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"project-requirements",children:"Project Requirements"}),"\n",(0,o.jsx)(n.p,{children:"Create a VLA system with:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Voice command processing with Whisper for speech recognition"}),"\n",(0,o.jsx)(n.li,{children:"Language understanding with LLMs for command interpretation"}),"\n",(0,o.jsx)(n.li,{children:"Visual perception for scene understanding and object detection"}),"\n",(0,o.jsx)(n.li,{children:"Action planning and execution for robot control"}),"\n",(0,o.jsx)(n.li,{children:"Multimodal feedback and interaction capabilities"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"step-by-step-implementation",children:"Step-by-Step Implementation"}),"\n",(0,o.jsx)(n.h3,{id:"step-1-create-the-vla-system-architecture",children:"Step 1: Create the VLA System Architecture"}),"\n",(0,o.jsxs)(n.p,{children:["Create the file ",(0,o.jsx)(n.code,{children:"vla_system/launch/vla_system.launch.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\n\ndef generate_launch_description():\n    # Launch configurations\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n    robot_namespace = LaunchConfiguration('robot_namespace', default='')\n\n    # Package directories\n    pkg_vla_system = get_package_share_directory('vla_system')\n\n    # VLA main node\n    vla_main_node = Node(\n        package='vla_system',\n        executable='vla_main',\n        name='vla_main',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'model_name': 'gpt-3.5-turbo',  # or your preferred LLM\n            'whisper_model': 'base',\n            'confidence_threshold': 0.7,\n        }],\n        remappings=[\n            ('/camera/image_raw', '/camera/image_raw'),\n            ('/cmd_vel', '/cmd_vel'),\n            ('/joint_commands', '/joint_commands'),\n        ],\n        output='screen'\n    )\n\n    # Voice processing node\n    voice_processing_node = Node(\n        package='vla_system',\n        executable='voice_processor',\n        name='voice_processor',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'whisper_model_size': 'base',\n            'language': 'en',\n            'temperature': 0.0,\n        }],\n        output='screen'\n    )\n\n    # Vision processing node\n    vision_processing_node = Node(\n        package='vla_system',\n        executable='vision_processor',\n        name='vision_processor',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'detection_threshold': 0.5,\n            'enable_segmentation': True,\n        }],\n        remappings=[\n            ('/camera/image_raw', '/camera/image_raw'),\n            ('/camera/camera_info', '/camera/camera_info'),\n        ],\n        output='screen'\n    )\n\n    # Language understanding node\n    language_understanding_node = Node(\n        package='vla_system',\n        executable='language_understanding',\n        name='language_understanding',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'model_name': 'gpt-3.5-turbo',\n            'max_tokens': 1000,\n            'temperature': 0.1,\n        }],\n        output='screen'\n    )\n\n    # Action planning node\n    action_planning_node = Node(\n        package='vla_system',\n        executable='action_planner',\n        name='action_planner',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'planning_timeout': 30.0,\n            'max_retries': 3,\n        }],\n        output='screen'\n    )\n\n    # State manager node\n    state_manager_node = Node(\n        package='vla_system',\n        executable='state_manager',\n        name='state_manager',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'state_update_rate': 10.0,\n        }],\n        output='screen'\n    )\n\n    # Feedback and interaction node\n    feedback_node = Node(\n        package='vla_system',\n        executable='feedback_manager',\n        name='feedback_manager',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'enable_audio_feedback': True,\n            'enable_visual_feedback': True,\n        }],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation time if true'),\n        DeclareLaunchArgument(\n            'robot_namespace',\n            default_value='',\n            description='Namespace for robot topics'),\n\n        vla_main_node,\n        voice_processing_node,\n        vision_processing_node,\n        language_understanding_node,\n        action_planning_node,\n        state_manager_node,\n        feedback_node,\n    ])\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-2-create-the-main-vla-controller",children:"Step 2: Create the Main VLA Controller"}),"\n",(0,o.jsxs)(n.p,{children:["Create the file ",(0,o.jsx)(n.code,{children:"vla_system/src/vla_main.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import Twist\nfrom builtin_interfaces.msg import Time\nimport numpy as np\nimport threading\nimport time\nfrom queue import Queue\nimport openai\nfrom openai import OpenAI\nimport whisper\nimport torch\nimport json\nfrom tf2_ros import TransformListener, Buffer\n\n\nclass VLAMain(Node):\n    def __init__(self):\n        super().__init__('vla_main')\n\n        # Initialize components\n        self.setup_subscribers()\n        self.setup_publishers()\n        self.initialize_models()\n        self.setup_internal_state()\n\n        # Task queue for processing\n        self.task_queue = Queue()\n        self.response_queue = Queue()\n\n        # Processing thread\n        self.processing_thread = threading.Thread(target=self.process_commands)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n        # Timer for main loop\n        self.main_timer = self.create_timer(0.1, self.main_loop)\n\n    def setup_subscribers(self):\n        \"\"\"Setup ROS subscribers\"\"\"\n        self.voice_sub = self.create_subscription(\n            String,\n            'voice_commands',\n            self.voice_command_callback,\n            10\n        )\n\n        self.image_sub = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.state_sub = self.create_subscription(\n            String,\n            'robot_state',\n            self.state_callback,\n            10\n        )\n\n    def setup_publishers(self):\n        \"\"\"Setup ROS publishers\"\"\"\n        self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)\n        self.response_pub = self.create_publisher(String, 'vla_response', 10)\n        self.action_pub = self.create_publisher(String, 'robot_actions', 10)\n\n    def initialize_models(self):\n        \"\"\"Initialize AI models\"\"\"\n        try:\n            # Initialize Whisper model for speech recognition\n            self.whisper_model = whisper.load_model(\"base\")\n            self.get_logger().info(\"Whisper model loaded successfully\")\n        except Exception as e:\n            self.get_logger().error(f\"Failed to load Whisper model: {e}\")\n            self.whisper_model = None\n\n        try:\n            # Initialize OpenAI client (you would need to set your API key)\n            # self.openai_client = OpenAI(api_key='your-api-key-here')\n            self.get_logger().info(\"OpenAI client initialized\")\n        except Exception as e:\n            self.get_logger().error(f\"Failed to initialize OpenAI client: {e}\")\n            # For this example, we'll use a mock implementation\n\n    def setup_internal_state(self):\n        \"\"\"Setup internal state variables\"\"\"\n        self.current_image = None\n        self.robot_state = {}\n        self.conversation_history = []\n        self.is_processing = False\n\n    def voice_command_callback(self, msg):\n        \"\"\"Handle incoming voice commands\"\"\"\n        command = msg.data\n        self.get_logger().info(f\"Received voice command: {command}\")\n\n        # Add to task queue for processing\n        task = {\n            'type': 'command',\n            'data': command,\n            'timestamp': self.get_clock().now().to_msg()\n        }\n        self.task_queue.put(task)\n\n    def image_callback(self, msg):\n        \"\"\"Handle incoming camera images\"\"\"\n        self.current_image = msg\n        # Store for use in processing\n\n    def state_callback(self, msg):\n        \"\"\"Handle robot state updates\"\"\"\n        try:\n            self.robot_state = json.loads(msg.data)\n        except json.JSONDecodeError:\n            self.get_logger().error(\"Failed to parse robot state JSON\")\n\n    def process_commands(self):\n        \"\"\"Process commands in a separate thread\"\"\"\n        while rclpy.ok():\n            try:\n                if not self.task_queue.empty():\n                    task = self.task_queue.get(timeout=1.0)\n\n                    if task['type'] == 'command':\n                        response = self.process_natural_language_command(task['data'])\n                        response_msg = String()\n                        response_msg.data = response\n                        self.response_queue.put(response_msg)\n\n                time.sleep(0.01)  # Small delay to prevent busy waiting\n            except Exception as e:\n                self.get_logger().error(f\"Error in processing thread: {e}\")\n\n    def process_natural_language_command(self, command):\n        \"\"\"Process natural language command using LLM\"\"\"\n        try:\n            # Create a prompt for the LLM\n            prompt = self.create_llm_prompt(command)\n\n            # For this example, we'll simulate the LLM response\n            # In a real implementation, you would call the LLM API\n            response = self.mock_llm_response(command)\n\n            # Parse the response and execute actions\n            self.execute_parsed_actions(response)\n\n            return response\n\n        except Exception as e:\n            self.get_logger().error(f\"Error processing command: {e}\")\n            return f\"Sorry, I encountered an error processing your command: {str(e)}\"\n\n    def create_llm_prompt(self, command):\n        \"\"\"Create a prompt for the LLM with context\"\"\"\n        # Get current context (image, state, etc.)\n        context = self.get_current_context()\n\n        prompt = f\"\"\"\n        You are a helpful robot assistant. The user has given the following command: \"{command}\"\n\n        Current robot state: {context['state']}\n        Current environment: {context['environment']}\n\n        Please interpret this command and provide a structured response with:\n        1. The intent of the command\n        2. Specific actions to execute\n        3. Any relevant objects or locations\n\n        Respond in JSON format with keys: intent, actions, objects, locations.\n        \"\"\"\n\n        return prompt\n\n    def get_current_context(self):\n        \"\"\"Get current context including state and environment\"\"\"\n        context = {\n            'state': self.robot_state,\n            'environment': 'Visual scene analysis would go here',\n            'timestamp': self.get_clock().now().to_msg()\n        }\n\n        return context\n\n    def mock_llm_response(self, command):\n        \"\"\"Mock LLM response for demonstration\"\"\"\n        # This is a simplified mock - in reality, you'd call an LLM\n        command_lower = command.lower()\n\n        if 'move' in command_lower or 'go' in command_lower:\n            if 'forward' in command_lower:\n                return json.dumps({\n                    'intent': 'navigation',\n                    'actions': ['move_forward'],\n                    'objects': [],\n                    'locations': [],\n                    'parameters': {'distance': 1.0}\n                })\n            elif 'backward' in command_lower:\n                return json.dumps({\n                    'intent': 'navigation',\n                    'actions': ['move_backward'],\n                    'objects': [],\n                    'locations': [],\n                    'parameters': {'distance': 1.0}\n                })\n            elif 'left' in command_lower or 'right' in command_lower:\n                return json.dumps({\n                    'intent': 'navigation',\n                    'actions': ['turn'],\n                    'objects': [],\n                    'locations': [],\n                    'parameters': {'direction': 'left' if 'left' in command_lower else 'right'}\n                })\n        elif 'pick' in command_lower or 'grasp' in command_lower or 'take' in command_lower:\n            return json.dumps({\n                'intent': 'manipulation',\n                'actions': ['approach_object', 'grasp_object'],\n                'objects': ['object'],\n                'locations': [],\n                'parameters': {'object_name': 'object'}\n            })\n        elif 'find' in command_lower or 'look' in command_lower:\n            return json.dumps({\n                'intent': 'perception',\n                'actions': ['scan_environment', 'detect_objects'],\n                'objects': ['object'],\n                'locations': [],\n                'parameters': {'object_name': 'object'}\n            })\n\n        return json.dumps({\n            'intent': 'unknown',\n            'actions': [],\n            'objects': [],\n            'locations': [],\n            'parameters': {}\n        })\n\n    def execute_parsed_actions(self, response_json):\n        \"\"\"Execute actions parsed from LLM response\"\"\"\n        try:\n            response = json.loads(response_json)\n            intent = response.get('intent', 'unknown')\n            actions = response.get('actions', [])\n\n            for action in actions:\n                if action == 'move_forward':\n                    self.execute_navigation_command('forward', response.get('parameters', {}))\n                elif action == 'move_backward':\n                    self.execute_navigation_command('backward', response.get('parameters', {}))\n                elif action == 'turn':\n                    self.execute_navigation_command('turn', response.get('parameters', {}))\n                elif action == 'approach_object':\n                    self.execute_approach_object(response.get('parameters', {}))\n                elif action == 'grasp_object':\n                    self.execute_grasp_object(response.get('parameters', {}))\n                elif action == 'scan_environment':\n                    self.execute_scan_environment()\n                elif action == 'detect_objects':\n                    self.execute_detect_objects()\n\n            # Publish action for logging\n            action_msg = String()\n            action_msg.data = f\"Executed {intent} with actions: {actions}\"\n            self.action_pub.publish(action_msg)\n\n        except json.JSONDecodeError as e:\n            self.get_logger().error(f\"Error parsing LLM response: {e}\")\n        except Exception as e:\n            self.get_logger().error(f\"Error executing actions: {e}\")\n\n    def execute_navigation_command(self, direction, parameters):\n        \"\"\"Execute navigation commands\"\"\"\n        cmd_vel = Twist()\n\n        if direction == 'forward':\n            cmd_vel.linear.x = 0.3  # m/s\n        elif direction == 'backward':\n            cmd_vel.linear.x = -0.3\n        elif direction == 'turn':\n            cmd_vel.angular.z = 0.5 if parameters.get('direction') == 'left' else -0.5\n\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info(f\"Executing navigation: {direction} with params: {parameters}\")\n\n    def execute_approach_object(self, parameters):\n        \"\"\"Execute approach object action\"\"\"\n        # In a real implementation, this would navigate to the object\n        self.get_logger().info(f\"Approaching object: {parameters.get('object_name')}\")\n\n    def execute_grasp_object(self, parameters):\n        \"\"\"Execute grasp object action\"\"\"\n        # In a real implementation, this would control manipulator\n        self.get_logger().info(f\"Attempting to grasp object: {parameters.get('object_name')}\")\n\n    def execute_scan_environment(self):\n        \"\"\"Execute environment scanning\"\"\"\n        self.get_logger().info(\"Scanning environment...\")\n\n    def execute_detect_objects(self):\n        \"\"\"Execute object detection\"\"\"\n        self.get_logger().info(\"Detecting objects...\")\n\n    def main_loop(self):\n        \"\"\"Main loop for the VLA system\"\"\"\n        # Publish any responses from the processing thread\n        while not self.response_queue.empty():\n            response_msg = self.response_queue.get()\n            self.response_pub.publish(response_msg)\n\n        # Update internal state periodically\n        self.update_internal_state()\n\n    def update_internal_state(self):\n        \"\"\"Update internal state variables\"\"\"\n        # This would update state based on current sensor data\n        pass\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vla_main = VLAMain()\n\n    try:\n        rclpy.spin(vla_main)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vla_main.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-3-create-the-voice-processor-node",children:"Step 3: Create the Voice Processor Node"}),"\n",(0,o.jsxs)(n.p,{children:["Create the file ",(0,o.jsx)(n.code,{children:"vla_system/src/voice_processor.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import AudioData\nimport pyaudio\nimport wave\nimport numpy as np\nimport whisper\nimport threading\nimport queue\nimport time\n\n\nclass VoiceProcessor(Node):\n    def __init__(self):\n        super().__init__(\'voice_processor\')\n\n        # Initialize Whisper model\n        try:\n            self.model = whisper.load_model("base")\n            self.get_logger().info("Whisper model loaded successfully")\n        except Exception as e:\n            self.get_logger().error(f"Failed to load Whisper model: {e}")\n            self.model = None\n\n        # Setup audio parameters\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 16000\n        self.chunk = 1024\n        self.record_seconds = 3\n\n        # Setup publishers and subscribers\n        self.voice_pub = self.create_publisher(String, \'voice_commands\', 10)\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            \'audio_input\',\n            self.audio_callback,\n            10\n        )\n\n        # Setup audio processing\n        self.audio_queue = queue.Queue()\n        self.processing_thread = threading.Thread(target=self.process_audio)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n        # Setup continuous listening\n        self.listening = True\n        self.listening_thread = threading.Thread(target=self.continuous_listening)\n        self.listening_thread.daemon = True\n        self.listening_thread.start()\n\n        self.get_logger().info("Voice processor initialized")\n\n    def audio_callback(self, msg):\n        """Handle audio data from ROS topic"""\n        # Add audio data to processing queue\n        self.audio_queue.put(msg.data)\n\n    def process_audio(self):\n        """Process audio data from queue"""\n        while rclpy.ok():\n            try:\n                if not self.audio_queue.empty():\n                    audio_data = self.audio_queue.get(timeout=1.0)\n\n                    # Process the audio with Whisper\n                    if self.model:\n                        try:\n                            # Save audio data temporarily for processing\n                            temp_filename = \'/tmp/temp_audio.wav\'\n\n                            # Write audio data to temporary file\n                            wf = wave.open(temp_filename, \'wb\')\n                            wf.setnchannels(self.channels)\n                            wf.setsampwidth(pyaudio.PyAudio().get_sample_size(self.format))\n                            wf.setframerate(self.rate)\n                            wf.writeframes(audio_data)\n                            wf.close()\n\n                            # Transcribe the audio\n                            result = self.model.transcribe(temp_filename)\n                            text = result["text"].strip()\n\n                            if text:  # Only publish if there\'s text\n                                self.get_logger().info(f"Transcribed: {text}")\n                                msg = String()\n                                msg.data = text\n                                self.voice_pub.publish(msg)\n\n                        except Exception as e:\n                            self.get_logger().error(f"Error processing audio: {e}")\n                else:\n                    time.sleep(0.01)  # Small delay to prevent busy waiting\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f"Error in audio processing: {e}")\n\n    def continuous_listening(self):\n        """Continuously listen for audio using PyAudio"""\n        if not self.model:\n            return\n\n        p = pyaudio.PyAudio()\n\n        try:\n            stream = p.open(\n                format=self.format,\n                channels=self.channels,\n                rate=self.rate,\n                input=True,\n                frames_per_buffer=self.chunk\n            )\n\n            self.get_logger().info("Started continuous listening...")\n\n            while self.listening and rclpy.ok():\n                frames = []\n\n                # Record for specified duration\n                for i in range(0, int(self.rate / self.chunk * self.record_seconds)):\n                    data = stream.read(self.chunk)\n                    frames.append(data)\n\n                # Convert frames to bytes\n                audio_data = b\'\'.join(frames)\n\n                # Add to processing queue\n                self.audio_queue.put(audio_data)\n\n        except Exception as e:\n            self.get_logger().error(f"Error in continuous listening: {e}")\n        finally:\n            stream.stop_stream()\n            stream.close()\n            p.terminate()\n\n    def destroy_node(self):\n        """Cleanup when node is destroyed"""\n        self.listening = False\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    voice_processor = VoiceProcessor()\n\n    try:\n        rclpy.spin(voice_processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        voice_processor.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"step-4-create-the-state-manager-node",children:"Step 4: Create the State Manager Node"}),"\n",(0,o.jsxs)(n.p,{children:["Create the file ",(0,o.jsx)(n.code,{children:"vla_system/src/state_manager.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, Pose\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import LaserScan, Image\nfrom builtin_interfaces.msg import Time\nimport json\nimport threading\nimport time\nfrom collections import deque\n\n\nclass StateManager(Node):\n    def __init__(self):\n        super().__init__('state_manager')\n\n        # Setup publishers\n        self.state_pub = self.create_publisher(String, 'robot_state', 10)\n\n        # Setup subscribers\n        self.odom_sub = self.create_subscription(\n            Odometry,\n            'odom',\n            self.odom_callback,\n            10\n        )\n\n        self.cmd_vel_sub = self.create_subscription(\n            Twist,\n            'cmd_vel',\n            self.cmd_vel_callback,\n            10\n        )\n\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            'scan',\n            self.scan_callback,\n            10\n        )\n\n        # Internal state\n        self.robot_state = {\n            'position': {'x': 0.0, 'y': 0.0, 'z': 0.0},\n            'orientation': {'x': 0.0, 'y': 0.0, 'z': 0.0, 'w': 1.0},\n            'velocity': {'linear': {'x': 0.0, 'y': 0.0, 'z': 0.0},\n                        'angular': {'x': 0.0, 'y': 0.0, 'z': 0.0}},\n            'battery_level': 100.0,\n            'last_command': '',\n            'command_timestamp': 0,\n            'safety_status': 'normal',\n            'navigation_status': 'idle',\n            'manipulation_status': 'idle',\n            'perception_status': 'idle',\n            'active_goals': [],\n            'completed_goals': [],\n            'error_count': 0,\n            'last_error': '',\n            'system_uptime': 0.0\n        }\n\n        # State history for debugging\n        self.state_history = deque(maxlen=100)\n\n        # Setup timer for state updates\n        self.state_timer = self.create_timer(1.0, self.publish_state)\n\n        # Setup timer for system monitoring\n        self.monitor_timer = self.create_timer(0.1, self.monitor_system)\n\n        # Initialize start time\n        self.start_time = self.get_clock().now().nanoseconds / 1e9\n\n        self.get_logger().info(\"State manager initialized\")\n\n    def odom_callback(self, msg):\n        \"\"\"Update position and orientation from odometry\"\"\"\n        self.robot_state['position']['x'] = msg.pose.pose.position.x\n        self.robot_state['position']['y'] = msg.pose.pose.position.y\n        self.robot_state['position']['z'] = msg.pose.pose.position.z\n\n        self.robot_state['orientation']['x'] = msg.pose.pose.orientation.x\n        self.robot_state['orientation']['y'] = msg.pose.pose.orientation.y\n        self.robot_state['orientation']['z'] = msg.pose.pose.orientation.z\n        self.robot_state['orientation']['w'] = msg.pose.pose.orientation.w\n\n        self.robot_state['velocity']['linear']['x'] = msg.twist.twist.linear.x\n        self.robot_state['velocity']['linear']['y'] = msg.twist.twist.linear.y\n        self.robot_state['velocity']['linear']['z'] = msg.twist.twist.linear.z\n\n        self.robot_state['velocity']['angular']['x'] = msg.twist.twist.angular.x\n        self.robot_state['velocity']['angular']['y'] = msg.twist.twist.angular.y\n        self.robot_state['velocity']['angular']['z'] = msg.twist.twist.angular.z\n\n    def cmd_vel_callback(self, msg):\n        \"\"\"Update last command and status\"\"\"\n        self.robot_state['last_command'] = f\"Lin:({msg.linear.x:.2f}, {msg.linear.y:.2f}, {msg.linear.z:.2f}), \" \\\n                                         f\"Ang:({msg.angular.x:.2f}, {msg.angular.y:.2f}, {msg.angular.z:.2f})\"\n        self.robot_state['command_timestamp'] = self.get_clock().now().nanoseconds / 1e9\n\n        # Update navigation status based on command\n        if abs(msg.linear.x) > 0.01 or abs(msg.angular.z) > 0.01:\n            self.robot_state['navigation_status'] = 'moving'\n        else:\n            self.robot_state['navigation_status'] = 'idle'\n\n    def scan_callback(self, msg):\n        \"\"\"Process laser scan for obstacle detection\"\"\"\n        if msg.ranges:\n            min_range = min([r for r in msg.ranges if r > msg.range_min and r < msg.range_max])\n            if min_range < 0.5:  # Dangerously close\n                self.robot_state['safety_status'] = 'danger'\n            elif min_range < 1.0:  # Close\n                self.robot_state['safety_status'] = 'warning'\n            else:  # Safe\n                self.robot_state['safety_status'] = 'normal'\n\n    def monitor_system(self):\n        \"\"\"Monitor system status and update state\"\"\"\n        current_time = self.get_clock().now().nanoseconds / 1e9\n        self.robot_state['system_uptime'] = current_time - self.start_time\n\n        # Update battery level simulation (decreasing over time)\n        self.robot_state['battery_level'] = max(0.0, self.robot_state['battery_level'] - 0.01)\n\n        # Add current state to history\n        self.state_history.append(dict(self.robot_state))\n\n    def publish_state(self):\n        \"\"\"Publish the current robot state as JSON\"\"\"\n        try:\n            state_msg = String()\n            state_msg.data = json.dumps(self.robot_state, indent=2)\n            self.state_pub.publish(state_msg)\n\n            self.get_logger().debug(f\"Published robot state: {self.robot_state['navigation_status']}, \"\n                                   f\"Pos:({self.robot_state['position']['x']:.2f}, {self.robot_state['position']['y']:.2f}), \"\n                                   f\"Vel:({self.robot_state['velocity']['linear']['x']:.2f})\")\n        except Exception as e:\n            self.get_logger().error(f\"Error publishing state: {e}\")\n\n    def add_goal(self, goal_id, goal_description):\n        \"\"\"Add a new goal to the active goals list\"\"\"\n        goal = {\n            'id': goal_id,\n            'description': goal_description,\n            'status': 'active',\n            'timestamp': self.get_clock().now().nanoseconds / 1e9\n        }\n        self.robot_state['active_goals'].append(goal)\n\n    def complete_goal(self, goal_id):\n        \"\"\"Move a goal from active to completed\"\"\"\n        for i, goal in enumerate(self.robot_state['active_goals']):\n            if goal['id'] == goal_id:\n                completed_goal = self.robot_state['active_goals'].pop(i)\n                completed_goal['status'] = 'completed'\n                completed_goal['completion_time'] = self.get_clock().now().nanoseconds / 1e9\n                self.robot_state['completed_goals'].append(completed_goal)\n                return True\n        return False\n\n    def log_error(self, error_message):\n        \"\"\"Log an error and update error count\"\"\"\n        self.robot_state['error_count'] += 1\n        self.robot_state['last_error'] = error_message\n        self.get_logger().error(f\"Logged error: {error_message} (Total: {self.robot_state['error_count']})\")\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    state_manager = StateManager()\n\n    try:\n        rclpy.spin(state_manager)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        state_manager.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-5-package-configuration",children:"Step 5: Package Configuration"}),"\n",(0,o.jsxs)(n.p,{children:["Create the file ",(0,o.jsx)(n.code,{children:"vla_system/package.xml"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>vla_system</name>\n  <version>0.0.0</version>\n  <description>Vision-Language-Action system for Module 4 milestone project</description>\n  <maintainer email="your_email@example.com">Your Name</maintainer>\n  <license>TODO: License declaration</license>\n\n  <buildtool_depend>ament_cmake</buildtool_depend>\n  <buildtool_depend>ament_cmake_python</buildtool_depend>\n\n  <depend>rclcpp</depend>\n  <depend>rclpy</depend>\n  <depend>std_msgs</depend>\n  <depend>sensor_msgs</end>\n  <depend>geometry_msgs</depend>\n  <depend>nav_msgs</depend>\n  <depend>builtin_interfaces</depend>\n  <depend>tf2</depend>\n  <depend>tf2_ros</depend>\n\n  <depend>openai</depend>\n  <depend>openai-whisper</depend>\n  <depend>pyaudio</depend>\n\n  <exec_depend>launch</exec_depend>\n  <exec_depend>launch_ros</exec_depend>\n\n  <test_depend>ament_lint_auto</test_depend>\n  <test_depend>ament_lint_common</test_depend>\n\n  <export>\n    <build_type>ament_cmake</build_type>\n  </export>\n</package>\n'})}),"\n",(0,o.jsxs)(n.p,{children:["And create the ",(0,o.jsx)(n.code,{children:"vla_system/CMakeLists.txt"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cmake",children:'cmake_minimum_required(VERSION 3.8)\nproject(vla_system)\n\nif(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES "Clang")\n  add_compile_options(-Wall -Wextra -Wpedantic)\nendif()\n\n# find dependencies\nfind_package(ament_cmake REQUIRED)\nfind_package(rclcpp REQUIRED)\nfind_package(rclpy REQUIRED)\nfind_package(std_msgs REQUIRED)\nfind_package(sensor_msgs REQUIRED)\nfind_package(geometry_msgs REQUIRED)\nfind_package(nav_msgs REQUIRED)\nfind_package(builtin_interfaces REQUIRED)\nfind_package(tf2 REQUIRED)\nfind_package(tf2_ros REQUIRED)\n\n# Install Python modules\nament_python_install_package(${PROJECT_NAME})\n\n# Install Python executables\ninstall(PROGRAMS\n  src/vla_main.py\n  src/voice_processor.py\n  src/state_manager.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n\n# Install launch files\ninstall(DIRECTORY launch\n  DESTINATION share/${PROJECT_NAME}/\n)\n\n# Install config files\ninstall(DIRECTORY config\n  DESTINATION share/${PROJECT_NAME}/\n)\n\n# Install RViz config\ninstall(DIRECTORY rviz\n  DESTINATION share/${PROJECT_NAME}/\n)\n\nif(BUILD_TESTING)\n  find_package(ament_lint_auto REQUIRED)\n  set(ament_cmake_copyright_FOUND TRUE)\n  set(ament_cmake_cpplint_FOUND TRUE)\n  ament_lint_auto_find_test_dependencies()\nendif()\n\nament_package()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"running-the-vla-system",children:"Running the VLA System"}),"\n",(0,o.jsx)(n.h3,{id:"to-run-the-complete-vla-system",children:"To run the complete VLA system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Start the VLA system\nros2 launch vla_system vla_system.launch.py\n\n# Terminal 2: Simulate voice commands\nros2 topic pub /voice_commands std_msgs/String \"data: 'Move forward 1 meter'\"\n\n# Terminal 3: Monitor system state\nros2 topic echo /robot_state\n\n# Terminal 4: Monitor VLA responses\nros2 topic echo /vla_response\n"})}),"\n",(0,o.jsx)(n.h2,{id:"testing-your-implementation",children:"Testing Your Implementation"}),"\n",(0,o.jsx)(n.h3,{id:"test-1-verify-vla-nodes-are-running",children:"Test 1: Verify VLA Nodes Are Running"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Check active nodes\nros2 node list | grep vla\n\n# Check topics\nros2 topic list | grep -E "(voice|vla|state)"\n'})}),"\n",(0,o.jsx)(n.h3,{id:"test-2-test-voice-command-processing",children:"Test 2: Test Voice Command Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Send a test command\nros2 topic pub /voice_commands std_msgs/String \"data: 'Please move to the kitchen'\"\n\n# Monitor the response\nros2 topic echo /vla_response\n"})}),"\n",(0,o.jsx)(n.h3,{id:"test-3-check-system-state-updates",children:"Test 3: Check System State Updates"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Monitor robot state\nros2 topic echo /robot_state\n"})}),"\n",(0,o.jsx)(n.h2,{id:"enhancement-challenges",children:"Enhancement Challenges"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Add multimodal grounding to connect language to visual perception"}),"\n",(0,o.jsx)(n.li,{children:"Implement a more sophisticated dialogue manager"}),"\n",(0,o.jsx)(n.li,{children:"Add memory capabilities for context-aware interactions"}),"\n",(0,o.jsx)(n.li,{children:"Integrate with a real robot or advanced simulation"}),"\n",(0,o.jsx)(n.li,{children:"Add safety checks and validation for command execution"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"solution-summary",children:"Solution Summary"}),"\n",(0,o.jsx)(n.p,{children:"This milestone project demonstrates how to create a complete Vision-Language-Action system with:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Speech recognition using OpenAI Whisper"}),"\n",(0,o.jsx)(n.li,{children:"Language understanding with LLMs"}),"\n",(0,o.jsx)(n.li,{children:"State management for tracking robot status"}),"\n",(0,o.jsx)(n.li,{children:"Action planning and execution"}),"\n",(0,o.jsx)(n.li,{children:"Multimodal interaction capabilities"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"learning-review",children:"Learning Review"}),"\n",(0,o.jsx)(n.p,{children:"After completing this project, reflect on:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"How did the integration of vision, language, and action create emergent behaviors?"}),"\n",(0,o.jsx)(n.li,{children:"What challenges did you face when connecting different AI modalities?"}),"\n",(0,o.jsx)(n.li,{children:"How did the system handle ambiguous or complex commands?"}),"\n",(0,o.jsx)(n.li,{children:"What safety considerations are important for VLA systems?"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(n.p,{children:"This VLA system represents the culmination of all modules in this educational book:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"The ROS 2 foundation for communication"}),"\n",(0,o.jsx)(n.li,{children:"The simulation environment for testing"}),"\n",(0,o.jsx)(n.li,{children:"The perception pipeline for understanding"}),"\n",(0,o.jsx)(n.li,{children:"The VLA integration for intelligent interaction"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"You can extend this system by:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Adding more complex manipulation capabilities"}),"\n",(0,o.jsx)(n.li,{children:"Integrating with cloud-based AI services"}),"\n",(0,o.jsx)(n.li,{children:"Creating custom user interfaces"}),"\n",(0,o.jsx)(n.li,{children:"Deploying on real hardware platforms"}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var s=t(6540);const o={},a=s.createContext(o);function i(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);