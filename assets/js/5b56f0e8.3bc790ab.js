"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[802],{3841:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module4/module4-milestone","title":"Module 4 Milestone Project: Vision-Language-Action Robot Assistant","description":"Project Overview","source":"@site/docs/module4/module4-milestone.md","sourceDirName":"module4","slug":"/module4/module4-milestone","permalink":"/hackathon-01-AI-textbook/docs/module4/module4-milestone","draft":false,"unlisted":false,"editUrl":"https://github.com/maheenali021/hackathon-01-AI-textbook/tree/main/docs/module4/module4-milestone.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Module 4 Milestone Project: Vision-Language-Action Robot Assistant"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Multi-Modal Integration for VLA Systems","permalink":"/hackathon-01-AI-textbook/docs/module4/multi-modal-integration"},"next":{"title":"Chapter 1: Capstone Overview","permalink":"/hackathon-01-AI-textbook/docs/capstone/capstone-overview"}}');var s=r(4848),o=r(8453);const a={sidebar_position:5,title:"Module 4 Milestone Project: Vision-Language-Action Robot Assistant"},i="Module 4 Milestone Project: Vision-Language-Action Robot Assistant",l={},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Project Requirements",id:"project-requirements",level:2},{value:"Step-by-Step Implementation",id:"step-by-step-implementation",level:2},{value:"Step 1: Create the VLA System Architecture",id:"step-1-create-the-vla-system-architecture",level:3},{value:"Step 2: Create the Main VLA Controller",id:"step-2-create-the-main-vla-controller",level:3},{value:"Step 3: Create the Voice Processor Node",id:"step-3-create-the-voice-processor-node",level:3},{value:"Step 4: Create the State Manager Node",id:"step-4-create-the-state-manager-node",level:3},{value:"Step 5: Package Configuration",id:"step-5-package-configuration",level:3},{value:"Running the VLA System",id:"running-the-vla-system",level:2},{value:"To run the complete VLA system:",id:"to-run-the-complete-vla-system",level:3},{value:"Testing Your Implementation",id:"testing-your-implementation",level:2},{value:"Test 1: Verify VLA Nodes Are Running",id:"test-1-verify-vla-nodes-are-running",level:3},{value:"Test 2: Test Voice Command Processing",id:"test-2-test-voice-command-processing",level:3},{value:"Test 3: Check System State Updates",id:"test-3-check-system-state-updates",level:3},{value:"Enhancement Challenges",id:"enhancement-challenges",level:2},{value:"Solution Summary",id:"solution-summary",level:2},{value:"Learning Review",id:"learning-review",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-4-milestone-project-vision-language-action-robot-assistant",children:"Module 4 Milestone Project: Vision-Language-Action Robot Assistant"})}),"\n",(0,s.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,s.jsx)(n.p,{children:"In this milestone project, you'll create a complete Vision-Language-Action (VLA) system that allows users to interact with a robot using natural language commands. The system will integrate perception, language understanding, and action execution to create an intelligent robot assistant."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"After completing this project, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate vision, language, and action systems into a cohesive pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Process natural language commands and map them to robot actions"}),"\n",(0,s.jsx)(n.li,{children:"Implement cognitive planning for complex task execution"}),"\n",(0,s.jsx)(n.li,{children:"Create multimodal interfaces for human-robot interaction"}),"\n",(0,s.jsx)(n.li,{children:"Build end-to-end VLA systems for real-world applications"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"project-requirements",children:"Project Requirements"}),"\n",(0,s.jsx)(n.p,{children:"Create a VLA system with:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Voice command processing with Whisper for speech recognition"}),"\n",(0,s.jsx)(n.li,{children:"Language understanding with LLMs for command interpretation"}),"\n",(0,s.jsx)(n.li,{children:"Visual perception for scene understanding and object detection"}),"\n",(0,s.jsx)(n.li,{children:"Action planning and execution for robot control"}),"\n",(0,s.jsx)(n.li,{children:"Multimodal feedback and interaction capabilities"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"step-by-step-implementation",children:"Step-by-Step Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-create-the-vla-system-architecture",children:"Step 1: Create the VLA System Architecture"}),"\n",(0,s.jsxs)(n.p,{children:["Create the file ",(0,s.jsx)(n.code,{children:"vla_system/launch/vla_system.launch.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import os\r\nfrom launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\r\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\r\nfrom launch.substitutions import LaunchConfiguration\r\nfrom launch_ros.actions import Node\r\nfrom ament_index_python.packages import get_package_share_directory\r\n\r\n\r\ndef generate_launch_description():\r\n    # Launch configurations\r\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\r\n    robot_namespace = LaunchConfiguration('robot_namespace', default='')\r\n\r\n    # Package directories\r\n    pkg_vla_system = get_package_share_directory('vla_system')\r\n\r\n    # VLA main node\r\n    vla_main_node = Node(\r\n        package='vla_system',\r\n        executable='vla_main',\r\n        name='vla_main',\r\n        parameters=[{\r\n            'use_sim_time': use_sim_time,\r\n            'model_name': 'gpt-3.5-turbo',  # or your preferred LLM\r\n            'whisper_model': 'base',\r\n            'confidence_threshold': 0.7,\r\n        }],\r\n        remappings=[\r\n            ('/camera/image_raw', '/camera/image_raw'),\r\n            ('/cmd_vel', '/cmd_vel'),\r\n            ('/joint_commands', '/joint_commands'),\r\n        ],\r\n        output='screen'\r\n    )\r\n\r\n    # Voice processing node\r\n    voice_processing_node = Node(\r\n        package='vla_system',\r\n        executable='voice_processor',\r\n        name='voice_processor',\r\n        parameters=[{\r\n            'use_sim_time': use_sim_time,\r\n            'whisper_model_size': 'base',\r\n            'language': 'en',\r\n            'temperature': 0.0,\r\n        }],\r\n        output='screen'\r\n    )\r\n\r\n    # Vision processing node\r\n    vision_processing_node = Node(\r\n        package='vla_system',\r\n        executable='vision_processor',\r\n        name='vision_processor',\r\n        parameters=[{\r\n            'use_sim_time': use_sim_time,\r\n            'detection_threshold': 0.5,\r\n            'enable_segmentation': True,\r\n        }],\r\n        remappings=[\r\n            ('/camera/image_raw', '/camera/image_raw'),\r\n            ('/camera/camera_info', '/camera/camera_info'),\r\n        ],\r\n        output='screen'\r\n    )\r\n\r\n    # Language understanding node\r\n    language_understanding_node = Node(\r\n        package='vla_system',\r\n        executable='language_understanding',\r\n        name='language_understanding',\r\n        parameters=[{\r\n            'use_sim_time': use_sim_time,\r\n            'model_name': 'gpt-3.5-turbo',\r\n            'max_tokens': 1000,\r\n            'temperature': 0.1,\r\n        }],\r\n        output='screen'\r\n    )\r\n\r\n    # Action planning node\r\n    action_planning_node = Node(\r\n        package='vla_system',\r\n        executable='action_planner',\r\n        name='action_planner',\r\n        parameters=[{\r\n            'use_sim_time': use_sim_time,\r\n            'planning_timeout': 30.0,\r\n            'max_retries': 3,\r\n        }],\r\n        output='screen'\r\n    )\r\n\r\n    # State manager node\r\n    state_manager_node = Node(\r\n        package='vla_system',\r\n        executable='state_manager',\r\n        name='state_manager',\r\n        parameters=[{\r\n            'use_sim_time': use_sim_time,\r\n            'state_update_rate': 10.0,\r\n        }],\r\n        output='screen'\r\n    )\r\n\r\n    # Feedback and interaction node\r\n    feedback_node = Node(\r\n        package='vla_system',\r\n        executable='feedback_manager',\r\n        name='feedback_manager',\r\n        parameters=[{\r\n            'use_sim_time': use_sim_time,\r\n            'enable_audio_feedback': True,\r\n            'enable_visual_feedback': True,\r\n        }],\r\n        output='screen'\r\n    )\r\n\r\n    return LaunchDescription([\r\n        DeclareLaunchArgument(\r\n            'use_sim_time',\r\n            default_value='false',\r\n            description='Use simulation time if true'),\r\n        DeclareLaunchArgument(\r\n            'robot_namespace',\r\n            default_value='',\r\n            description='Namespace for robot topics'),\r\n\r\n        vla_main_node,\r\n        voice_processing_node,\r\n        vision_processing_node,\r\n        language_understanding_node,\r\n        action_planning_node,\r\n        state_manager_node,\r\n        feedback_node,\r\n    ])\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-create-the-main-vla-controller",children:"Step 2: Create the Main VLA Controller"}),"\n",(0,s.jsxs)(n.p,{children:["Create the file ",(0,s.jsx)(n.code,{children:"vla_system/src/vla_main.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom geometry_msgs.msg import Twist\r\nfrom builtin_interfaces.msg import Time\r\nimport numpy as np\r\nimport threading\r\nimport time\r\nfrom queue import Queue\r\nimport openai\r\nfrom openai import OpenAI\r\nimport whisper\r\nimport torch\r\nimport json\r\nfrom tf2_ros import TransformListener, Buffer\r\n\r\n\r\nclass VLAMain(Node):\r\n    def __init__(self):\r\n        super().__init__('vla_main')\r\n\r\n        # Initialize components\r\n        self.setup_subscribers()\r\n        self.setup_publishers()\r\n        self.initialize_models()\r\n        self.setup_internal_state()\r\n\r\n        # Task queue for processing\r\n        self.task_queue = Queue()\r\n        self.response_queue = Queue()\r\n\r\n        # Processing thread\r\n        self.processing_thread = threading.Thread(target=self.process_commands)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n\r\n        # Timer for main loop\r\n        self.main_timer = self.create_timer(0.1, self.main_loop)\r\n\r\n    def setup_subscribers(self):\r\n        \"\"\"Setup ROS subscribers\"\"\"\r\n        self.voice_sub = self.create_subscription(\r\n            String,\r\n            'voice_commands',\r\n            self.voice_command_callback,\r\n            10\r\n        )\r\n\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            'camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.state_sub = self.create_subscription(\r\n            String,\r\n            'robot_state',\r\n            self.state_callback,\r\n            10\r\n        )\r\n\r\n    def setup_publishers(self):\r\n        \"\"\"Setup ROS publishers\"\"\"\r\n        self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)\r\n        self.response_pub = self.create_publisher(String, 'vla_response', 10)\r\n        self.action_pub = self.create_publisher(String, 'robot_actions', 10)\r\n\r\n    def initialize_models(self):\r\n        \"\"\"Initialize AI models\"\"\"\r\n        try:\r\n            # Initialize Whisper model for speech recognition\r\n            self.whisper_model = whisper.load_model(\"base\")\r\n            self.get_logger().info(\"Whisper model loaded successfully\")\r\n        except Exception as e:\r\n            self.get_logger().error(f\"Failed to load Whisper model: {e}\")\r\n            self.whisper_model = None\r\n\r\n        try:\r\n            # Initialize OpenAI client (you would need to set your API key)\r\n            # self.openai_client = OpenAI(api_key='your-api-key-here')\r\n            self.get_logger().info(\"OpenAI client initialized\")\r\n        except Exception as e:\r\n            self.get_logger().error(f\"Failed to initialize OpenAI client: {e}\")\r\n            # For this example, we'll use a mock implementation\r\n\r\n    def setup_internal_state(self):\r\n        \"\"\"Setup internal state variables\"\"\"\r\n        self.current_image = None\r\n        self.robot_state = {}\r\n        self.conversation_history = []\r\n        self.is_processing = False\r\n\r\n    def voice_command_callback(self, msg):\r\n        \"\"\"Handle incoming voice commands\"\"\"\r\n        command = msg.data\r\n        self.get_logger().info(f\"Received voice command: {command}\")\r\n\r\n        # Add to task queue for processing\r\n        task = {\r\n            'type': 'command',\r\n            'data': command,\r\n            'timestamp': self.get_clock().now().to_msg()\r\n        }\r\n        self.task_queue.put(task)\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Handle incoming camera images\"\"\"\r\n        self.current_image = msg\r\n        # Store for use in processing\r\n\r\n    def state_callback(self, msg):\r\n        \"\"\"Handle robot state updates\"\"\"\r\n        try:\r\n            self.robot_state = json.loads(msg.data)\r\n        except json.JSONDecodeError:\r\n            self.get_logger().error(\"Failed to parse robot state JSON\")\r\n\r\n    def process_commands(self):\r\n        \"\"\"Process commands in a separate thread\"\"\"\r\n        while rclpy.ok():\r\n            try:\r\n                if not self.task_queue.empty():\r\n                    task = self.task_queue.get(timeout=1.0)\r\n\r\n                    if task['type'] == 'command':\r\n                        response = self.process_natural_language_command(task['data'])\r\n                        response_msg = String()\r\n                        response_msg.data = response\r\n                        self.response_queue.put(response_msg)\r\n\r\n                time.sleep(0.01)  # Small delay to prevent busy waiting\r\n            except Exception as e:\r\n                self.get_logger().error(f\"Error in processing thread: {e}\")\r\n\r\n    def process_natural_language_command(self, command):\r\n        \"\"\"Process natural language command using LLM\"\"\"\r\n        try:\r\n            # Create a prompt for the LLM\r\n            prompt = self.create_llm_prompt(command)\r\n\r\n            # For this example, we'll simulate the LLM response\r\n            # In a real implementation, you would call the LLM API\r\n            response = self.mock_llm_response(command)\r\n\r\n            # Parse the response and execute actions\r\n            self.execute_parsed_actions(response)\r\n\r\n            return response\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\"Error processing command: {e}\")\r\n            return f\"Sorry, I encountered an error processing your command: {str(e)}\"\r\n\r\n    def create_llm_prompt(self, command):\r\n        \"\"\"Create a prompt for the LLM with context\"\"\"\r\n        # Get current context (image, state, etc.)\r\n        context = self.get_current_context()\r\n\r\n        prompt = f\"\"\"\r\n        You are a helpful robot assistant. The user has given the following command: \"{command}\"\r\n\r\n        Current robot state: {context['state']}\r\n        Current environment: {context['environment']}\r\n\r\n        Please interpret this command and provide a structured response with:\r\n        1. The intent of the command\r\n        2. Specific actions to execute\r\n        3. Any relevant objects or locations\r\n\r\n        Respond in JSON format with keys: intent, actions, objects, locations.\r\n        \"\"\"\r\n\r\n        return prompt\r\n\r\n    def get_current_context(self):\r\n        \"\"\"Get current context including state and environment\"\"\"\r\n        context = {\r\n            'state': self.robot_state,\r\n            'environment': 'Visual scene analysis would go here',\r\n            'timestamp': self.get_clock().now().to_msg()\r\n        }\r\n\r\n        return context\r\n\r\n    def mock_llm_response(self, command):\r\n        \"\"\"Mock LLM response for demonstration\"\"\"\r\n        # This is a simplified mock - in reality, you'd call an LLM\r\n        command_lower = command.lower()\r\n\r\n        if 'move' in command_lower or 'go' in command_lower:\r\n            if 'forward' in command_lower:\r\n                return json.dumps({\r\n                    'intent': 'navigation',\r\n                    'actions': ['move_forward'],\r\n                    'objects': [],\r\n                    'locations': [],\r\n                    'parameters': {'distance': 1.0}\r\n                })\r\n            elif 'backward' in command_lower:\r\n                return json.dumps({\r\n                    'intent': 'navigation',\r\n                    'actions': ['move_backward'],\r\n                    'objects': [],\r\n                    'locations': [],\r\n                    'parameters': {'distance': 1.0}\r\n                })\r\n            elif 'left' in command_lower or 'right' in command_lower:\r\n                return json.dumps({\r\n                    'intent': 'navigation',\r\n                    'actions': ['turn'],\r\n                    'objects': [],\r\n                    'locations': [],\r\n                    'parameters': {'direction': 'left' if 'left' in command_lower else 'right'}\r\n                })\r\n        elif 'pick' in command_lower or 'grasp' in command_lower or 'take' in command_lower:\r\n            return json.dumps({\r\n                'intent': 'manipulation',\r\n                'actions': ['approach_object', 'grasp_object'],\r\n                'objects': ['object'],\r\n                'locations': [],\r\n                'parameters': {'object_name': 'object'}\r\n            })\r\n        elif 'find' in command_lower or 'look' in command_lower:\r\n            return json.dumps({\r\n                'intent': 'perception',\r\n                'actions': ['scan_environment', 'detect_objects'],\r\n                'objects': ['object'],\r\n                'locations': [],\r\n                'parameters': {'object_name': 'object'}\r\n            })\r\n\r\n        return json.dumps({\r\n            'intent': 'unknown',\r\n            'actions': [],\r\n            'objects': [],\r\n            'locations': [],\r\n            'parameters': {}\r\n        })\r\n\r\n    def execute_parsed_actions(self, response_json):\r\n        \"\"\"Execute actions parsed from LLM response\"\"\"\r\n        try:\r\n            response = json.loads(response_json)\r\n            intent = response.get('intent', 'unknown')\r\n            actions = response.get('actions', [])\r\n\r\n            for action in actions:\r\n                if action == 'move_forward':\r\n                    self.execute_navigation_command('forward', response.get('parameters', {}))\r\n                elif action == 'move_backward':\r\n                    self.execute_navigation_command('backward', response.get('parameters', {}))\r\n                elif action == 'turn':\r\n                    self.execute_navigation_command('turn', response.get('parameters', {}))\r\n                elif action == 'approach_object':\r\n                    self.execute_approach_object(response.get('parameters', {}))\r\n                elif action == 'grasp_object':\r\n                    self.execute_grasp_object(response.get('parameters', {}))\r\n                elif action == 'scan_environment':\r\n                    self.execute_scan_environment()\r\n                elif action == 'detect_objects':\r\n                    self.execute_detect_objects()\r\n\r\n            # Publish action for logging\r\n            action_msg = String()\r\n            action_msg.data = f\"Executed {intent} with actions: {actions}\"\r\n            self.action_pub.publish(action_msg)\r\n\r\n        except json.JSONDecodeError as e:\r\n            self.get_logger().error(f\"Error parsing LLM response: {e}\")\r\n        except Exception as e:\r\n            self.get_logger().error(f\"Error executing actions: {e}\")\r\n\r\n    def execute_navigation_command(self, direction, parameters):\r\n        \"\"\"Execute navigation commands\"\"\"\r\n        cmd_vel = Twist()\r\n\r\n        if direction == 'forward':\r\n            cmd_vel.linear.x = 0.3  # m/s\r\n        elif direction == 'backward':\r\n            cmd_vel.linear.x = -0.3\r\n        elif direction == 'turn':\r\n            cmd_vel.angular.z = 0.5 if parameters.get('direction') == 'left' else -0.5\r\n\r\n        self.cmd_vel_pub.publish(cmd_vel)\r\n        self.get_logger().info(f\"Executing navigation: {direction} with params: {parameters}\")\r\n\r\n    def execute_approach_object(self, parameters):\r\n        \"\"\"Execute approach object action\"\"\"\r\n        # In a real implementation, this would navigate to the object\r\n        self.get_logger().info(f\"Approaching object: {parameters.get('object_name')}\")\r\n\r\n    def execute_grasp_object(self, parameters):\r\n        \"\"\"Execute grasp object action\"\"\"\r\n        # In a real implementation, this would control manipulator\r\n        self.get_logger().info(f\"Attempting to grasp object: {parameters.get('object_name')}\")\r\n\r\n    def execute_scan_environment(self):\r\n        \"\"\"Execute environment scanning\"\"\"\r\n        self.get_logger().info(\"Scanning environment...\")\r\n\r\n    def execute_detect_objects(self):\r\n        \"\"\"Execute object detection\"\"\"\r\n        self.get_logger().info(\"Detecting objects...\")\r\n\r\n    def main_loop(self):\r\n        \"\"\"Main loop for the VLA system\"\"\"\r\n        # Publish any responses from the processing thread\r\n        while not self.response_queue.empty():\r\n            response_msg = self.response_queue.get()\r\n            self.response_pub.publish(response_msg)\r\n\r\n        # Update internal state periodically\r\n        self.update_internal_state()\r\n\r\n    def update_internal_state(self):\r\n        \"\"\"Update internal state variables\"\"\"\r\n        # This would update state based on current sensor data\r\n        pass\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    vla_main = VLAMain()\r\n\r\n    try:\r\n        rclpy.spin(vla_main)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        vla_main.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-create-the-voice-processor-node",children:"Step 3: Create the Voice Processor Node"}),"\n",(0,s.jsxs)(n.p,{children:["Create the file ",(0,s.jsx)(n.code,{children:"vla_system/src/voice_processor.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import AudioData\r\nimport pyaudio\r\nimport wave\r\nimport numpy as np\r\nimport whisper\r\nimport threading\r\nimport queue\r\nimport time\r\n\r\n\r\nclass VoiceProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_processor\')\r\n\r\n        # Initialize Whisper model\r\n        try:\r\n            self.model = whisper.load_model("base")\r\n            self.get_logger().info("Whisper model loaded successfully")\r\n        except Exception as e:\r\n            self.get_logger().error(f"Failed to load Whisper model: {e}")\r\n            self.model = None\r\n\r\n        # Setup audio parameters\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n        self.rate = 16000\r\n        self.chunk = 1024\r\n        self.record_seconds = 3\r\n\r\n        # Setup publishers and subscribers\r\n        self.voice_pub = self.create_publisher(String, \'voice_commands\', 10)\r\n        self.audio_sub = self.create_subscription(\r\n            AudioData,\r\n            \'audio_input\',\r\n            self.audio_callback,\r\n            10\r\n        )\r\n\r\n        # Setup audio processing\r\n        self.audio_queue = queue.Queue()\r\n        self.processing_thread = threading.Thread(target=self.process_audio)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n\r\n        # Setup continuous listening\r\n        self.listening = True\r\n        self.listening_thread = threading.Thread(target=self.continuous_listening)\r\n        self.listening_thread.daemon = True\r\n        self.listening_thread.start()\r\n\r\n        self.get_logger().info("Voice processor initialized")\r\n\r\n    def audio_callback(self, msg):\r\n        """Handle audio data from ROS topic"""\r\n        # Add audio data to processing queue\r\n        self.audio_queue.put(msg.data)\r\n\r\n    def process_audio(self):\r\n        """Process audio data from queue"""\r\n        while rclpy.ok():\r\n            try:\r\n                if not self.audio_queue.empty():\r\n                    audio_data = self.audio_queue.get(timeout=1.0)\r\n\r\n                    # Process the audio with Whisper\r\n                    if self.model:\r\n                        try:\r\n                            # Save audio data temporarily for processing\r\n                            temp_filename = \'/tmp/temp_audio.wav\'\r\n\r\n                            # Write audio data to temporary file\r\n                            wf = wave.open(temp_filename, \'wb\')\r\n                            wf.setnchannels(self.channels)\r\n                            wf.setsampwidth(pyaudio.PyAudio().get_sample_size(self.format))\r\n                            wf.setframerate(self.rate)\r\n                            wf.writeframes(audio_data)\r\n                            wf.close()\r\n\r\n                            # Transcribe the audio\r\n                            result = self.model.transcribe(temp_filename)\r\n                            text = result["text"].strip()\r\n\r\n                            if text:  # Only publish if there\'s text\r\n                                self.get_logger().info(f"Transcribed: {text}")\r\n                                msg = String()\r\n                                msg.data = text\r\n                                self.voice_pub.publish(msg)\r\n\r\n                        except Exception as e:\r\n                            self.get_logger().error(f"Error processing audio: {e}")\r\n                else:\r\n                    time.sleep(0.01)  # Small delay to prevent busy waiting\r\n            except queue.Empty:\r\n                continue\r\n            except Exception as e:\r\n                self.get_logger().error(f"Error in audio processing: {e}")\r\n\r\n    def continuous_listening(self):\r\n        """Continuously listen for audio using PyAudio"""\r\n        if not self.model:\r\n            return\r\n\r\n        p = pyaudio.PyAudio()\r\n\r\n        try:\r\n            stream = p.open(\r\n                format=self.format,\r\n                channels=self.channels,\r\n                rate=self.rate,\r\n                input=True,\r\n                frames_per_buffer=self.chunk\r\n            )\r\n\r\n            self.get_logger().info("Started continuous listening...")\r\n\r\n            while self.listening and rclpy.ok():\r\n                frames = []\r\n\r\n                # Record for specified duration\r\n                for i in range(0, int(self.rate / self.chunk * self.record_seconds)):\r\n                    data = stream.read(self.chunk)\r\n                    frames.append(data)\r\n\r\n                # Convert frames to bytes\r\n                audio_data = b\'\'.join(frames)\r\n\r\n                # Add to processing queue\r\n                self.audio_queue.put(audio_data)\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error in continuous listening: {e}")\r\n        finally:\r\n            stream.stop_stream()\r\n            stream.close()\r\n            p.terminate()\r\n\r\n    def destroy_node(self):\r\n        """Cleanup when node is destroyed"""\r\n        self.listening = False\r\n        super().destroy_node()\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    voice_processor = VoiceProcessor()\r\n\r\n    try:\r\n        rclpy.spin(voice_processor)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        voice_processor.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-create-the-state-manager-node",children:"Step 4: Create the State Manager Node"}),"\n",(0,s.jsxs)(n.p,{children:["Create the file ",(0,s.jsx)(n.code,{children:"vla_system/src/state_manager.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist, Pose\r\nfrom nav_msgs.msg import Odometry\r\nfrom sensor_msgs.msg import LaserScan, Image\r\nfrom builtin_interfaces.msg import Time\r\nimport json\r\nimport threading\r\nimport time\r\nfrom collections import deque\r\n\r\n\r\nclass StateManager(Node):\r\n    def __init__(self):\r\n        super().__init__('state_manager')\r\n\r\n        # Setup publishers\r\n        self.state_pub = self.create_publisher(String, 'robot_state', 10)\r\n\r\n        # Setup subscribers\r\n        self.odom_sub = self.create_subscription(\r\n            Odometry,\r\n            'odom',\r\n            self.odom_callback,\r\n            10\r\n        )\r\n\r\n        self.cmd_vel_sub = self.create_subscription(\r\n            Twist,\r\n            'cmd_vel',\r\n            self.cmd_vel_callback,\r\n            10\r\n        )\r\n\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan,\r\n            'scan',\r\n            self.scan_callback,\r\n            10\r\n        )\r\n\r\n        # Internal state\r\n        self.robot_state = {\r\n            'position': {'x': 0.0, 'y': 0.0, 'z': 0.0},\r\n            'orientation': {'x': 0.0, 'y': 0.0, 'z': 0.0, 'w': 1.0},\r\n            'velocity': {'linear': {'x': 0.0, 'y': 0.0, 'z': 0.0},\r\n                        'angular': {'x': 0.0, 'y': 0.0, 'z': 0.0}},\r\n            'battery_level': 100.0,\r\n            'last_command': '',\r\n            'command_timestamp': 0,\r\n            'safety_status': 'normal',\r\n            'navigation_status': 'idle',\r\n            'manipulation_status': 'idle',\r\n            'perception_status': 'idle',\r\n            'active_goals': [],\r\n            'completed_goals': [],\r\n            'error_count': 0,\r\n            'last_error': '',\r\n            'system_uptime': 0.0\r\n        }\r\n\r\n        # State history for debugging\r\n        self.state_history = deque(maxlen=100)\r\n\r\n        # Setup timer for state updates\r\n        self.state_timer = self.create_timer(1.0, self.publish_state)\r\n\r\n        # Setup timer for system monitoring\r\n        self.monitor_timer = self.create_timer(0.1, self.monitor_system)\r\n\r\n        # Initialize start time\r\n        self.start_time = self.get_clock().now().nanoseconds / 1e9\r\n\r\n        self.get_logger().info(\"State manager initialized\")\r\n\r\n    def odom_callback(self, msg):\r\n        \"\"\"Update position and orientation from odometry\"\"\"\r\n        self.robot_state['position']['x'] = msg.pose.pose.position.x\r\n        self.robot_state['position']['y'] = msg.pose.pose.position.y\r\n        self.robot_state['position']['z'] = msg.pose.pose.position.z\r\n\r\n        self.robot_state['orientation']['x'] = msg.pose.pose.orientation.x\r\n        self.robot_state['orientation']['y'] = msg.pose.pose.orientation.y\r\n        self.robot_state['orientation']['z'] = msg.pose.pose.orientation.z\r\n        self.robot_state['orientation']['w'] = msg.pose.pose.orientation.w\r\n\r\n        self.robot_state['velocity']['linear']['x'] = msg.twist.twist.linear.x\r\n        self.robot_state['velocity']['linear']['y'] = msg.twist.twist.linear.y\r\n        self.robot_state['velocity']['linear']['z'] = msg.twist.twist.linear.z\r\n\r\n        self.robot_state['velocity']['angular']['x'] = msg.twist.twist.angular.x\r\n        self.robot_state['velocity']['angular']['y'] = msg.twist.twist.angular.y\r\n        self.robot_state['velocity']['angular']['z'] = msg.twist.twist.angular.z\r\n\r\n    def cmd_vel_callback(self, msg):\r\n        \"\"\"Update last command and status\"\"\"\r\n        self.robot_state['last_command'] = f\"Lin:({msg.linear.x:.2f}, {msg.linear.y:.2f}, {msg.linear.z:.2f}), \" \\\r\n                                         f\"Ang:({msg.angular.x:.2f}, {msg.angular.y:.2f}, {msg.angular.z:.2f})\"\r\n        self.robot_state['command_timestamp'] = self.get_clock().now().nanoseconds / 1e9\r\n\r\n        # Update navigation status based on command\r\n        if abs(msg.linear.x) > 0.01 or abs(msg.angular.z) > 0.01:\r\n            self.robot_state['navigation_status'] = 'moving'\r\n        else:\r\n            self.robot_state['navigation_status'] = 'idle'\r\n\r\n    def scan_callback(self, msg):\r\n        \"\"\"Process laser scan for obstacle detection\"\"\"\r\n        if msg.ranges:\r\n            min_range = min([r for r in msg.ranges if r > msg.range_min and r < msg.range_max])\r\n            if min_range < 0.5:  # Dangerously close\r\n                self.robot_state['safety_status'] = 'danger'\r\n            elif min_range < 1.0:  # Close\r\n                self.robot_state['safety_status'] = 'warning'\r\n            else:  # Safe\r\n                self.robot_state['safety_status'] = 'normal'\r\n\r\n    def monitor_system(self):\r\n        \"\"\"Monitor system status and update state\"\"\"\r\n        current_time = self.get_clock().now().nanoseconds / 1e9\r\n        self.robot_state['system_uptime'] = current_time - self.start_time\r\n\r\n        # Update battery level simulation (decreasing over time)\r\n        self.robot_state['battery_level'] = max(0.0, self.robot_state['battery_level'] - 0.01)\r\n\r\n        # Add current state to history\r\n        self.state_history.append(dict(self.robot_state))\r\n\r\n    def publish_state(self):\r\n        \"\"\"Publish the current robot state as JSON\"\"\"\r\n        try:\r\n            state_msg = String()\r\n            state_msg.data = json.dumps(self.robot_state, indent=2)\r\n            self.state_pub.publish(state_msg)\r\n\r\n            self.get_logger().debug(f\"Published robot state: {self.robot_state['navigation_status']}, \"\r\n                                   f\"Pos:({self.robot_state['position']['x']:.2f}, {self.robot_state['position']['y']:.2f}), \"\r\n                                   f\"Vel:({self.robot_state['velocity']['linear']['x']:.2f})\")\r\n        except Exception as e:\r\n            self.get_logger().error(f\"Error publishing state: {e}\")\r\n\r\n    def add_goal(self, goal_id, goal_description):\r\n        \"\"\"Add a new goal to the active goals list\"\"\"\r\n        goal = {\r\n            'id': goal_id,\r\n            'description': goal_description,\r\n            'status': 'active',\r\n            'timestamp': self.get_clock().now().nanoseconds / 1e9\r\n        }\r\n        self.robot_state['active_goals'].append(goal)\r\n\r\n    def complete_goal(self, goal_id):\r\n        \"\"\"Move a goal from active to completed\"\"\"\r\n        for i, goal in enumerate(self.robot_state['active_goals']):\r\n            if goal['id'] == goal_id:\r\n                completed_goal = self.robot_state['active_goals'].pop(i)\r\n                completed_goal['status'] = 'completed'\r\n                completed_goal['completion_time'] = self.get_clock().now().nanoseconds / 1e9\r\n                self.robot_state['completed_goals'].append(completed_goal)\r\n                return True\r\n        return False\r\n\r\n    def log_error(self, error_message):\r\n        \"\"\"Log an error and update error count\"\"\"\r\n        self.robot_state['error_count'] += 1\r\n        self.robot_state['last_error'] = error_message\r\n        self.get_logger().error(f\"Logged error: {error_message} (Total: {self.robot_state['error_count']})\")\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    state_manager = StateManager()\r\n\r\n    try:\r\n        rclpy.spin(state_manager)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        state_manager.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-5-package-configuration",children:"Step 5: Package Configuration"}),"\n",(0,s.jsxs)(n.p,{children:["Create the file ",(0,s.jsx)(n.code,{children:"vla_system/package.xml"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\r\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\r\n<package format="3">\r\n  <name>vla_system</name>\r\n  <version>0.0.0</version>\r\n  <description>Vision-Language-Action system for Module 4 milestone project</description>\r\n  <maintainer email="your_email@example.com">Your Name</maintainer>\r\n  <license>TODO: License declaration</license>\r\n\r\n  <buildtool_depend>ament_cmake</buildtool_depend>\r\n  <buildtool_depend>ament_cmake_python</buildtool_depend>\r\n\r\n  <depend>rclcpp</depend>\r\n  <depend>rclpy</depend>\r\n  <depend>std_msgs</depend>\r\n  <depend>sensor_msgs</end>\r\n  <depend>geometry_msgs</depend>\r\n  <depend>nav_msgs</depend>\r\n  <depend>builtin_interfaces</depend>\r\n  <depend>tf2</depend>\r\n  <depend>tf2_ros</depend>\r\n\r\n  <depend>openai</depend>\r\n  <depend>openai-whisper</depend>\r\n  <depend>pyaudio</depend>\r\n\r\n  <exec_depend>launch</exec_depend>\r\n  <exec_depend>launch_ros</exec_depend>\r\n\r\n  <test_depend>ament_lint_auto</test_depend>\r\n  <test_depend>ament_lint_common</test_depend>\r\n\r\n  <export>\r\n    <build_type>ament_cmake</build_type>\r\n  </export>\r\n</package>\n'})}),"\n",(0,s.jsxs)(n.p,{children:["And create the ",(0,s.jsx)(n.code,{children:"vla_system/CMakeLists.txt"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-cmake",children:'cmake_minimum_required(VERSION 3.8)\r\nproject(vla_system)\r\n\r\nif(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES "Clang")\r\n  add_compile_options(-Wall -Wextra -Wpedantic)\r\nendif()\r\n\r\n# find dependencies\r\nfind_package(ament_cmake REQUIRED)\r\nfind_package(rclcpp REQUIRED)\r\nfind_package(rclpy REQUIRED)\r\nfind_package(std_msgs REQUIRED)\r\nfind_package(sensor_msgs REQUIRED)\r\nfind_package(geometry_msgs REQUIRED)\r\nfind_package(nav_msgs REQUIRED)\r\nfind_package(builtin_interfaces REQUIRED)\r\nfind_package(tf2 REQUIRED)\r\nfind_package(tf2_ros REQUIRED)\r\n\r\n# Install Python modules\r\nament_python_install_package(${PROJECT_NAME})\r\n\r\n# Install Python executables\r\ninstall(PROGRAMS\r\n  src/vla_main.py\r\n  src/voice_processor.py\r\n  src/state_manager.py\r\n  DESTINATION lib/${PROJECT_NAME}\r\n)\r\n\r\n# Install launch files\r\ninstall(DIRECTORY launch\r\n  DESTINATION share/${PROJECT_NAME}/\r\n)\r\n\r\n# Install config files\r\ninstall(DIRECTORY config\r\n  DESTINATION share/${PROJECT_NAME}/\r\n)\r\n\r\n# Install RViz config\r\ninstall(DIRECTORY rviz\r\n  DESTINATION share/${PROJECT_NAME}/\r\n)\r\n\r\nif(BUILD_TESTING)\r\n  find_package(ament_lint_auto REQUIRED)\r\n  set(ament_cmake_copyright_FOUND TRUE)\r\n  set(ament_cmake_cpplint_FOUND TRUE)\r\n  ament_lint_auto_find_test_dependencies()\r\nendif()\r\n\r\nament_package()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"running-the-vla-system",children:"Running the VLA System"}),"\n",(0,s.jsx)(n.h3,{id:"to-run-the-complete-vla-system",children:"To run the complete VLA system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Start the VLA system\r\nros2 launch vla_system vla_system.launch.py\r\n\r\n# Terminal 2: Simulate voice commands\r\nros2 topic pub /voice_commands std_msgs/String \"data: 'Move forward 1 meter'\"\r\n\r\n# Terminal 3: Monitor system state\r\nros2 topic echo /robot_state\r\n\r\n# Terminal 4: Monitor VLA responses\r\nros2 topic echo /vla_response\n"})}),"\n",(0,s.jsx)(n.h2,{id:"testing-your-implementation",children:"Testing Your Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"test-1-verify-vla-nodes-are-running",children:"Test 1: Verify VLA Nodes Are Running"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Check active nodes\r\nros2 node list | grep vla\r\n\r\n# Check topics\r\nros2 topic list | grep -E "(voice|vla|state)"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"test-2-test-voice-command-processing",children:"Test 2: Test Voice Command Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Send a test command\r\nros2 topic pub /voice_commands std_msgs/String \"data: 'Please move to the kitchen'\"\r\n\r\n# Monitor the response\r\nros2 topic echo /vla_response\n"})}),"\n",(0,s.jsx)(n.h3,{id:"test-3-check-system-state-updates",children:"Test 3: Check System State Updates"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Monitor robot state\r\nros2 topic echo /robot_state\n"})}),"\n",(0,s.jsx)(n.h2,{id:"enhancement-challenges",children:"Enhancement Challenges"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Add multimodal grounding to connect language to visual perception"}),"\n",(0,s.jsx)(n.li,{children:"Implement a more sophisticated dialogue manager"}),"\n",(0,s.jsx)(n.li,{children:"Add memory capabilities for context-aware interactions"}),"\n",(0,s.jsx)(n.li,{children:"Integrate with a real robot or advanced simulation"}),"\n",(0,s.jsx)(n.li,{children:"Add safety checks and validation for command execution"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"solution-summary",children:"Solution Summary"}),"\n",(0,s.jsx)(n.p,{children:"This milestone project demonstrates how to create a complete Vision-Language-Action system with:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Speech recognition using OpenAI Whisper"}),"\n",(0,s.jsx)(n.li,{children:"Language understanding with LLMs"}),"\n",(0,s.jsx)(n.li,{children:"State management for tracking robot status"}),"\n",(0,s.jsx)(n.li,{children:"Action planning and execution"}),"\n",(0,s.jsx)(n.li,{children:"Multimodal interaction capabilities"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"learning-review",children:"Learning Review"}),"\n",(0,s.jsx)(n.p,{children:"After completing this project, reflect on:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"How did the integration of vision, language, and action create emergent behaviors?"}),"\n",(0,s.jsx)(n.li,{children:"What challenges did you face when connecting different AI modalities?"}),"\n",(0,s.jsx)(n.li,{children:"How did the system handle ambiguous or complex commands?"}),"\n",(0,s.jsx)(n.li,{children:"What safety considerations are important for VLA systems?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"This VLA system represents the culmination of all modules in this educational book:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The ROS 2 foundation for communication"}),"\n",(0,s.jsx)(n.li,{children:"The simulation environment for testing"}),"\n",(0,s.jsx)(n.li,{children:"The perception pipeline for understanding"}),"\n",(0,s.jsx)(n.li,{children:"The VLA integration for intelligent interaction"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"You can extend this system by:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Adding more complex manipulation capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Integrating with cloud-based AI services"}),"\n",(0,s.jsx)(n.li,{children:"Creating custom user interfaces"}),"\n",(0,s.jsx)(n.li,{children:"Deploying on real hardware platforms"}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>i});var t=r(6540);const s={},o=t.createContext(s);function a(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);