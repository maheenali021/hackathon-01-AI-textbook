"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[476],{2241:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module4/cognitive-planning","title":"Chapter 3: Cognitive Planning with Large Language Models","description":"Introduction to Cognitive Planning","source":"@site/docs/module4/cognitive-planning.md","sourceDirName":"module4","slug":"/module4/cognitive-planning","permalink":"/hackathon-01-AI-textbook/docs/module4/cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/maheenali021/hackathon-01-AI-textbook/tree/main/docs/module4/cognitive-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Chapter 3: Cognitive Planning with Large Language Models"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Voice-to-Action with OpenAI Whisper","permalink":"/hackathon-01-AI-textbook/docs/module4/voice-to-action"},"next":{"title":"Chapter 4: Multi-Modal Integration for VLA Systems","permalink":"/hackathon-01-AI-textbook/docs/module4/multi-modal-integration"}}');var i=t(4848),o=t(8453);const s={sidebar_position:3,title:"Chapter 3: Cognitive Planning with Large Language Models"},r="Cognitive Planning with Large Language Models",l={},c=[{value:"Introduction to Cognitive Planning",id:"introduction-to-cognitive-planning",level:2},{value:"Planning Architecture with LLMs",id:"planning-architecture-with-llms",level:2},{value:"Cognitive Planning Pipeline",id:"cognitive-planning-pipeline",level:3},{value:"Hierarchical Task Planning",id:"hierarchical-task-planning",level:2},{value:"High-Level Task Decomposition",id:"high-level-task-decomposition",level:3},{value:"Symbolic Reasoning and Knowledge Representation",id:"symbolic-reasoning-and-knowledge-representation",level:2},{value:"Knowledge Graph Integration",id:"knowledge-graph-integration",level:3},{value:"Planning with Uncertainty",id:"planning-with-uncertainty",level:2},{value:"Probabilistic Planning",id:"probabilistic-planning",level:3},{value:"Learning from Execution",id:"learning-from-execution",level:2},{value:"Plan Refinement and Adaptation",id:"plan-refinement-and-adaptation",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2}];function p(n){const e={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"cognitive-planning-with-large-language-models",children:"Cognitive Planning with Large Language Models"})}),"\n",(0,i.jsx)(e.h2,{id:"introduction-to-cognitive-planning",children:"Introduction to Cognitive Planning"}),"\n",(0,i.jsx)(e.p,{children:"Cognitive planning in robotics involves using artificial intelligence to create high-level plans that allow robots to achieve complex goals. Large Language Models (LLMs) provide new opportunities for robots to understand natural language commands and generate executable plans."}),"\n",(0,i.jsx)(e.h2,{id:"planning-architecture-with-llms",children:"Planning Architecture with LLMs"}),"\n",(0,i.jsx)(e.h3,{id:"cognitive-planning-pipeline",children:"Cognitive Planning Pipeline"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\nfrom nav2_msgs.action import NavigateToPose\nimport openai\nimport json\nimport re\nfrom typing import List, Dict, Any\n\nclass CognitivePlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'cognitive_planning_node\')\n\n        # Subscribers for natural language commands\n        self.command_sub = self.create_subscription(\n            String, \'natural_language_commands\', self.command_callback, 10)\n\n        # Publishers for plan status and feedback\n        self.plan_status_pub = self.create_publisher(String, \'plan_status\', 10)\n        self.feedback_pub = self.create_publisher(String, \'planning_feedback\', 10)\n\n        # Action clients for robot execution\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # LLM configuration\n        self.llm_model = "gpt-3.5-turbo"  # or "gpt-4" for more complex tasks\n        self.max_tokens = 1000\n        self.temperature = 0.1  # Low temperature for more deterministic planning\n\n        # Robot state and capabilities\n        self.robot_capabilities = [\n            "navigation", "manipulation", "perception",\n            "communication", "grasping", "transporting"\n        ]\n\n        # Environment representation\n        self.environment_map = self.initialize_environment_map()\n        self.known_objects = self.initialize_known_objects()\n        self.known_locations = self.initialize_known_locations()\n\n        self.get_logger().info(\'Cognitive planning node initialized\')\n\n    def initialize_environment_map(self):\n        """Initialize environment map with known locations"""\n        return {\n            "kitchen": {"x": 1.0, "y": 2.0, "theta": 0.0},\n            "living_room": {"x": 3.0, "y": 1.0, "theta": 0.0},\n            "bedroom": {"x": 5.0, "y": 3.0, "theta": 0.0},\n            "office": {"x": 2.0, "y": 5.0, "theta": 0.0},\n            "charging_station": {"x": 0.0, "y": 0.0, "theta": 0.0}\n        }\n\n    def initialize_known_objects(self):\n        """Initialize known objects in the environment"""\n        return {\n            "cup": {"type": "drinkware", "graspable": True},\n            "book": {"type": "reading_material", "graspable": True},\n            "phone": {"type": "electronics", "graspable": True},\n            "keys": {"type": "personal_item", "graspable": True},\n            "water_bottle": {"type": "drinkware", "graspable": True}\n        }\n\n    def initialize_known_locations(self):\n        """Initialize known object locations"""\n        return {\n            "kitchen_counter": ["cup", "water_bottle"],\n            "coffee_table": ["book", "phone"],\n            "desk": ["keys", "book"],\n            "bedside_table": ["phone", "book"]\n        }\n\n    def command_callback(self, msg):\n        """Process natural language command"""\n        command = msg.data\n        self.get_logger().info(f\'Received command: {command}\')\n\n        # Generate plan using LLM\n        plan = self.generate_plan_with_llm(command)\n\n        if plan:\n            self.execute_plan(plan)\n        else:\n            self.get_logger().error(f\'Could not generate plan for command: {command}\')\n\n    def generate_plan_with_llm(self, command: str) -> List[Dict[str, Any]]:\n        """Generate a plan using Large Language Model"""\n        try:\n            # Construct prompt for the LLM\n            prompt = self.construct_planning_prompt(command)\n\n            # Call the LLM (this would use your API key in practice)\n            response = self.call_llm(prompt)\n\n            # Parse the response into a structured plan\n            plan = self.parse_llm_response(response)\n\n            self.get_logger().info(f\'Generated plan: {plan}\')\n            return plan\n\n        except Exception as e:\n            self.get_logger().error(f\'Error generating plan: {e}\')\n            return None\n\n    def construct_planning_prompt(self, command: str) -> str:\n        """Construct the prompt for the LLM"""\n        prompt = f"""\n        You are a cognitive planning system for a household robot. Your task is to create a detailed execution plan for the given command.\n\n        Current environment knowledge:\n        - Locations: {list(self.environment_map.keys())}\n        - Known objects: {list(self.known_objects.keys())}\n        - Object locations: {self.known_locations}\n        - Robot capabilities: {self.robot_capabilities}\n\n        Command: "{command}"\n\n        Please provide a step-by-step plan in JSON format with the following structure:\n        {{\n            "steps": [\n                {{\n                    "step_number": 1,\n                    "action": "action_name",\n                    "target_object": "object_name",\n                    "target_location": "location_name",\n                    "description": "Detailed description of the step",\n                    "dependencies": ["previous_step_numbers_if_any"]\n                }}\n            ],\n            "estimated_duration": "Estimated time in seconds",\n            "potential_issues": ["List of potential issues"]\n        }}\n\n        The available actions are: navigate_to, pick_up, place_down, detect_object, communicate, wait, charge_battery\n        """\n\n        return prompt\n\n    def call_llm(self, prompt: str) -> str:\n        """Call the Large Language Model"""\n        # In a real implementation, this would call the LLM API\n        # For this example, we\'ll simulate a response\n        return self.simulate_llm_response(prompt)\n\n    def simulate_llm_response(self, prompt: str) -> str:\n        """Simulate LLM response for demonstration"""\n        # This is a simplified simulation\n        # In practice, you would call the actual LLM API\n        if "bring me the cup" in prompt.lower():\n            return json.dumps({\n                "steps": [\n                    {\n                        "step_number": 1,\n                        "action": "navigate_to",\n                        "target_location": "kitchen",\n                        "description": "Navigate to the kitchen to find the cup",\n                        "dependencies": []\n                    },\n                    {\n                        "step_number": 2,\n                        "action": "detect_object",\n                        "target_object": "cup",\n                        "description": "Detect and locate the cup on the counter",\n                        "dependencies": [1]\n                    },\n                    {\n                        "step_number": 3,\n                        "action": "pick_up",\n                        "target_object": "cup",\n                        "description": "Pick up the cup from the counter",\n                        "dependencies": [2]\n                    },\n                    {\n                        "step_number": 4,\n                        "action": "navigate_to",\n                        "target_location": "user_location",\n                        "description": "Navigate back to the user with the cup",\n                        "dependencies": [3]\n                    },\n                    {\n                        "step_number": 5,\n                        "action": "place_down",\n                        "target_object": "cup",\n                        "description": "Place the cup down near the user",\n                        "dependencies": [4]\n                    }\n                ],\n                "estimated_duration": "120",\n                "potential_issues": ["Cup might not be in expected location", "Path might be blocked"]\n            })\n        else:\n            # Default response for other commands\n            return json.dumps({\n                "steps": [],\n                "estimated_duration": "0",\n                "potential_issues": ["Command not understood"]\n            })\n\n    def parse_llm_response(self, response: str) -> List[Dict[str, Any]]:\n        """Parse the LLM response into a structured plan"""\n        try:\n            parsed_response = json.loads(response)\n            return parsed_response.get(\'steps\', [])\n        except json.JSONDecodeError:\n            self.get_logger().error(f\'Could not parse LLM response: {response}\')\n            return None\n\n    def execute_plan(self, plan: List[Dict[str, Any]]):\n        """Execute the generated plan"""\n        self.get_logger().info(f\'Executing plan with {len(plan)} steps\')\n\n        for step in plan:\n            success = self.execute_plan_step(step)\n            if not success:\n                self.get_logger().error(f\'Plan execution failed at step: {step}\')\n                break\n\n        self.get_logger().info(\'Plan execution completed\')\n\n    def execute_plan_step(self, step: Dict[str, Any]) -> bool:\n        """Execute a single step of the plan"""\n        action = step[\'action\']\n        self.get_logger().info(f\'Executing step {step["step_number"]}: {action}\')\n\n        if action == \'navigate_to\':\n            return self.execute_navigation_step(step)\n        elif action == \'pick_up\':\n            return self.execute_pickup_step(step)\n        elif action == \'place_down\':\n            return self.execute_placement_step(step)\n        elif action == \'detect_object\':\n            return self.execute_detection_step(step)\n        elif action == \'communicate\':\n            return self.execute_communication_step(step)\n        elif action == \'wait\':\n            return self.execute_wait_step(step)\n        elif action == \'charge_battery\':\n            return self.execute_charging_step(step)\n        else:\n            self.get_logger().error(f\'Unknown action: {action}\')\n            return False\n\n    def execute_navigation_step(self, step: Dict[str, Any]) -> bool:\n        """Execute navigation step"""\n        target_location = step[\'target_location\']\n\n        if target_location == \'user_location\':\n            # In practice, this would get user location from localization\n            target_pose = self.get_user_location()\n        else:\n            location_data = self.environment_map.get(target_location)\n            if not location_data:\n                self.get_logger().error(f\'Unknown location: {target_location}\')\n                return False\n\n            target_pose = self.create_pose_stamped(\n                location_data[\'x\'],\n                location_data[\'y\'],\n                location_data[\'theta\']\n            )\n\n        return self.navigate_to_pose(target_pose)\n\n    def execute_pickup_step(self, step: Dict[str, Any]) -> bool:\n        """Execute pickup step"""\n        target_object = step[\'target_object\']\n        self.get_logger().info(f\'Attempting to pick up {target_object}\')\n\n        # In practice, this would involve manipulation\n        # For simulation, we\'ll just return success\n        return True\n\n    def execute_placement_step(self, step: Dict[str, Any]) -> bool:\n        """Execute placement step"""\n        target_object = step[\'target_object\']\n        self.get_logger().info(f\'Attempting to place down {target_object}\')\n\n        # In practice, this would involve manipulation\n        # For simulation, we\'ll just return success\n        return True\n\n    def execute_detection_step(self, step: Dict[str, Any]) -> bool:\n        """Execute object detection step"""\n        target_object = step[\'target_object\']\n        self.get_logger().info(f\'Attempting to detect {target_object}\')\n\n        # In practice, this would use perception systems\n        # For simulation, assume object is detected\n        return True\n\n    def execute_communication_step(self, step: Dict[str, Any]) -> bool:\n        """Execute communication step"""\n        description = step[\'description\']\n        self.get_logger().info(f\'Communicating: {description}\')\n\n        # In practice, this might use text-to-speech\n        return True\n\n    def execute_wait_step(self, step: Dict[str, Any]) -> bool:\n        """Execute wait step"""\n        # In practice, this would wait for a condition\n        return True\n\n    def execute_charging_step(self, step: Dict[str, Any]) -> bool:\n        """Execute charging step"""\n        return self.navigate_to_pose(self.get_charging_station_pose())\n\n    def navigate_to_pose(self, pose: PoseStamped) -> bool:\n        """Navigate to the specified pose using Navigation2"""\n        if not self.nav_client.wait_for_server(timeout_sec=5.0):\n            self.get_logger().error(\'Navigation action server not available\')\n            return False\n\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = pose\n\n        future = self.nav_client.send_goal_async(goal_msg)\n        rclpy.spin_until_future_complete(self, future)\n\n        result = future.result()\n        if result is not None:\n            return True\n        else:\n            self.get_logger().error(\'Navigation failed\')\n            return False\n\n    def create_pose_stamped(self, x: float, y: float, theta: float) -> PoseStamped:\n        """Create a PoseStamped message from coordinates"""\n        pose = PoseStamped()\n        pose.header.stamp = self.get_clock().now().to_msg()\n        pose.header.frame_id = \'map\'\n        pose.pose.position.x = x\n        pose.pose.position.y = y\n        pose.pose.position.z = 0.0\n\n        # Convert theta to quaternion\n        import math\n        sin_half_theta = math.sin(theta / 2.0)\n        cos_half_theta = math.cos(theta / 2.0)\n        pose.pose.orientation.x = 0.0\n        pose.pose.orientation.y = 0.0\n        pose.pose.orientation.z = sin_half_theta\n        pose.pose.orientation.w = cos_half_theta\n\n        return pose\n\n    def get_user_location(self) -> PoseStamped:\n        """Get user location (in practice, this would come from localization)"""\n        # For simulation, return a default user location\n        return self.create_pose_stamped(0.0, 0.0, 0.0)\n\n    def get_charging_station_pose(self) -> PoseStamped:\n        """Get charging station pose"""\n        charging_data = self.environment_map[\'charging_station\']\n        return self.create_pose_stamped(\n            charging_data[\'x\'],\n            charging_data[\'y\'],\n            charging_data[\'theta\']\n        )\n'})}),"\n",(0,i.jsx)(e.h2,{id:"hierarchical-task-planning",children:"Hierarchical Task Planning"}),"\n",(0,i.jsx)(e.h3,{id:"high-level-task-decomposition",children:"High-Level Task Decomposition"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class HierarchicalPlanner:\n    def __init__(self):\n        self.task_library = self.initialize_task_library()\n\n    def initialize_task_library(self):\n        """Initialize a library of known tasks and their decompositions"""\n        return {\n            "fetch_object": {\n                "description": "Fetch an object from one location to another",\n                "subtasks": [\n                    {"action": "navigate_to", "params": ["source_location"]},\n                    {"action": "detect_object", "params": ["object_name"]},\n                    {"action": "pick_up", "params": ["object_name"]},\n                    {"action": "navigate_to", "params": ["destination_location"]},\n                    {"action": "place_down", "params": ["object_name"]}\n                ]\n            },\n            "clean_surface": {\n                "description": "Clean a surface by removing objects",\n                "subtasks": [\n                    {"action": "navigate_to", "params": ["surface_location"]},\n                    {"action": "detect_objects", "params": ["surface_location"]},\n                    {"action": "pick_up", "params": ["object_name"]},\n                    {"action": "navigate_to", "params": ["storage_location"]},\n                    {"action": "place_down", "params": ["object_name"]}\n                ]\n            },\n            "set_table": {\n                "description": "Set a table with specific items",\n                "subtasks": [\n                    {"action": "navigate_to", "params": ["storage_location"]},\n                    {"action": "pick_up", "params": ["item_name"]},\n                    {"action": "navigate_to", "params": ["table_location"]},\n                    {"action": "place_down", "params": ["item_name"]}\n                ]\n            }\n        }\n\n    def decompose_task(self, task_name: str, params: Dict[str, Any]) -> List[Dict[str, Any]]:\n        """Decompose a high-level task into primitive actions"""\n        if task_name not in self.task_library:\n            raise ValueError(f"Unknown task: {task_name}")\n\n        template = self.task_library[task_name]\n        subtasks = []\n\n        for i, subtask_template in enumerate(template["subtasks"]):\n            subtask = {\n                "step_number": i + 1,\n                "action": subtask_template["action"],\n                "description": f"Execute {subtask_template[\'action\']} for {task_name}",\n                "dependencies": [i] if i > 0 else []\n            }\n\n            # Bind parameters\n            for param_name in subtask_template["params"]:\n                if param_name in params:\n                    subtask[param_name] = params[param_name]\n                else:\n                    # Use default or raise error\n                    subtask[param_name] = f"default_{param_name}"\n\n            subtasks.append(subtask)\n\n        return subtasks\n'})}),"\n",(0,i.jsx)(e.h2,{id:"symbolic-reasoning-and-knowledge-representation",children:"Symbolic Reasoning and Knowledge Representation"}),"\n",(0,i.jsx)(e.h3,{id:"knowledge-graph-integration",children:"Knowledge Graph Integration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from typing import Set, Tuple\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.entities = set()\n        self.relations = set()\n        self.facts = set()\n\n    def add_entity(self, entity: str, entity_type: str):\n        """Add an entity to the knowledge graph"""\n        self.entities.add((entity, entity_type))\n\n    def add_relation(self, subject: str, predicate: str, obj: str):\n        """Add a relation between entities"""\n        fact = (subject, predicate, obj)\n        self.facts.add(fact)\n\n    def query(self, subject: str, predicate: str = None, obj: str = None) -> Set[Tuple]:\n        """Query the knowledge graph"""\n        results = set()\n\n        for fact in self.facts:\n            s, p, o = fact\n            if (subject is None or s == subject) and \\\n               (predicate is None or p == predicate) and \\\n               (obj is None or o == obj):\n                results.add(fact)\n\n        return results\n\n    def get_related_entities(self, entity: str) -> Set[str]:\n        """Get entities related to the given entity"""\n        related = set()\n\n        for fact in self.facts:\n            s, p, o = fact\n            if s == entity:\n                related.add(o)\n            elif o == entity:\n                related.add(s)\n\n        return related\n\nclass SemanticPlanner(Node):\n    def __init__(self):\n        super().__init__(\'semantic_planner\')\n        self.knowledge_graph = KnowledgeGraph()\n        self.build_initial_knowledge()\n\n    def build_initial_knowledge(self):\n        """Build initial knowledge base"""\n        # Add locations\n        locations = ["kitchen", "living_room", "bedroom", "office", "hallway"]\n        for loc in locations:\n            self.knowledge_graph.add_entity(loc, "location")\n\n        # Add objects\n        objects = ["cup", "book", "phone", "keys", "water_bottle", "plate", "fork"]\n        for obj in objects:\n            self.knowledge_graph.add_entity(obj, "object")\n\n        # Add relations\n        self.knowledge_graph.add_relation("kitchen", "contains", "cup")\n        self.knowledge_graph.add_relation("kitchen", "contains", "plate")\n        self.knowledge_graph.add_relation("kitchen", "contains", "fork")\n        self.knowledge_graph.add_relation("living_room", "contains", "book")\n        self.knowledge_graph.add_relation("office", "contains", "phone")\n        self.knowledge_graph.add_relation("bedroom", "contains", "keys")\n\n        # Add object affordances\n        graspable_objects = ["cup", "book", "phone", "keys", "water_bottle", "plate", "fork"]\n        for obj in graspable_objects:\n            self.knowledge_graph.add_relation(obj, "affordance", "graspable")\n\n    def semantic_query_planning(self, command: str) -> List[Dict[str, Any]]:\n        """Use semantic knowledge to inform planning"""\n        # Parse command to extract key entities\n        entities = self.extract_entities(command)\n\n        # Query knowledge graph for relevant information\n        plan_steps = []\n        for entity in entities:\n            related_facts = self.knowledge_graph.query(subject=entity)\n\n            for fact in related_facts:\n                _, predicate, obj = fact\n                if predicate == "contains":\n                    # Plan to navigate to the location containing the object\n                    plan_steps.extend([\n                        {\n                            "action": "navigate_to",\n                            "target_location": obj,\n                            "description": f"Navigate to {obj} where {entity} is located"\n                        },\n                        {\n                            "action": "detect_object",\n                            "target_object": entity,\n                            "description": f"Detect {entity} in {obj}"\n                        }\n                    ])\n\n        return plan_steps\n\n    def extract_entities(self, command: str) -> List[str]:\n        """Extract relevant entities from command"""\n        # Simple keyword matching (in practice, use NER)\n        entities = []\n        words = command.lower().split()\n\n        for word in words:\n            # Remove punctuation\n            clean_word = word.strip(\'.,!?\')\n            if clean_word in [entity for entity, _ in self.knowledge_graph.entities]:\n                entities.append(clean_word)\n\n        return entities\n'})}),"\n",(0,i.jsx)(e.h2,{id:"planning-with-uncertainty",children:"Planning with Uncertainty"}),"\n",(0,i.jsx)(e.h3,{id:"probabilistic-planning",children:"Probabilistic Planning"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom typing import Optional\n\nclass ProbabilisticPlanner:\n    def __init__(self):\n        self.action_success_probabilities = {\n            "navigate_to": 0.95,\n            "detect_object": 0.85,\n            "pick_up": 0.90,\n            "place_down": 0.95,\n            "communicate": 0.99\n        }\n\n        self.object_location_probabilities = self.initialize_location_probabilities()\n\n    def initialize_location_probabilities(self):\n        """Initialize probabilities of objects being in locations"""\n        return {\n            "cup": {"kitchen": 0.8, "office": 0.1, "bedroom": 0.1},\n            "book": {"living_room": 0.6, "office": 0.3, "bedroom": 0.1},\n            "phone": {"office": 0.4, "bedroom": 0.4, "living_room": 0.2},\n            "keys": {"bedroom": 0.5, "office": 0.3, "kitchen": 0.2},\n            "water_bottle": {"kitchen": 0.7, "office": 0.2, "living_room": 0.1}\n        }\n\n    def probabilistic_plan_generation(self, command: str) -> List[Dict[str, Any]]:\n        """Generate plan considering action and state uncertainties"""\n        # Extract target object from command\n        target_object = self.extract_target_object(command)\n\n        if not target_object:\n            return []\n\n        # Find most probable location for the object\n        location_probs = self.object_location_probabilities.get(target_object, {})\n        if not location_probs:\n            return []\n\n        # Sort locations by probability (descending)\n        sorted_locations = sorted(location_probs.items(), key=lambda x: x[1], reverse=True)\n\n        plan = []\n\n        # For each location (starting with most probable)\n        for location, prob in sorted_locations:\n            # Add navigation and detection steps with success probability\n            nav_step = {\n                "action": "navigate_to",\n                "target_location": location,\n                "success_probability": self.action_success_probabilities["navigate_to"],\n                "description": f"Navigate to {location} (prob: {prob:.2f})"\n            }\n\n            detect_step = {\n                "action": "detect_object",\n                "target_object": target_object,\n                "success_probability": self.action_success_probabilities["detect_object"] * prob,\n                "description": f"Detect {target_object} in {location} (combined prob: {self.action_success_probabilities[\'detect_object\'] * prob:.2f})"\n            }\n\n            plan.extend([nav_step, detect_step])\n\n        return plan\n\n    def extract_target_object(self, command: str) -> Optional[str]:\n        """Extract target object from command"""\n        command_lower = command.lower()\n\n        # Simple keyword matching (in practice, use better NLU)\n        for obj in self.object_location_probabilities.keys():\n            if obj in command_lower:\n                return obj\n\n        return None\n\n    def plan_evaluation(self, plan: List[Dict[str, Any]]) -> float:\n        """Evaluate plan based on success probability"""\n        if not plan:\n            return 0.0\n\n        # Calculate overall success probability\n        total_prob = 1.0\n        for step in plan:\n            if "success_probability" in step:\n                total_prob *= step["success_probability"]\n\n        return total_prob\n\n    def plan_optimization(self, command: str) -> List[Dict[str, Any]]:\n        """Optimize plan considering success probabilities"""\n        # Generate multiple plan alternatives\n        basic_plan = self.probabilistic_plan_generation(command)\n\n        # Evaluate and potentially refine the plan\n        success_prob = self.plan_evaluation(basic_plan)\n\n        if success_prob < 0.5:  # Threshold for plan quality\n            # Add backup plans or alternative routes\n            return self.add_backup_strategies(basic_plan, command)\n\n        return basic_plan\n\n    def add_backup_strategies(self, plan: List[Dict[str, Any]], command: str) -> List[Dict[str, Any]]:\n        """Add backup strategies to improve plan robustness"""\n        # Add alternative locations to check if primary locations fail\n        # Add verification steps after critical actions\n        # Add recovery behaviors\n\n        enhanced_plan = []\n\n        for step in plan:\n            enhanced_plan.append(step)\n\n            # Add verification after critical actions\n            if step["action"] in ["detect_object", "pick_up", "place_down"]:\n                verification_step = {\n                    "action": "verify_success",\n                    "depends_on": step["action"],\n                    "description": f"Verify success of {step[\'action\']}"\n                }\n                enhanced_plan.append(verification_step)\n\n        return enhanced_plan\n'})}),"\n",(0,i.jsx)(e.h2,{id:"learning-from-execution",children:"Learning from Execution"}),"\n",(0,i.jsx)(e.h3,{id:"plan-refinement-and-adaptation",children:"Plan Refinement and Adaptation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class AdaptivePlanner(Node):\n    def __init__(self):\n        super().__init__(\'adaptive_planner\')\n\n        # Plan execution history\n        self.execution_history = []\n\n        # Learned heuristics\n        self.learned_patterns = {}\n\n        # Performance metrics\n        self.performance_stats = {\n            "success_rate": {},\n            "execution_time": {},\n            "failure_modes": {}\n        }\n\n    def record_execution(self, plan: List[Dict[str, Any]],\n                         success: bool,\n                         execution_time: float,\n                         failure_reason: str = None):\n        """Record plan execution for learning"""\n        execution_record = {\n            "plan": plan,\n            "success": success,\n            "time": execution_time,\n            "failure_reason": failure_reason,\n            "timestamp": self.get_clock().now().to_msg()\n        }\n\n        self.execution_history.append(execution_record)\n\n        # Update statistics\n        self.update_performance_stats(plan, success, execution_time, failure_reason)\n\n    def update_performance_stats(self, plan: List[Dict[str, Any]],\n                               success: bool,\n                               execution_time: float,\n                               failure_reason: str = None):\n        """Update performance statistics"""\n        for step in plan:\n            action = step["action"]\n\n            if action not in self.performance_stats["success_rate"]:\n                self.performance_stats["success_rate"][action] = []\n                self.performance_stats["execution_time"][action] = []\n\n            self.performance_stats["success_rate"][action].append(success)\n            self.performance_stats["execution_time"][action].append(execution_time)\n\n        if failure_reason:\n            if failure_reason not in self.performance_stats["failure_modes"]:\n                self.performance_stats["failure_modes"][failure_reason] = 0\n            self.performance_stats["failure_modes"][failure_reason] += 1\n\n    def adapt_plan(self, command: str, current_plan: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        """Adapt plan based on execution history and learned patterns"""\n        # Analyze past executions for similar commands\n        similar_executions = self.find_similar_executions(command, current_plan)\n\n        if similar_executions:\n            # Apply learned adaptations\n            adapted_plan = self.apply_learned_adaptations(current_plan, similar_executions)\n            return adapted_plan\n\n        return current_plan\n\n    def find_similar_executions(self, command: str, plan: List[Dict[str, Any]]):\n        """Find similar past executions"""\n        # Simple similarity based on actions and objects\n        similar = []\n\n        for record in self.execution_history:\n            # Compare plans based on action sequence and objects\n            if self.plans_are_similar(plan, record["plan"]):\n                similar.append(record)\n\n        return similar\n\n    def plans_are_similar(self, plan1: List[Dict[str, Any]], plan2: List[Dict[str, Any]]) -> bool:\n        """Check if two plans are similar"""\n        if len(plan1) != len(plan2):\n            return False\n\n        for step1, step2 in zip(plan1, plan2):\n            if step1.get("action") != step2.get("action"):\n                return False\n            if step1.get("target_object") != step2.get("target_object"):\n                return False\n\n        return True\n\n    def apply_learned_adaptations(self, original_plan: List[Dict[str, Any]],\n                                 similar_executions: List[dict]) -> List[Dict[str, Any]]:\n        """Apply learned adaptations to the plan"""\n        adapted_plan = []\n\n        for i, step in enumerate(original_plan):\n            # Get adaptations for this step type\n            step_executions = [ex for ex in similar_executions\n                              if i < len(ex["plan"]) and ex["plan"][i]["action"] == step["action"]]\n\n            if step_executions:\n                # Apply successful patterns\n                successful_executions = [ex for ex in step_executions if ex["success"]]\n\n                if len(successful_executions) / len(step_executions) < 0.7:  # Low success rate\n                    # Add verification or alternative approach\n                    verification_step = {\n                        "action": "verify_environment",\n                        "description": f"Verify environment before {step[\'action\']} due to low success rate",\n                        "critical": True\n                    }\n                    adapted_plan.append(verification_step)\n\n            adapted_plan.append(step)\n\n        return adapted_plan\n'})}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Design cognitive planning systems using Large Language Models"}),"\n",(0,i.jsx)(e.li,{children:"Implement hierarchical task decomposition"}),"\n",(0,i.jsx)(e.li,{children:"Create knowledge representations for robotic planning"}),"\n",(0,i.jsx)(e.li,{children:"Incorporate uncertainty into planning processes"}),"\n",(0,i.jsx)(e.li,{children:"Build adaptive planning systems that learn from execution"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,i.jsx)(e.p,{children:"Create a cognitive planning system that accepts natural language commands and generates executable plans for a simulated robot. Test the system with various commands and evaluate its performance in different scenarios."}),"\n",(0,i.jsx)(e.admonition,{type:"tip",children:(0,i.jsx)(e.p,{children:"Combine LLM-based high-level planning with traditional path planning algorithms for robust navigation tasks."})}),"\n",(0,i.jsx)(e.admonition,{type:"note",children:(0,i.jsx)(e.p,{children:"Consider the computational cost of LLM calls and implement caching mechanisms for frequently requested plans."})})]})}function d(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(p,{...n})}):p(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var a=t(6540);const i={},o=a.createContext(i);function s(n){const e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),a.createElement(o.Provider,{value:e},n.children)}}}]);