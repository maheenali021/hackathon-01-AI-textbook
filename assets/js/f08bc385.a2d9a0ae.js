"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[392],{5292:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>_,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module2/sensor-simulation","title":"Chapter 4: Sensor Simulation in Digital Twins","description":"Introduction to Sensor Simulation","source":"@site/docs/module2/sensor-simulation.md","sourceDirName":"module2","slug":"/module2/sensor-simulation","permalink":"/hackathon-01-AI-textbook/docs/module2/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/maheenali021/hackathon-01-AI-textbook/tree/main/docs/module2/sensor-simulation.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Chapter 4: Sensor Simulation in Digital Twins"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Unity Visualization for Robotics","permalink":"/hackathon-01-AI-textbook/docs/module2/unity-visualization"},"next":{"title":"Module 2 Milestone Project: Custom Robot Simulation","permalink":"/hackathon-01-AI-textbook/docs/module2/module2-milestone"}}');var s=i(4848),o=i(8453);const r={sidebar_position:4,title:"Chapter 4: Sensor Simulation in Digital Twins"},t="Sensor Simulation in Digital Twins",l={},c=[{value:"Introduction to Sensor Simulation",id:"introduction-to-sensor-simulation",level:2},{value:"Types of Sensors in Robotics",id:"types-of-sensors-in-robotics",level:2},{value:"Camera Sensors",id:"camera-sensors",level:3},{value:"LIDAR Sensors",id:"lidar-sensors",level:3},{value:"IMU Sensors",id:"imu-sensors",level:3},{value:"Sensor Fusion Simulation",id:"sensor-fusion-simulation",level:2},{value:"Combining Multiple Sensors",id:"combining-multiple-sensors",level:3},{value:"Physics-Based Sensor Simulation",id:"physics-based-sensor-simulation",level:2},{value:"Realistic Sensor Noise Modeling",id:"realistic-sensor-noise-modeling",level:3},{value:"Environmental Effects on Sensors",id:"environmental-effects-on-sensors",level:2},{value:"Weather Simulation",id:"weather-simulation",level:3},{value:"Sensor Calibration Simulation",id:"sensor-calibration-simulation",level:2},{value:"Calibration Process Simulation",id:"calibration-process-simulation",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2}];function m(n){const e={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"sensor-simulation-in-digital-twins",children:"Sensor Simulation in Digital Twins"})}),"\n",(0,s.jsx)(e.h2,{id:"introduction-to-sensor-simulation",children:"Introduction to Sensor Simulation"}),"\n",(0,s.jsx)(e.p,{children:"Sensor simulation is a crucial aspect of digital twin technology in robotics. It allows for testing perception algorithms, sensor fusion, and robot behaviors in a safe, controlled virtual environment before deployment on real hardware."}),"\n",(0,s.jsx)(e.h2,{id:"types-of-sensors-in-robotics",children:"Types of Sensors in Robotics"}),"\n",(0,s.jsx)(e.h3,{id:"camera-sensors",children:"Camera Sensors"}),"\n",(0,s.jsx)(e.p,{children:"Camera sensors provide visual information to robots. In simulation, they can be configured with various properties:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\n\nclass CameraSimulator(Node):\n    def __init__(self):\n        super().__init__('camera_simulator')\n\n        # Publisher for camera images\n        self.image_pub = self.create_publisher(Image, '/camera/image_raw', 10)\n        self.info_pub = self.create_publisher(CameraInfo, '/camera/camera_info', 10)\n\n        # CV Bridge for image conversion\n        self.bridge = CvBridge()\n\n        # Camera parameters\n        self.width = 640\n        self.height = 480\n        self.fps = 30\n\n        # Timer for image generation\n        self.timer = self.create_timer(1.0/self.fps, self.generate_image)\n\n        self.get_logger().info('Camera simulator initialized')\n\n    def generate_image(self):\n        \"\"\"Generate simulated camera image\"\"\"\n        # Create synthetic image (in real implementation, this would come from simulation engine)\n        image = np.zeros((self.height, self.width, 3), dtype=np.uint8)\n\n        # Add some synthetic objects to the image\n        cv2.circle(image, (self.width//2, self.height//2), 50, (255, 0, 0), -1)  # Blue circle\n        cv2.rectangle(image, (100, 100), (200, 200), (0, 255, 0), 2)  # Green rectangle\n\n        # Add noise to make it more realistic\n        noise = np.random.normal(0, 10, image.shape).astype(np.uint8)\n        image = cv2.add(image, noise)\n\n        # Convert to ROS Image message\n        ros_image = self.bridge.cv2_to_imgmsg(image, encoding='bgr8')\n        ros_image.header.stamp = self.get_clock().now().to_msg()\n        ros_image.header.frame_id = 'camera_link'\n\n        # Publish image\n        self.image_pub.publish(ros_image)\n\n        # Publish camera info\n        self.publish_camera_info()\n\n    def publish_camera_info(self):\n        \"\"\"Publish camera calibration information\"\"\"\n        info_msg = CameraInfo()\n        info_msg.header.stamp = self.get_clock().now().to_msg()\n        info_msg.header.frame_id = 'camera_link'\n        info_msg.width = self.width\n        info_msg.height = self.height\n\n        # Camera intrinsic parameters (example values)\n        info_msg.k = [500.0, 0.0, self.width/2,    # fx, 0, cx\n                      0.0, 500.0, self.height/2,  # 0, fy, cy\n                      0.0, 0.0, 1.0]              # 0, 0, 1\n\n        info_msg.d = [0.0, 0.0, 0.0, 0.0, 0.0]  # Distortion coefficients\n\n        self.info_pub.publish(info_msg)\n"})}),"\n",(0,s.jsx)(e.h3,{id:"lidar-sensors",children:"LIDAR Sensors"}),"\n",(0,s.jsx)(e.p,{children:"LIDAR sensors provide distance measurements in a 2D or 3D space:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from sensor_msgs.msg import LaserScan\nimport math\n\nclass LIDARSimulator(Node):\n    def __init__(self):\n        super().__init__(\'lidar_simulator\')\n\n        # Publisher for LIDAR scan\n        self.scan_pub = self.create_publisher(LaserScan, \'/scan\', 10)\n\n        # LIDAR parameters\n        self.angle_min = -math.pi\n        self.angle_max = math.pi\n        self.angle_increment = math.pi / 180  # 1 degree resolution\n        self.time_increment = 0.0\n        self.scan_time = 0.1\n        self.range_min = 0.1\n        self.range_max = 10.0\n\n        # Timer for scan generation\n        self.timer = self.create_timer(0.1, self.generate_scan)\n\n    def generate_scan(self):\n        """Generate simulated LIDAR scan"""\n        num_scans = int((self.angle_max - self.angle_min) / self.angle_increment)\n\n        # Generate ranges with simulated environment\n        ranges = []\n        for i in range(num_scans):\n            angle = self.angle_min + i * self.angle_increment\n\n            # Simulate environment with walls and obstacles\n            distance = self.simulate_distance(angle)\n\n            # Add noise to make it realistic\n            noise = np.random.normal(0, 0.02)  # 2cm noise\n            distance_with_noise = max(self.range_min, min(self.range_max, distance + noise))\n\n            ranges.append(distance_with_noise)\n\n        # Create LaserScan message\n        scan_msg = LaserScan()\n        scan_msg.header.stamp = self.get_clock().now().to_msg()\n        scan_msg.header.frame_id = \'laser_frame\'\n        scan_msg.angle_min = self.angle_min\n        scan_msg.angle_max = self.angle_max\n        scan_msg.angle_increment = self.angle_increment\n        scan_msg.time_increment = self.time_increment\n        scan_msg.scan_time = self.scan_time\n        scan_msg.range_min = self.range_min\n        scan_msg.range_max = self.range_max\n        scan_msg.ranges = ranges\n        scan_msg.intensities = []  # Intensities are optional\n\n        # Publish scan\n        self.scan_pub.publish(scan_msg)\n\n    def simulate_distance(self, angle):\n        """Simulate distance measurement based on angle"""\n        # Example: simple square room with side length 8m\n        # Robot is in the center\n        half_room_size = 4.0\n\n        # Calculate distance to walls based on angle\n        abs_angle = abs(angle) % (math.pi / 2)\n\n        if abs_angle < math.pi / 4:\n            # Horizontal wall\n            distance = half_room_size / math.cos(abs_angle)\n        else:\n            # Vertical wall\n            distance = half_room_size / math.sin(abs_angle)\n\n        # Add some obstacles\n        if 0.5 < distance < 2.0:\n            # Add a column in front of the robot\n            distance = 1.5\n\n        return min(distance, self.range_max)\n'})}),"\n",(0,s.jsx)(e.h3,{id:"imu-sensors",children:"IMU Sensors"}),"\n",(0,s.jsx)(e.p,{children:"IMU sensors provide information about acceleration, angular velocity, and orientation:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Vector3, Quaternion\n\nclass IMUSimulator(Node):\n    def __init__(self):\n        super().__init__('imu_simulator')\n\n        # Publisher for IMU data\n        self.imu_pub = self.create_publisher(Imu, '/imu/data', 10)\n\n        # IMU parameters\n        self.linear_acceleration_variance = 0.01\n        self.angular_velocity_variance = 0.01\n        self.orientation_variance = 0.001\n\n        # Robot state (for simulating motion)\n        self.position = [0.0, 0.0, 0.0]\n        self.velocity = [0.0, 0.0, 0.0]\n        self.acceleration = [0.0, 0.0, 0.0]\n\n        # Timer for IMU data generation\n        self.timer = self.create_timer(0.01, self.generate_imu_data)  # 100Hz\n\n    def generate_imu_data(self):\n        \"\"\"Generate simulated IMU data\"\"\"\n        imu_msg = Imu()\n        imu_msg.header.stamp = self.get_clock().now().to_msg()\n        imu_msg.header.frame_id = 'imu_link'\n\n        # Simulate gravity in z-axis\n        imu_msg.linear_acceleration.x = self.acceleration[0] + np.random.normal(0, self.linear_acceleration_variance)\n        imu_msg.linear_acceleration.y = self.acceleration[1] + np.random.normal(0, self.linear_acceleration_variance)\n        imu_msg.linear_acceleration.z = self.acceleration[2] + 9.81 + np.random.normal(0, self.linear_acceleration_variance)\n\n        # Simulate angular velocity (could be from robot motion)\n        imu_msg.angular_velocity.x = np.random.normal(0, self.angular_velocity_variance)\n        imu_msg.angular_velocity.y = np.random.normal(0, self.angular_velocity_variance)\n        imu_msg.angular_velocity.z = np.random.normal(0, self.angular_velocity_variance)\n\n        # Simulate orientation (assuming robot is upright)\n        imu_msg.orientation.x = np.random.normal(0, self.orientation_variance)\n        imu_msg.orientation.y = np.random.normal(0, self.orientation_variance)\n        imu_msg.orientation.z = np.random.normal(0, self.orientation_variance)\n        imu_msg.orientation.w = 1.0  # Normalize quaternion\n\n        # Set covariance matrices\n        # Linear acceleration covariance\n        imu_msg.linear_acceleration_covariance = [\n            self.linear_acceleration_variance, 0, 0,\n            0, self.linear_acceleration_variance, 0,\n            0, 0, self.linear_acceleration_variance\n        ]\n\n        # Angular velocity covariance\n        imu_msg.angular_velocity_covariance = [\n            self.angular_velocity_variance, 0, 0,\n            0, self.angular_velocity_variance, 0,\n            0, 0, self.angular_velocity_variance\n        ]\n\n        # Orientation covariance\n        imu_msg.orientation_covariance = [\n            self.orientation_variance, 0, 0,\n            0, self.orientation_variance, 0,\n            0, 0, self.orientation_variance\n        ]\n\n        # Publish IMU data\n        self.imu_pub.publish(imu_msg)\n"})}),"\n",(0,s.jsx)(e.h2,{id:"sensor-fusion-simulation",children:"Sensor Fusion Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"combining-multiple-sensors",children:"Combining Multiple Sensors"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from sensor_msgs.msg import PointCloud2\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\nimport tf2_ros\nfrom tf2_ros import TransformBroadcaster\n\nclass SensorFusionSimulator(Node):\n    def __init__(self):\n        super().__init__(\'sensor_fusion_simulator\')\n\n        # Subscribers for different sensors\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10)\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu/data\', self.imu_callback, 10)\n\n        # Publishers for fused data\n        self.fused_pub = self.create_publisher(\n            PointCloud2, \'/fused_pointcloud\', 10)\n        self.pose_pub = self.create_publisher(\n            PoseWithCovarianceStamped, \'/estimated_pose\', 10)\n\n        # TF broadcaster for transforms\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Internal state\n        self.last_image = None\n        self.last_scan = None\n        self.last_imu = None\n\n        # Timer for fusion\n        self.fusion_timer = self.create_timer(0.1, self.perform_fusion)\n\n    def image_callback(self, msg):\n        """Handle image data"""\n        self.last_image = msg\n\n    def scan_callback(self, msg):\n        """Handle LIDAR scan data"""\n        self.last_scan = msg\n\n    def imu_callback(self, msg):\n        """Handle IMU data"""\n        self.last_imu = msg\n\n    def perform_fusion(self):\n        """Perform sensor fusion to estimate robot state"""\n        if not all([self.last_image, self.last_scan, self.last_imu]):\n            return  # Not all sensors have published yet\n\n        # Example fusion: combine IMU orientation with position from other sources\n        estimated_pose = self.estimate_pose_from_sensors()\n\n        # Publish fused estimate\n        pose_msg = PoseWithCovarianceStamped()\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.header.frame_id = \'map\'\n        pose_msg.pose = estimated_pose\n\n        self.pose_pub.publish(pose_msg)\n\n        # Create fused point cloud from camera and LIDAR\n        fused_cloud = self.create_fused_pointcloud()\n        if fused_cloud:\n            self.fused_pub.publish(fused_cloud)\n\n    def estimate_pose_from_sensors(self):\n        """Estimate pose by combining sensor data"""\n        # This is a simplified example\n        # In practice, you would use Kalman filters or particle filters\n        from geometry_msgs.msg import PoseWithCovariance\n\n        pose = PoseWithCovariance()\n\n        # Use IMU for orientation\n        pose.pose.orientation = self.last_imu.orientation\n\n        # For position, you might combine odometry with sensor data\n        # For simulation, we\'ll just use a fixed position with IMU orientation\n        pose.pose.position.x = 0.0\n        pose.pose.position.y = 0.0\n        pose.pose.position.z = 0.0\n\n        # Set covariance based on sensor uncertainties\n        pose.covariance = [0.1, 0, 0, 0, 0, 0,  # Position covariance\n                          0, 0.1, 0, 0, 0, 0,\n                          0, 0, 0.1, 0, 0, 0,\n                          0, 0, 0, 0.1, 0, 0,  # Orientation covariance\n                          0, 0, 0, 0, 0.1, 0,\n                          0, 0, 0, 0, 0, 0.1]\n\n        return pose\n\n    def create_fused_pointcloud(self):\n        """Create a fused point cloud from camera and LIDAR data"""\n        # In a real implementation, this would project camera pixels\n        # into 3D space using LIDAR depth information\n        pass\n'})}),"\n",(0,s.jsx)(e.h2,{id:"physics-based-sensor-simulation",children:"Physics-Based Sensor Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"realistic-sensor-noise-modeling",children:"Realistic Sensor Noise Modeling"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass PhysicsBasedSensorSimulator:\n    def __init__(self):\n        # Sensor characteristics\n        self.camera_noise_params = {\n            'gaussian_noise': 0.02,\n            'poisson_noise': 0.01,\n            'uniform_distortion': 0.05\n        }\n\n        self.lidar_noise_params = {\n            'range_bias': 0.02,  # 2cm bias\n            'range_noise': 0.01,  # 1cm noise\n            'angular_bias': 0.001,  # 0.057 degrees\n            'angular_noise': 0.0005  # 0.029 degrees\n        }\n\n        self.imu_noise_params = {\n            'accel_bias': 0.01,  # 1% of gravity\n            'accel_noise': 0.001,\n            'gyro_bias': 0.001,  # rad/s\n            'gyro_noise': 0.0001,\n            'mag_bias': 0.1,    # micro Tesla\n            'mag_noise': 0.01\n        }\n\n    def add_camera_noise(self, image):\n        \"\"\"Add realistic noise to camera images\"\"\"\n        # Add Gaussian noise\n        gaussian_noise = np.random.normal(\n            0, self.camera_noise_params['gaussian_noise'], image.shape)\n        noisy_image = image.astype(np.float32) + gaussian_noise * 255\n\n        # Add Poisson noise (signal-dependent)\n        poisson_noise = np.random.poisson(noisy_image / 255.0) * 255 - noisy_image\n        noisy_image += poisson_noise * self.camera_noise_params['poisson_noise']\n\n        # Ensure values are in valid range\n        noisy_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n\n        return noisy_image\n\n    def add_lidar_noise(self, ranges, angles):\n        \"\"\"Add realistic noise to LIDAR measurements\"\"\"\n        noisy_ranges = []\n\n        for i, (range_val, angle) in enumerate(zip(ranges, angles)):\n            if range_val >= 10.0:  # Maximum range\n                noisy_ranges.append(range_val)\n                continue\n\n            # Add bias and noise\n            biased_range = range_val + self.lidar_noise_params['range_bias']\n            noisy_range = biased_range + np.random.normal(0, self.lidar_noise_params['range_noise'])\n\n            # Range-dependent noise (closer objects have less noise)\n            range_dependent_noise = self.lidar_noise_params['range_noise'] * (1.0 - range_val / 10.0)\n            final_range = noisy_range + np.random.normal(0, range_dependent_noise)\n\n            noisy_ranges.append(max(0.1, final_range))  # Minimum range constraint\n\n        return noisy_ranges\n\n    def add_imu_noise(self, accel_true, gyro_true, dt):\n        \"\"\"Add realistic noise to IMU measurements\"\"\"\n        # Accelerometer noise\n        accel_bias = np.random.normal(0, self.imu_noise_params['accel_bias'], 3)\n        accel_noise = np.random.normal(0, self.imu_noise_params['accel_noise'], 3)\n        accel_measurement = accel_true + accel_bias + accel_noise\n\n        # Gyroscope noise\n        gyro_bias = np.random.normal(0, self.imu_noise_params['gyro_bias'], 3)\n        gyro_noise = np.random.normal(0, self.imu_noise_params['gyro_noise'], 3)\n        gyro_measurement = gyro_true + gyro_bias + gyro_noise\n\n        return accel_measurement, gyro_measurement\n"})}),"\n",(0,s.jsx)(e.h2,{id:"environmental-effects-on-sensors",children:"Environmental Effects on Sensors"}),"\n",(0,s.jsx)(e.h3,{id:"weather-simulation",children:"Weather Simulation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class WeatherEffectSimulator:\n    def __init__(self):\n        self.weather_conditions = {\n            'clear': {'visibility': 100.0, 'precipitation': 0.0},\n            'rain_light': {'visibility': 30.0, 'precipitation': 2.0},\n            'rain_heavy': {'visibility': 10.0, 'precipitation': 8.0},\n            'fog_light': {'visibility': 20.0, 'precipitation': 0.0},\n            'fog_dense': {'visibility': 5.0, 'precipitation': 0.0}\n        }\n\n        self.current_weather = 'clear'\n\n    def set_weather(self, condition):\n        \"\"\"Set the current weather condition\"\"\"\n        if condition in self.weather_conditions:\n            self.current_weather = condition\n            return True\n        return False\n\n    def affect_camera(self, image):\n        \"\"\"Apply weather effects to camera images\"\"\"\n        condition = self.weather_conditions[self.current_weather]\n\n        if 'rain' in self.current_weather:\n            # Rain effect: reduce contrast and add water droplets\n            image = self.apply_rain_effect(image, condition['precipitation'])\n        elif 'fog' in self.current_weather:\n            # Fog effect: reduce visibility and contrast\n            image = self.apply_fog_effect(image, condition['visibility'])\n\n        return image\n\n    def affect_lidar(self, ranges, angles):\n        \"\"\"Apply weather effects to LIDAR measurements\"\"\"\n        condition = self.weather_conditions[self.current_weather]\n\n        affected_ranges = []\n        for r in ranges:\n            if r >= 10.0:  # Maximum range\n                affected_ranges.append(r)\n                continue\n\n            # In bad weather, effective range decreases\n            visibility_factor = min(condition['visibility'] / 10.0, 1.0)\n            effective_range = r * visibility_factor\n\n            # Add weather-related noise\n            if 'rain' in self.current_weather:\n                weather_noise = np.random.normal(0, 0.05)  # More noise in rain\n            elif 'fog' in self.current_weather:\n                weather_noise = np.random.normal(0, 0.03)  # Some noise in fog\n            else:\n                weather_noise = 0\n\n            affected_ranges.append(max(0.1, effective_range + weather_noise))\n\n        return affected_ranges\n\n    def apply_rain_effect(self, image, precipitation_level):\n        \"\"\"Apply rain effect to image\"\"\"\n        # Reduce visibility\n        alpha = 1.0 - (precipitation_level / 10.0) * 0.3\n        image = (image * alpha).astype(np.uint8)\n\n        # Add rain streaks (simplified)\n        rain_intensity = precipitation_level / 10.0\n        rain_pixels = np.random.random(image.shape[:2]) < rain_intensity * 0.05\n        image[rain_pixels] = [200, 200, 255]  # Light blue streaks\n\n        return image\n\n    def apply_fog_effect(self, image, visibility):\n        \"\"\"Apply fog effect to image\"\"\"\n        # Calculate fog factor based on visibility\n        fog_factor = 1.0 - (visibility / 100.0)\n\n        # Apply fog by blending with gray\n        fog_color = np.array([128, 128, 128], dtype=np.uint8)\n        fogged_image = (image * (1 - fog_factor) + fog_color * fog_factor).astype(np.uint8)\n\n        return fogged_image\n"})}),"\n",(0,s.jsx)(e.h2,{id:"sensor-calibration-simulation",children:"Sensor Calibration Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"calibration-process-simulation",children:"Calibration Process Simulation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class SensorCalibrationSimulator:\n    def __init__(self):\n        # True calibration parameters\n        self.true_intrinsics = np.array([\n            [500.0, 0.0, 320.0],  # fx, 0, cx\n            [0.0, 500.0, 240.0],  # 0, fy, cy\n            [0.0, 0.0, 1.0]       # 0, 0, 1\n        ])\n\n        self.true_distortion = np.array([0.1, -0.2, 0.001, 0.002, 0.0])  # k1, k2, p1, p2, k3\n\n        # Initial calibration guess (with errors)\n        self.calibrated_intrinsics = np.array([\n            [495.0, 0.0, 322.0],\n            [0.0, 498.0, 238.0],\n            [0.0, 0.0, 1.0]\n        ])\n\n        self.calibrated_distortion = np.array([0.09, -0.18, 0.002, 0.001, 0.001])\n\n    def simulate_calibration_target(self, num_points=50):\n        """Simulate a calibration target (checkerboard) in 3D"""\n        # Create a checkerboard pattern in 3D\n        rows, cols = 8, 6\n        square_size = 0.025  # 2.5 cm squares\n\n        # Generate 3D points of the checkerboard\n        obj_points = []\n        for j in range(rows):\n            for i in range(cols):\n                obj_points.append([i * square_size, j * square_size, 0])\n\n        obj_points = np.array(obj_points, dtype=np.float32)\n\n        # Generate multiple views by applying random transformations\n        views = []\n        for _ in range(10):  # 10 different views\n            # Random rotation and translation\n            rvec = np.random.uniform(-0.5, 0.5, 3)  # Small rotations\n            tvec = np.random.uniform(-0.1, 0.1, 3)  # Small translations\n\n            # Project 3D points to 2D using true intrinsics\n            projected_points, _ = cv2.projectPoints(\n                obj_points, rvec, tvec,\n                self.true_intrinsics, self.true_distortion\n            )\n\n            views.append({\n                \'obj_points\': obj_points.copy(),\n                \'img_points\': projected_points.reshape(-1, 2),\n                \'rvec\': rvec,\n                \'tvec\': tvec\n            })\n\n        return views\n\n    def calibrate_camera(self, views):\n        """Simulate camera calibration process"""\n        # Collect all object points and image points\n        all_obj_points = []\n        all_img_points = []\n\n        for view in views:\n            all_obj_points.append(view[\'obj_points\'])\n            all_img_points.append(view[\'img_points\'])\n\n        # Perform calibration (would normally use cv2.calibrateCamera)\n        # Here we\'ll just show the process\n        ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\n            all_obj_points, all_img_points,\n            (640, 480), None, None\n        )\n\n        return ret, mtx, dist, rvecs, tvecs\n\n    def evaluate_calibration(self, calibrated_intrinsics, calibrated_distortion):\n        """Evaluate the quality of calibration"""\n        # Calculate reprojection error\n        test_points_3d = np.array([[0, 0, 0], [0.1, 0, 0], [0, 0.1, 0], [0.1, 0.1, 0]], dtype=np.float32)\n\n        # Project using true parameters\n        true_2d, _ = cv2.projectPoints(\n            test_points_3d, np.zeros(3), np.zeros(3),\n            self.true_intrinsics, self.true_distortion\n        )\n\n        # Project using calibrated parameters\n        calibrated_2d, _ = cv2.projectPoints(\n            test_points_3d, np.zeros(3), np.zeros(3),\n            calibrated_intrinsics, calibrated_distortion\n        )\n\n        # Calculate reprojection error\n        error = np.mean(np.linalg.norm(true_2d - calibrated_2d, axis=2))\n\n        return error\n'})}),"\n",(0,s.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(e.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Simulate various types of sensors used in robotics"}),"\n",(0,s.jsx)(e.li,{children:"Implement realistic noise models for sensor data"}),"\n",(0,s.jsx)(e.li,{children:"Perform sensor fusion to combine data from multiple sensors"}),"\n",(0,s.jsx)(e.li,{children:"Account for environmental effects on sensor performance"}),"\n",(0,s.jsx)(e.li,{children:"Understand the calibration process for sensors"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,s.jsx)(e.p,{children:"Create a simulation node that publishes data from multiple sensors (camera, LIDAR, IMU) with realistic noise models. Implement a simple sensor fusion algorithm that combines these sensor readings to estimate the robot's state."}),"\n",(0,s.jsx)(e.admonition,{type:"tip",children:(0,s.jsxs)(e.p,{children:["Use ",(0,s.jsx)(e.code,{children:"rqt_plot"})," or ",(0,s.jsx)(e.code,{children:"plotjuggler"})," to visualize sensor data streams and verify that your noise models produce realistic-looking signals."]})}),"\n",(0,s.jsx)(e.admonition,{type:"note",children:(0,s.jsx)(e.p,{children:"Consider the computational cost of sensor simulation, especially when simulating multiple sensors at high frequencies."})})]})}function _(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(m,{...n})}):m(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>t});var a=i(6540);const s={},o=a.createContext(s);function r(n){const e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:r(n.components),a.createElement(o.Provider,{value:e},n.children)}}}]);