"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[392],{5292:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>_,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module2/sensor-simulation","title":"Chapter 4: Sensor Simulation in Digital Twins","description":"Introduction to Sensor Simulation","source":"@site/docs/module2/sensor-simulation.md","sourceDirName":"module2","slug":"/module2/sensor-simulation","permalink":"/hackathon-01-AI-textbook/docs/module2/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/maheenali021/hackathon-01-AI-textbook/tree/main/docs/module2/sensor-simulation.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Chapter 4: Sensor Simulation in Digital Twins"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Unity Visualization for Robotics","permalink":"/hackathon-01-AI-textbook/docs/module2/unity-visualization"},"next":{"title":"Module 2 Milestone Project: Custom Robot Simulation","permalink":"/hackathon-01-AI-textbook/docs/module2/module2-milestone"}}');var a=r(4848),s=r(8453);const o={sidebar_position:4,title:"Chapter 4: Sensor Simulation in Digital Twins"},t="Sensor Simulation in Digital Twins",l={},c=[{value:"Introduction to Sensor Simulation",id:"introduction-to-sensor-simulation",level:2},{value:"Types of Sensors in Robotics",id:"types-of-sensors-in-robotics",level:2},{value:"Camera Sensors",id:"camera-sensors",level:3},{value:"LIDAR Sensors",id:"lidar-sensors",level:3},{value:"IMU Sensors",id:"imu-sensors",level:3},{value:"Sensor Fusion Simulation",id:"sensor-fusion-simulation",level:2},{value:"Combining Multiple Sensors",id:"combining-multiple-sensors",level:3},{value:"Physics-Based Sensor Simulation",id:"physics-based-sensor-simulation",level:2},{value:"Realistic Sensor Noise Modeling",id:"realistic-sensor-noise-modeling",level:3},{value:"Environmental Effects on Sensors",id:"environmental-effects-on-sensors",level:2},{value:"Weather Simulation",id:"weather-simulation",level:3},{value:"Sensor Calibration Simulation",id:"sensor-calibration-simulation",level:2},{value:"Calibration Process Simulation",id:"calibration-process-simulation",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Hands-on Exercise",id:"hands-on-exercise",level:2}];function m(n){const e={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"sensor-simulation-in-digital-twins",children:"Sensor Simulation in Digital Twins"})}),"\n",(0,a.jsx)(e.h2,{id:"introduction-to-sensor-simulation",children:"Introduction to Sensor Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Sensor simulation is a crucial aspect of digital twin technology in robotics. It allows for testing perception algorithms, sensor fusion, and robot behaviors in a safe, controlled virtual environment before deployment on real hardware."}),"\n",(0,a.jsx)(e.h2,{id:"types-of-sensors-in-robotics",children:"Types of Sensors in Robotics"}),"\n",(0,a.jsx)(e.h3,{id:"camera-sensors",children:"Camera Sensors"}),"\n",(0,a.jsx)(e.p,{children:"Camera sensors provide visual information to robots. In simulation, they can be configured with various properties:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport cv2\r\n\r\nclass CameraSimulator(Node):\r\n    def __init__(self):\r\n        super().__init__('camera_simulator')\r\n\r\n        # Publisher for camera images\r\n        self.image_pub = self.create_publisher(Image, '/camera/image_raw', 10)\r\n        self.info_pub = self.create_publisher(CameraInfo, '/camera/camera_info', 10)\r\n\r\n        # CV Bridge for image conversion\r\n        self.bridge = CvBridge()\r\n\r\n        # Camera parameters\r\n        self.width = 640\r\n        self.height = 480\r\n        self.fps = 30\r\n\r\n        # Timer for image generation\r\n        self.timer = self.create_timer(1.0/self.fps, self.generate_image)\r\n\r\n        self.get_logger().info('Camera simulator initialized')\r\n\r\n    def generate_image(self):\r\n        \"\"\"Generate simulated camera image\"\"\"\r\n        # Create synthetic image (in real implementation, this would come from simulation engine)\r\n        image = np.zeros((self.height, self.width, 3), dtype=np.uint8)\r\n\r\n        # Add some synthetic objects to the image\r\n        cv2.circle(image, (self.width//2, self.height//2), 50, (255, 0, 0), -1)  # Blue circle\r\n        cv2.rectangle(image, (100, 100), (200, 200), (0, 255, 0), 2)  # Green rectangle\r\n\r\n        # Add noise to make it more realistic\r\n        noise = np.random.normal(0, 10, image.shape).astype(np.uint8)\r\n        image = cv2.add(image, noise)\r\n\r\n        # Convert to ROS Image message\r\n        ros_image = self.bridge.cv2_to_imgmsg(image, encoding='bgr8')\r\n        ros_image.header.stamp = self.get_clock().now().to_msg()\r\n        ros_image.header.frame_id = 'camera_link'\r\n\r\n        # Publish image\r\n        self.image_pub.publish(ros_image)\r\n\r\n        # Publish camera info\r\n        self.publish_camera_info()\r\n\r\n    def publish_camera_info(self):\r\n        \"\"\"Publish camera calibration information\"\"\"\r\n        info_msg = CameraInfo()\r\n        info_msg.header.stamp = self.get_clock().now().to_msg()\r\n        info_msg.header.frame_id = 'camera_link'\r\n        info_msg.width = self.width\r\n        info_msg.height = self.height\r\n\r\n        # Camera intrinsic parameters (example values)\r\n        info_msg.k = [500.0, 0.0, self.width/2,    # fx, 0, cx\r\n                      0.0, 500.0, self.height/2,  # 0, fy, cy\r\n                      0.0, 0.0, 1.0]              # 0, 0, 1\r\n\r\n        info_msg.d = [0.0, 0.0, 0.0, 0.0, 0.0]  # Distortion coefficients\r\n\r\n        self.info_pub.publish(info_msg)\n"})}),"\n",(0,a.jsx)(e.h3,{id:"lidar-sensors",children:"LIDAR Sensors"}),"\n",(0,a.jsx)(e.p,{children:"LIDAR sensors provide distance measurements in a 2D or 3D space:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from sensor_msgs.msg import LaserScan\r\nimport math\r\n\r\nclass LIDARSimulator(Node):\r\n    def __init__(self):\r\n        super().__init__(\'lidar_simulator\')\r\n\r\n        # Publisher for LIDAR scan\r\n        self.scan_pub = self.create_publisher(LaserScan, \'/scan\', 10)\r\n\r\n        # LIDAR parameters\r\n        self.angle_min = -math.pi\r\n        self.angle_max = math.pi\r\n        self.angle_increment = math.pi / 180  # 1 degree resolution\r\n        self.time_increment = 0.0\r\n        self.scan_time = 0.1\r\n        self.range_min = 0.1\r\n        self.range_max = 10.0\r\n\r\n        # Timer for scan generation\r\n        self.timer = self.create_timer(0.1, self.generate_scan)\r\n\r\n    def generate_scan(self):\r\n        """Generate simulated LIDAR scan"""\r\n        num_scans = int((self.angle_max - self.angle_min) / self.angle_increment)\r\n\r\n        # Generate ranges with simulated environment\r\n        ranges = []\r\n        for i in range(num_scans):\r\n            angle = self.angle_min + i * self.angle_increment\r\n\r\n            # Simulate environment with walls and obstacles\r\n            distance = self.simulate_distance(angle)\r\n\r\n            # Add noise to make it realistic\r\n            noise = np.random.normal(0, 0.02)  # 2cm noise\r\n            distance_with_noise = max(self.range_min, min(self.range_max, distance + noise))\r\n\r\n            ranges.append(distance_with_noise)\r\n\r\n        # Create LaserScan message\r\n        scan_msg = LaserScan()\r\n        scan_msg.header.stamp = self.get_clock().now().to_msg()\r\n        scan_msg.header.frame_id = \'laser_frame\'\r\n        scan_msg.angle_min = self.angle_min\r\n        scan_msg.angle_max = self.angle_max\r\n        scan_msg.angle_increment = self.angle_increment\r\n        scan_msg.time_increment = self.time_increment\r\n        scan_msg.scan_time = self.scan_time\r\n        scan_msg.range_min = self.range_min\r\n        scan_msg.range_max = self.range_max\r\n        scan_msg.ranges = ranges\r\n        scan_msg.intensities = []  # Intensities are optional\r\n\r\n        # Publish scan\r\n        self.scan_pub.publish(scan_msg)\r\n\r\n    def simulate_distance(self, angle):\r\n        """Simulate distance measurement based on angle"""\r\n        # Example: simple square room with side length 8m\r\n        # Robot is in the center\r\n        half_room_size = 4.0\r\n\r\n        # Calculate distance to walls based on angle\r\n        abs_angle = abs(angle) % (math.pi / 2)\r\n\r\n        if abs_angle < math.pi / 4:\r\n            # Horizontal wall\r\n            distance = half_room_size / math.cos(abs_angle)\r\n        else:\r\n            # Vertical wall\r\n            distance = half_room_size / math.sin(abs_angle)\r\n\r\n        # Add some obstacles\r\n        if 0.5 < distance < 2.0:\r\n            # Add a column in front of the robot\r\n            distance = 1.5\r\n\r\n        return min(distance, self.range_max)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"imu-sensors",children:"IMU Sensors"}),"\n",(0,a.jsx)(e.p,{children:"IMU sensors provide information about acceleration, angular velocity, and orientation:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"from sensor_msgs.msg import Imu\r\nfrom geometry_msgs.msg import Vector3, Quaternion\r\n\r\nclass IMUSimulator(Node):\r\n    def __init__(self):\r\n        super().__init__('imu_simulator')\r\n\r\n        # Publisher for IMU data\r\n        self.imu_pub = self.create_publisher(Imu, '/imu/data', 10)\r\n\r\n        # IMU parameters\r\n        self.linear_acceleration_variance = 0.01\r\n        self.angular_velocity_variance = 0.01\r\n        self.orientation_variance = 0.001\r\n\r\n        # Robot state (for simulating motion)\r\n        self.position = [0.0, 0.0, 0.0]\r\n        self.velocity = [0.0, 0.0, 0.0]\r\n        self.acceleration = [0.0, 0.0, 0.0]\r\n\r\n        # Timer for IMU data generation\r\n        self.timer = self.create_timer(0.01, self.generate_imu_data)  # 100Hz\r\n\r\n    def generate_imu_data(self):\r\n        \"\"\"Generate simulated IMU data\"\"\"\r\n        imu_msg = Imu()\r\n        imu_msg.header.stamp = self.get_clock().now().to_msg()\r\n        imu_msg.header.frame_id = 'imu_link'\r\n\r\n        # Simulate gravity in z-axis\r\n        imu_msg.linear_acceleration.x = self.acceleration[0] + np.random.normal(0, self.linear_acceleration_variance)\r\n        imu_msg.linear_acceleration.y = self.acceleration[1] + np.random.normal(0, self.linear_acceleration_variance)\r\n        imu_msg.linear_acceleration.z = self.acceleration[2] + 9.81 + np.random.normal(0, self.linear_acceleration_variance)\r\n\r\n        # Simulate angular velocity (could be from robot motion)\r\n        imu_msg.angular_velocity.x = np.random.normal(0, self.angular_velocity_variance)\r\n        imu_msg.angular_velocity.y = np.random.normal(0, self.angular_velocity_variance)\r\n        imu_msg.angular_velocity.z = np.random.normal(0, self.angular_velocity_variance)\r\n\r\n        # Simulate orientation (assuming robot is upright)\r\n        imu_msg.orientation.x = np.random.normal(0, self.orientation_variance)\r\n        imu_msg.orientation.y = np.random.normal(0, self.orientation_variance)\r\n        imu_msg.orientation.z = np.random.normal(0, self.orientation_variance)\r\n        imu_msg.orientation.w = 1.0  # Normalize quaternion\r\n\r\n        # Set covariance matrices\r\n        # Linear acceleration covariance\r\n        imu_msg.linear_acceleration_covariance = [\r\n            self.linear_acceleration_variance, 0, 0,\r\n            0, self.linear_acceleration_variance, 0,\r\n            0, 0, self.linear_acceleration_variance\r\n        ]\r\n\r\n        # Angular velocity covariance\r\n        imu_msg.angular_velocity_covariance = [\r\n            self.angular_velocity_variance, 0, 0,\r\n            0, self.angular_velocity_variance, 0,\r\n            0, 0, self.angular_velocity_variance\r\n        ]\r\n\r\n        # Orientation covariance\r\n        imu_msg.orientation_covariance = [\r\n            self.orientation_variance, 0, 0,\r\n            0, self.orientation_variance, 0,\r\n            0, 0, self.orientation_variance\r\n        ]\r\n\r\n        # Publish IMU data\r\n        self.imu_pub.publish(imu_msg)\n"})}),"\n",(0,a.jsx)(e.h2,{id:"sensor-fusion-simulation",children:"Sensor Fusion Simulation"}),"\n",(0,a.jsx)(e.h3,{id:"combining-multiple-sensors",children:"Combining Multiple Sensors"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from sensor_msgs.msg import PointCloud2\r\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\r\nimport tf2_ros\r\nfrom tf2_ros import TransformBroadcaster\r\n\r\nclass SensorFusionSimulator(Node):\r\n    def __init__(self):\r\n        super().__init__(\'sensor_fusion_simulator\')\r\n\r\n        # Subscribers for different sensors\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.image_callback, 10)\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan, \'/scan\', self.scan_callback, 10)\r\n        self.imu_sub = self.create_subscription(\r\n            Imu, \'/imu/data\', self.imu_callback, 10)\r\n\r\n        # Publishers for fused data\r\n        self.fused_pub = self.create_publisher(\r\n            PointCloud2, \'/fused_pointcloud\', 10)\r\n        self.pose_pub = self.create_publisher(\r\n            PoseWithCovarianceStamped, \'/estimated_pose\', 10)\r\n\r\n        # TF broadcaster for transforms\r\n        self.tf_broadcaster = TransformBroadcaster(self)\r\n\r\n        # Internal state\r\n        self.last_image = None\r\n        self.last_scan = None\r\n        self.last_imu = None\r\n\r\n        # Timer for fusion\r\n        self.fusion_timer = self.create_timer(0.1, self.perform_fusion)\r\n\r\n    def image_callback(self, msg):\r\n        """Handle image data"""\r\n        self.last_image = msg\r\n\r\n    def scan_callback(self, msg):\r\n        """Handle LIDAR scan data"""\r\n        self.last_scan = msg\r\n\r\n    def imu_callback(self, msg):\r\n        """Handle IMU data"""\r\n        self.last_imu = msg\r\n\r\n    def perform_fusion(self):\r\n        """Perform sensor fusion to estimate robot state"""\r\n        if not all([self.last_image, self.last_scan, self.last_imu]):\r\n            return  # Not all sensors have published yet\r\n\r\n        # Example fusion: combine IMU orientation with position from other sources\r\n        estimated_pose = self.estimate_pose_from_sensors()\r\n\r\n        # Publish fused estimate\r\n        pose_msg = PoseWithCovarianceStamped()\r\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\r\n        pose_msg.header.frame_id = \'map\'\r\n        pose_msg.pose = estimated_pose\r\n\r\n        self.pose_pub.publish(pose_msg)\r\n\r\n        # Create fused point cloud from camera and LIDAR\r\n        fused_cloud = self.create_fused_pointcloud()\r\n        if fused_cloud:\r\n            self.fused_pub.publish(fused_cloud)\r\n\r\n    def estimate_pose_from_sensors(self):\r\n        """Estimate pose by combining sensor data"""\r\n        # This is a simplified example\r\n        # In practice, you would use Kalman filters or particle filters\r\n        from geometry_msgs.msg import PoseWithCovariance\r\n\r\n        pose = PoseWithCovariance()\r\n\r\n        # Use IMU for orientation\r\n        pose.pose.orientation = self.last_imu.orientation\r\n\r\n        # For position, you might combine odometry with sensor data\r\n        # For simulation, we\'ll just use a fixed position with IMU orientation\r\n        pose.pose.position.x = 0.0\r\n        pose.pose.position.y = 0.0\r\n        pose.pose.position.z = 0.0\r\n\r\n        # Set covariance based on sensor uncertainties\r\n        pose.covariance = [0.1, 0, 0, 0, 0, 0,  # Position covariance\r\n                          0, 0.1, 0, 0, 0, 0,\r\n                          0, 0, 0.1, 0, 0, 0,\r\n                          0, 0, 0, 0.1, 0, 0,  # Orientation covariance\r\n                          0, 0, 0, 0, 0.1, 0,\r\n                          0, 0, 0, 0, 0, 0.1]\r\n\r\n        return pose\r\n\r\n    def create_fused_pointcloud(self):\r\n        """Create a fused point cloud from camera and LIDAR data"""\r\n        # In a real implementation, this would project camera pixels\r\n        # into 3D space using LIDAR depth information\r\n        pass\n'})}),"\n",(0,a.jsx)(e.h2,{id:"physics-based-sensor-simulation",children:"Physics-Based Sensor Simulation"}),"\n",(0,a.jsx)(e.h3,{id:"realistic-sensor-noise-modeling",children:"Realistic Sensor Noise Modeling"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import numpy as np\r\nfrom scipy.spatial.transform import Rotation as R\r\n\r\nclass PhysicsBasedSensorSimulator:\r\n    def __init__(self):\r\n        # Sensor characteristics\r\n        self.camera_noise_params = {\r\n            'gaussian_noise': 0.02,\r\n            'poisson_noise': 0.01,\r\n            'uniform_distortion': 0.05\r\n        }\r\n\r\n        self.lidar_noise_params = {\r\n            'range_bias': 0.02,  # 2cm bias\r\n            'range_noise': 0.01,  # 1cm noise\r\n            'angular_bias': 0.001,  # 0.057 degrees\r\n            'angular_noise': 0.0005  # 0.029 degrees\r\n        }\r\n\r\n        self.imu_noise_params = {\r\n            'accel_bias': 0.01,  # 1% of gravity\r\n            'accel_noise': 0.001,\r\n            'gyro_bias': 0.001,  # rad/s\r\n            'gyro_noise': 0.0001,\r\n            'mag_bias': 0.1,    # micro Tesla\r\n            'mag_noise': 0.01\r\n        }\r\n\r\n    def add_camera_noise(self, image):\r\n        \"\"\"Add realistic noise to camera images\"\"\"\r\n        # Add Gaussian noise\r\n        gaussian_noise = np.random.normal(\r\n            0, self.camera_noise_params['gaussian_noise'], image.shape)\r\n        noisy_image = image.astype(np.float32) + gaussian_noise * 255\r\n\r\n        # Add Poisson noise (signal-dependent)\r\n        poisson_noise = np.random.poisson(noisy_image / 255.0) * 255 - noisy_image\r\n        noisy_image += poisson_noise * self.camera_noise_params['poisson_noise']\r\n\r\n        # Ensure values are in valid range\r\n        noisy_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\r\n\r\n        return noisy_image\r\n\r\n    def add_lidar_noise(self, ranges, angles):\r\n        \"\"\"Add realistic noise to LIDAR measurements\"\"\"\r\n        noisy_ranges = []\r\n\r\n        for i, (range_val, angle) in enumerate(zip(ranges, angles)):\r\n            if range_val >= 10.0:  # Maximum range\r\n                noisy_ranges.append(range_val)\r\n                continue\r\n\r\n            # Add bias and noise\r\n            biased_range = range_val + self.lidar_noise_params['range_bias']\r\n            noisy_range = biased_range + np.random.normal(0, self.lidar_noise_params['range_noise'])\r\n\r\n            # Range-dependent noise (closer objects have less noise)\r\n            range_dependent_noise = self.lidar_noise_params['range_noise'] * (1.0 - range_val / 10.0)\r\n            final_range = noisy_range + np.random.normal(0, range_dependent_noise)\r\n\r\n            noisy_ranges.append(max(0.1, final_range))  # Minimum range constraint\r\n\r\n        return noisy_ranges\r\n\r\n    def add_imu_noise(self, accel_true, gyro_true, dt):\r\n        \"\"\"Add realistic noise to IMU measurements\"\"\"\r\n        # Accelerometer noise\r\n        accel_bias = np.random.normal(0, self.imu_noise_params['accel_bias'], 3)\r\n        accel_noise = np.random.normal(0, self.imu_noise_params['accel_noise'], 3)\r\n        accel_measurement = accel_true + accel_bias + accel_noise\r\n\r\n        # Gyroscope noise\r\n        gyro_bias = np.random.normal(0, self.imu_noise_params['gyro_bias'], 3)\r\n        gyro_noise = np.random.normal(0, self.imu_noise_params['gyro_noise'], 3)\r\n        gyro_measurement = gyro_true + gyro_bias + gyro_noise\r\n\r\n        return accel_measurement, gyro_measurement\n"})}),"\n",(0,a.jsx)(e.h2,{id:"environmental-effects-on-sensors",children:"Environmental Effects on Sensors"}),"\n",(0,a.jsx)(e.h3,{id:"weather-simulation",children:"Weather Simulation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class WeatherEffectSimulator:\r\n    def __init__(self):\r\n        self.weather_conditions = {\r\n            'clear': {'visibility': 100.0, 'precipitation': 0.0},\r\n            'rain_light': {'visibility': 30.0, 'precipitation': 2.0},\r\n            'rain_heavy': {'visibility': 10.0, 'precipitation': 8.0},\r\n            'fog_light': {'visibility': 20.0, 'precipitation': 0.0},\r\n            'fog_dense': {'visibility': 5.0, 'precipitation': 0.0}\r\n        }\r\n\r\n        self.current_weather = 'clear'\r\n\r\n    def set_weather(self, condition):\r\n        \"\"\"Set the current weather condition\"\"\"\r\n        if condition in self.weather_conditions:\r\n            self.current_weather = condition\r\n            return True\r\n        return False\r\n\r\n    def affect_camera(self, image):\r\n        \"\"\"Apply weather effects to camera images\"\"\"\r\n        condition = self.weather_conditions[self.current_weather]\r\n\r\n        if 'rain' in self.current_weather:\r\n            # Rain effect: reduce contrast and add water droplets\r\n            image = self.apply_rain_effect(image, condition['precipitation'])\r\n        elif 'fog' in self.current_weather:\r\n            # Fog effect: reduce visibility and contrast\r\n            image = self.apply_fog_effect(image, condition['visibility'])\r\n\r\n        return image\r\n\r\n    def affect_lidar(self, ranges, angles):\r\n        \"\"\"Apply weather effects to LIDAR measurements\"\"\"\r\n        condition = self.weather_conditions[self.current_weather]\r\n\r\n        affected_ranges = []\r\n        for r in ranges:\r\n            if r >= 10.0:  # Maximum range\r\n                affected_ranges.append(r)\r\n                continue\r\n\r\n            # In bad weather, effective range decreases\r\n            visibility_factor = min(condition['visibility'] / 10.0, 1.0)\r\n            effective_range = r * visibility_factor\r\n\r\n            # Add weather-related noise\r\n            if 'rain' in self.current_weather:\r\n                weather_noise = np.random.normal(0, 0.05)  # More noise in rain\r\n            elif 'fog' in self.current_weather:\r\n                weather_noise = np.random.normal(0, 0.03)  # Some noise in fog\r\n            else:\r\n                weather_noise = 0\r\n\r\n            affected_ranges.append(max(0.1, effective_range + weather_noise))\r\n\r\n        return affected_ranges\r\n\r\n    def apply_rain_effect(self, image, precipitation_level):\r\n        \"\"\"Apply rain effect to image\"\"\"\r\n        # Reduce visibility\r\n        alpha = 1.0 - (precipitation_level / 10.0) * 0.3\r\n        image = (image * alpha).astype(np.uint8)\r\n\r\n        # Add rain streaks (simplified)\r\n        rain_intensity = precipitation_level / 10.0\r\n        rain_pixels = np.random.random(image.shape[:2]) < rain_intensity * 0.05\r\n        image[rain_pixels] = [200, 200, 255]  # Light blue streaks\r\n\r\n        return image\r\n\r\n    def apply_fog_effect(self, image, visibility):\r\n        \"\"\"Apply fog effect to image\"\"\"\r\n        # Calculate fog factor based on visibility\r\n        fog_factor = 1.0 - (visibility / 100.0)\r\n\r\n        # Apply fog by blending with gray\r\n        fog_color = np.array([128, 128, 128], dtype=np.uint8)\r\n        fogged_image = (image * (1 - fog_factor) + fog_color * fog_factor).astype(np.uint8)\r\n\r\n        return fogged_image\n"})}),"\n",(0,a.jsx)(e.h2,{id:"sensor-calibration-simulation",children:"Sensor Calibration Simulation"}),"\n",(0,a.jsx)(e.h3,{id:"calibration-process-simulation",children:"Calibration Process Simulation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class SensorCalibrationSimulator:\r\n    def __init__(self):\r\n        # True calibration parameters\r\n        self.true_intrinsics = np.array([\r\n            [500.0, 0.0, 320.0],  # fx, 0, cx\r\n            [0.0, 500.0, 240.0],  # 0, fy, cy\r\n            [0.0, 0.0, 1.0]       # 0, 0, 1\r\n        ])\r\n\r\n        self.true_distortion = np.array([0.1, -0.2, 0.001, 0.002, 0.0])  # k1, k2, p1, p2, k3\r\n\r\n        # Initial calibration guess (with errors)\r\n        self.calibrated_intrinsics = np.array([\r\n            [495.0, 0.0, 322.0],\r\n            [0.0, 498.0, 238.0],\r\n            [0.0, 0.0, 1.0]\r\n        ])\r\n\r\n        self.calibrated_distortion = np.array([0.09, -0.18, 0.002, 0.001, 0.001])\r\n\r\n    def simulate_calibration_target(self, num_points=50):\r\n        """Simulate a calibration target (checkerboard) in 3D"""\r\n        # Create a checkerboard pattern in 3D\r\n        rows, cols = 8, 6\r\n        square_size = 0.025  # 2.5 cm squares\r\n\r\n        # Generate 3D points of the checkerboard\r\n        obj_points = []\r\n        for j in range(rows):\r\n            for i in range(cols):\r\n                obj_points.append([i * square_size, j * square_size, 0])\r\n\r\n        obj_points = np.array(obj_points, dtype=np.float32)\r\n\r\n        # Generate multiple views by applying random transformations\r\n        views = []\r\n        for _ in range(10):  # 10 different views\r\n            # Random rotation and translation\r\n            rvec = np.random.uniform(-0.5, 0.5, 3)  # Small rotations\r\n            tvec = np.random.uniform(-0.1, 0.1, 3)  # Small translations\r\n\r\n            # Project 3D points to 2D using true intrinsics\r\n            projected_points, _ = cv2.projectPoints(\r\n                obj_points, rvec, tvec,\r\n                self.true_intrinsics, self.true_distortion\r\n            )\r\n\r\n            views.append({\r\n                \'obj_points\': obj_points.copy(),\r\n                \'img_points\': projected_points.reshape(-1, 2),\r\n                \'rvec\': rvec,\r\n                \'tvec\': tvec\r\n            })\r\n\r\n        return views\r\n\r\n    def calibrate_camera(self, views):\r\n        """Simulate camera calibration process"""\r\n        # Collect all object points and image points\r\n        all_obj_points = []\r\n        all_img_points = []\r\n\r\n        for view in views:\r\n            all_obj_points.append(view[\'obj_points\'])\r\n            all_img_points.append(view[\'img_points\'])\r\n\r\n        # Perform calibration (would normally use cv2.calibrateCamera)\r\n        # Here we\'ll just show the process\r\n        ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\r\n            all_obj_points, all_img_points,\r\n            (640, 480), None, None\r\n        )\r\n\r\n        return ret, mtx, dist, rvecs, tvecs\r\n\r\n    def evaluate_calibration(self, calibrated_intrinsics, calibrated_distortion):\r\n        """Evaluate the quality of calibration"""\r\n        # Calculate reprojection error\r\n        test_points_3d = np.array([[0, 0, 0], [0.1, 0, 0], [0, 0.1, 0], [0.1, 0.1, 0]], dtype=np.float32)\r\n\r\n        # Project using true parameters\r\n        true_2d, _ = cv2.projectPoints(\r\n            test_points_3d, np.zeros(3), np.zeros(3),\r\n            self.true_intrinsics, self.true_distortion\r\n        )\r\n\r\n        # Project using calibrated parameters\r\n        calibrated_2d, _ = cv2.projectPoints(\r\n            test_points_3d, np.zeros(3), np.zeros(3),\r\n            calibrated_intrinsics, calibrated_distortion\r\n        )\r\n\r\n        # Calculate reprojection error\r\n        error = np.mean(np.linalg.norm(true_2d - calibrated_2d, axis=2))\r\n\r\n        return error\n'})}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Simulate various types of sensors used in robotics"}),"\n",(0,a.jsx)(e.li,{children:"Implement realistic noise models for sensor data"}),"\n",(0,a.jsx)(e.li,{children:"Perform sensor fusion to combine data from multiple sensors"}),"\n",(0,a.jsx)(e.li,{children:"Account for environmental effects on sensor performance"}),"\n",(0,a.jsx)(e.li,{children:"Understand the calibration process for sensors"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-on Exercise"}),"\n",(0,a.jsx)(e.p,{children:"Create a simulation node that publishes data from multiple sensors (camera, LIDAR, IMU) with realistic noise models. Implement a simple sensor fusion algorithm that combines these sensor readings to estimate the robot's state."}),"\n",(0,a.jsx)(e.admonition,{type:"tip",children:(0,a.jsxs)(e.p,{children:["Use ",(0,a.jsx)(e.code,{children:"rqt_plot"})," or ",(0,a.jsx)(e.code,{children:"plotjuggler"})," to visualize sensor data streams and verify that your noise models produce realistic-looking signals."]})}),"\n",(0,a.jsx)(e.admonition,{type:"note",children:(0,a.jsx)(e.p,{children:"Consider the computational cost of sensor simulation, especially when simulating multiple sensors at high frequencies."})})]})}function _(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(m,{...n})}):m(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>o,x:()=>t});var i=r(6540);const a={},s=i.createContext(a);function o(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:o(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);