"""
Summary Result Response Model for the Reusable Intelligence System
Defines the structure for summary results from the Summarizer subagent
"""
from typing import Dict, Any, List
from datetime import datetime


class SummaryResult:
    """
    Class representing a summary result generated by the Summarizer subagent
    """

    def __init__(self, summary: str, reasoning: str, confidence_score: float, sources: List[str],
                 key_points: List[str], technical_accuracy_score: float):
        self.id = f"summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.summary = summary
        self.reasoning = reasoning
        self.confidence_score = confidence_score
        self.sources = sources
        self.key_points = key_points
        self.technical_accuracy_score = technical_accuracy_score
        self.created_at = datetime.now()
        self.updated_at = datetime.now()

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert the summary result to a dictionary representation

        Returns:
            Dictionary representation of the summary result
        """
        return {
            "id": self.id,
            "summary": self.summary,
            "reasoning": self.reasoning,
            "confidence_score": self.confidence_score,
            "sources": self.sources,
            "key_points": self.key_points,
            "technical_accuracy_score": self.technical_accuracy_score,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat()
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'SummaryResult':
        """
        Create a SummaryResult instance from a dictionary

        Args:
            data: Dictionary containing summary result data

        Returns:
            SummaryResult instance
        """
        return cls(
            summary=data["summary"],
            reasoning=data["reasoning"],
            confidence_score=data["confidence_score"],
            sources=data["sources"],
            key_points=data["key_points"],
            technical_accuracy_score=data["technical_accuracy_score"]
        )

    def update_summary(self, new_summary: str) -> None:
        """
        Update the summary text

        Args:
            new_summary: New summary text
        """
        self.summary = new_summary
        self.updated_at = datetime.now()

    def update_reasoning(self, new_reasoning: str) -> None:
        """
        Update the reasoning text

        Args:
            new_reasoning: New reasoning text
        """
        self.reasoning = new_reasoning
        self.updated_at = datetime.now()

    def add_source(self, source: str) -> None:
        """
        Add a new source to the list of sources

        Args:
            source: Source to add
        """
        if source not in self.sources:
            self.sources.append(source)
            self.updated_at = datetime.now()

    def add_key_point(self, key_point: str) -> None:
        """
        Add a new key point to the list of key points

        Args:
            key_point: Key point to add
        """
        if key_point not in self.key_points:
            self.key_points.append(key_point)
            self.updated_at = datetime.now()

    def update_confidence_score(self, new_score: float) -> None:
        """
        Update the confidence score

        Args:
            new_score: New confidence score (between 0 and 1)
        """
        if 0 <= new_score <= 1:
            self.confidence_score = new_score
            self.updated_at = datetime.now()
        else:
            raise ValueError("Confidence score must be between 0 and 1")

    def update_technical_accuracy_score(self, new_score: float) -> None:
        """
        Update the technical accuracy score

        Args:
            new_score: New technical accuracy score (between 0 and 1)
        """
        if 0 <= new_score <= 1:
            self.technical_accuracy_score = new_score
            self.updated_at = datetime.now()
        else:
            raise ValueError("Technical accuracy score must be between 0 and 1")

    def get_summary_length(self) -> int:
        """
        Get the length of the summary in words

        Returns:
            Number of words in the summary
        """
        return len(self.summary.split())

    def get_word_count_by_source(self) -> Dict[str, int]:
        """
        Get an estimated word count for each source mentioned in the summary

        Returns:
            Dictionary mapping sources to estimated word counts in summary
        """
        word_count_map = {}
        summary_words = self.summary.split()

        for source in self.sources:
            # Simple approach: distribute words proportionally based on source mentions
            source_mentions = self.summary.lower().count(source.lower())
            if source_mentions > 0:
                # Distribute words based on mention frequency
                words_per_mention = len(summary_words) // (len(self.sources) if self.sources else 1)
                word_count_map[source] = min(len(summary_words), words_per_mention * source_mentions)
            else:
                word_count_map[source] = 0

        return word_count_map

    def validate_structure(self) -> Dict[str, Any]:
        """
        Validate the structure of the summary result

        Returns:
            Dictionary with validation results
        """
        validation_results = {
            "is_valid": True,
            "issues": [],
            "warnings": []
        }

        # Check summary
        if not self.summary or len(self.summary.strip()) == 0:
            validation_results["is_valid"] = False
            validation_results["issues"].append("Summary is missing or empty")

        # Check confidence score
        if not isinstance(self.confidence_score, (int, float)) or not (0 <= self.confidence_score <= 1):
            validation_results["is_valid"] = False
            validation_results["issues"].append("Confidence score must be a number between 0 and 1")

        # Check technical accuracy score
        if not isinstance(self.technical_accuracy_score, (int, float)) or not (0 <= self.technical_accuracy_score <= 1):
            validation_results["is_valid"] = False
            validation_results["issues"].append("Technical accuracy score must be a number between 0 and 1")

        # Check sources
        if not isinstance(self.sources, list):
            validation_results["is_valid"] = False
            validation_results["issues"].append("Sources must be a list")

        # Check key points
        if not isinstance(self.key_points, list):
            validation_results["is_valid"] = False
            validation_results["issues"].append("Key points must be a list")

        # Check reasoning
        if not isinstance(self.reasoning, str):
            validation_results["is_valid"] = False
            validation_results["issues"].append("Reasoning must be a string")

        # Add warnings for potentially problematic content
        if self.get_summary_length() < 50:
            validation_results["warnings"].append("Summary is quite short (< 50 words), may lack sufficient detail")

        if len(self.key_points) == 0:
            validation_results["warnings"].append("No key points provided in summary")

        if len(self.sources) == 0:
            validation_results["warnings"].append("No sources provided, summary may lack proper attribution")

        if self.confidence_score < 0.5:
            validation_results["warnings"].append(f"Low confidence score ({self.confidence_score}), summary may be unreliable")

        if self.technical_accuracy_score < 0.7:
            validation_results["warnings"].append(f"Low technical accuracy score ({self.technical_accuracy_score}), technical content may be incorrect")

        return validation_results

    def get_summary_stats(self) -> Dict[str, Any]:
        """
        Get statistical summary of the summary result

        Returns:
            Dictionary with statistical summary
        """
        stats = {
            "summary_length_words": self.get_summary_length(),
            "summary_length_characters": len(self.summary),
            "confidence_score": self.confidence_score,
            "technical_accuracy_score": self.technical_accuracy_score,
            "number_of_sources": len(self.sources),
            "number_of_key_points": len(self.key_points),
            "reasoning_length": len(self.reasoning.split()) if self.reasoning else 0,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat()
        }

        # Add quality indicators
        stats["quality_score"] = (self.confidence_score + self.technical_accuracy_score) / 2
        stats["completeness_score"] = min(1.0, len(self.key_points) / 5)  # Assuming 5 key points is ideal

        return stats

    def filter_by_source(self, source_keywords: List[str]) -> 'SummaryResult':
        """
        Filter the summary to focus on content related to specific sources

        Args:
            source_keywords: List of keywords to match in sources

        Returns:
            New SummaryResult instance with content focused on matching sources
        """
        matching_sources = []
        for source in self.sources:
            if any(keyword.lower() in source.lower() for keyword in source_keywords):
                matching_sources.append(source)

        # For simplicity, we'll return a new summary result with filtered sources
        # In a real implementation, we would need to extract relevant parts of the summary
        # based on the matching sources
        filtered_result = SummaryResult(
            summary=self.summary,  # Would be filtered in real implementation
            reasoning=self.reasoning,  # Would be filtered in real implementation
            confidence_score=self.confidence_score,
            sources=matching_sources,
            key_points=self.key_points,  # Would be filtered in real implementation
            technical_accuracy_score=self.technical_accuracy_score
        )

        return filtered_result

    def merge_with(self, other_result: 'SummaryResult') -> 'SummaryResult':
        """
        Merge this summary result with another one

        Args:
            other_result: Another SummaryResult to merge with

        Returns:
            New SummaryResult instance with merged content
        """
        # Combine summaries (simple concatenation - in real implementation would be more sophisticated)
        combined_summary = f"{self.summary}\n\n{other_result.summary}"

        # Combine reasoning
        combined_reasoning = f"{self.reasoning}\n\n{other_result.reasoning}"

        # Calculate combined confidence score (average weighted by summary length)
        self_length = len(self.summary.split())
        other_length = len(other_result.summary.split())
        total_length = self_length + other_length

        if total_length > 0:
            combined_confidence = (
                (self.confidence_score * self_length) +
                (other_result.confidence_score * other_length)
            ) / total_length
        else:
            combined_confidence = 0.0

        # Calculate combined technical accuracy score (average)
        combined_tech_accuracy = (self.technical_accuracy_score + other_result.technical_accuracy_score) / 2

        # Combine sources and remove duplicates
        all_sources = list(set(self.sources + other_result.sources))

        # Combine key points and remove duplicates
        all_key_points = list(dict.fromkeys(self.key_points + other_result.key_points))  # Preserves order

        # Create merged result
        merged_result = SummaryResult(
            summary=combined_summary,
            reasoning=combined_reasoning,
            confidence_score=combined_confidence,
            sources=all_sources,
            key_points=all_key_points,
            technical_accuracy_score=combined_tech_accuracy
        )

        return merged_result

    def extract_key_themes(self) -> List[str]:
        """
        Extract key themes from the summary and key points

        Returns:
            List of extracted themes
        """
        # Simple keyword extraction approach
        all_text = f"{self.summary} {' '.join(self.key_points)}"
        words = all_text.lower().split()

        # Common technical terms that might represent themes
        technical_terms = [
            "algorithm", "method", "approach", "system", "model", "architecture",
            "learning", "network", "data", "process", "framework", "technique",
            "application", "challenge", "solution", "analysis", "design", "implementation"
        ]

        themes = []
        for term in technical_terms:
            if term in all_text.lower():
                themes.append(term)

        # Also include capitalized words that might be important concepts
        capitalized_words = [word for word in set(words) if word[0].isupper() and len(word) > 3]
        themes.extend(capitalized_words)

        # Remove duplicates while preserving order
        unique_themes = []
        for theme in themes:
            if theme not in unique_themes:
                unique_themes.append(theme)

        return unique_themes

    def get_reading_time_estimate(self) -> int:
        """
        Estimate reading time for the summary (in minutes)

        Returns:
            Estimated reading time in minutes
        """
        # Assuming 200 words per minute reading speed
        words_per_minute = 200
        total_words = self.get_summary_length()
        reading_time_minutes = max(1, total_words // words_per_minute)

        return reading_time_minutes

    def get_source_credibility_score(self) -> float:
        """
        Estimate credibility of sources based on their type

        Returns:
            Estimated credibility score between 0 and 1
        """
        if not self.sources:
            return 0.0

        credibility_score = 0.0
        total_sources = len(self.sources)

        for source in self.sources:
            source_lower = source.lower()

            # Academic or peer-reviewed sources
            if any(domain in source_lower for domain in [".edu", ".org", "arxiv", "ieee", "springer", "acm", "journal", "conference"]):
                credibility_score += 1.0
            # Official documentation or reputable organizations
            elif any(domain in source_lower for domain in [".gov", ".org", "official", "manual", "documentation"]):
                credibility_score += 0.8
            # News or educational sites
            elif any(domain in source_lower for domain in [".com", "news", "education", "tutorial"]):
                credibility_score += 0.6
            # Social media or informal sources
            elif any(domain in source_lower for domain in ["twitter", "facebook", "blog", "forum"]):
                credibility_score += 0.3
            # Unknown or unverifiable sources
            else:
                credibility_score += 0.5

        # Normalize the score
        return credibility_score / total_sources if total_sources > 0 else 0.0